{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# \n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import trange\n",
    "import parmap\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "#import umap\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import sklearn.experimental\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# \n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "# \n",
    "\n",
    "import numba\n",
    "from numba import jit\n",
    "import parmap\n",
    "\n",
    "\n",
    "from klaus_emd_tools import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################\n",
    "# #####################################################\n",
    "# #####################################################\n",
    "# def median_filter(data, max_gap, filter_width):\n",
    "    \n",
    "#     for f in trange(data.shape[1]):\n",
    "#         for l in range(data.shape[2]):\n",
    "#             x = data[:,f,l]\n",
    "#             for k in range(1000):\n",
    "#                 idx = np.where(np.isnan(x))[0]\n",
    "#                 if idx.shape[0]==0:\n",
    "#                     break\n",
    "\n",
    "#                 if idx[0]==0:\n",
    "#                     idx=idx[1:]\n",
    "#                 x[idx] = x[idx-1]\n",
    "\n",
    "#             x = scipy.ndimage.median_filter(x, size=filter_width)\n",
    "#             data[:,f,l]= x\n",
    "\n",
    "#     return data\n",
    "\n",
    "# #\n",
    "# def get_durations(data, ctr, animal_id, min_duration = 25, plotting = False):\n",
    "#     clrs = ['black','green', 'magenta','brown']\n",
    "#     labels = ['two', 'all body', 'headnose', 'sliding']\n",
    "\n",
    "    \n",
    "#     #\n",
    "#     root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "#     d = np.load(root_dir + '/animalID_'+\n",
    "#                 str(animal_id)+\n",
    "#                 '_alldata_imputed.npy')\n",
    "#     print (\"animal: \", animal_id, \"all data imputed: \", d.shape)\n",
    "\n",
    "#     #\n",
    "#     starts = []\n",
    "#     ends = []\n",
    "#     starts.append(data[0])\n",
    "#     for d in range(1,data.shape[0],1):\n",
    "#         if (data[d]-data[d-1])>1:\n",
    "#             ends.append(data[d-1])\n",
    "#             starts.append(data[d])\n",
    "\n",
    "#     #\n",
    "#     starts = np.array(starts)\n",
    "#     ends = np.array(ends)\n",
    "#     if starts.shape[0]==ends.shape[0]+1:\n",
    "#         starts=starts[:-1]\n",
    "\n",
    "#     durations = (ends - starts)+1\n",
    "    \n",
    "#     dd = []\n",
    "#     vectors = []\n",
    "#     for k in range(durations.shape[0]):\n",
    "#         temp = durations[k]\n",
    "#         if temp>=min_duration:\n",
    "#             for p in range(0,temp-min_duration,1):\n",
    "#                 dd.append(min_duration)\n",
    "                \n",
    "#                 #print (starts[k]+p, ends[k]+p)\n",
    "#                 vectors.append([starts[k]+p, starts[k]+min_duration+p])\n",
    "\n",
    "#     vectors = np.vstack(vectors)\n",
    "    \n",
    "    \n",
    "#     #\n",
    "#     if plotting:\n",
    "#         width = 10\n",
    "#         bins = np.arange(0,250,width)\n",
    "#         y = np.histogram(durations, bins= bins)\n",
    "#         plt.plot(y[1][1:], y[0], label=labels[ctr], c=clrs[ctr])\n",
    "\n",
    "#         # also plot sliding window data\n",
    "#         y = np.histogram(dd, bins= bins)\n",
    "#         plt.plot(y[1][1:], y[0], label=labels[-4], c=clrs[-4])\n",
    "\n",
    "#             #\n",
    "#         plt.semilogy()\n",
    "#         plt.plot([25,25],[0,5000],'r--', label='1sec')\n",
    "#         plt.plot([12.5,12.5],[0,5000],'b--', label='0.5sec')\n",
    "#         plt.ylim(bottom=1)\n",
    "#         plt.title(\"Cohort 1, March 26, 23hrs;  animal_id \" + str(animal_id))\n",
    "#         plt.legend()\n",
    "\n",
    "#     return durations, dd, vectors\n",
    "    \n",
    "# #\n",
    "# def get_lengths(animal_id):\n",
    "\n",
    "#     #\n",
    "#     root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "#     d = np.load(root_dir + '/animalID_'+\n",
    "#                 str(animal_id)+\n",
    "#                 '_alldata_imputed.npy')\n",
    "\n",
    "#     fname_out = os.path.join(root_dir,\"animalID_\"+str(animal_id)+\"lengths.npz\")\n",
    "    \n",
    "#     if os.path.exists(fname_out)==False:\n",
    "\n",
    "#         # \n",
    "#         two = []\n",
    "#         six = []\n",
    "        \n",
    "#         headnose = []\n",
    "#         for k in trange(d.shape[0]):\n",
    "#             temp = d[k]\n",
    "#             idx1 = np.where(np.isnan(temp[:,0])==False)[0]\n",
    "#             idx2 = np.where(np.isnan(temp[:,1])==False)[0]\n",
    "            \n",
    "#             # make sure sme missing\n",
    "#             if idx1.shape[0]!=idx2.shape[0]:\n",
    "#                 continue\n",
    "\n",
    "#             # \n",
    "#             if idx1.shape[0]==6:\n",
    "#                 six.append(k)\n",
    "#                 continue\n",
    "\n",
    "#             # \n",
    "#             if idx1.shape[0]>=2:\n",
    "#                 two.append(k)\n",
    "#                 if idx1[0]==0 and idx1[1]==1:\n",
    "#                     headnose.append(k)\n",
    "\n",
    "#         # \n",
    "#         two = np.array(two)\n",
    "#         six = np.array(six)\n",
    "#         headnose = np.array(headnose)\n",
    "#         print (\"two: \", two.shape, \" six: \", six.shape)\n",
    "\n",
    "#         np.savez(fname_out,\n",
    "#                 two=two,\n",
    "#                 six=six,\n",
    "#                 headnose=headnose)\n",
    "#     else:\n",
    "#         d = np.load(fname_out, allow_pickle=True)\n",
    "#         two = d['two']\n",
    "#         six = d['six']\n",
    "#         headnose = d['headnose']\n",
    "        \n",
    "#     return two, six, headnose\n",
    "\n",
    "# # \n",
    "# def visualize_durations():\n",
    "#     fig=plt.figure()\n",
    "#     animal_ids = np.arange(4)\n",
    "#     clrs = ['red','blue', 'cyan','green']\n",
    "#     labels = ['two', 'all body', 'headnose', 'sliding']\n",
    "\n",
    "#     two_feat_flag = False\n",
    "#     min_durations = np.array([25,50,125,250,500,750,1500,3000,4500,6000,7500])\n",
    "#     for animal_id in animal_ids:\n",
    "\n",
    "#         #\n",
    "#         # max_gap = 10\n",
    "#         # filter_width = 25\n",
    "#         # d = median_filter(d, max_gap, filter_width)\n",
    "\n",
    "#         #\n",
    "#         two, six, headnose = get_lengths(animal_id)\n",
    "\n",
    "#         res = []\n",
    "#         for min_duration in min_durations:\n",
    "#             # \n",
    "#             if two_feat_flag:\n",
    "#                 _, dur_1sec, _ = get_durations(two, 0, animal_id, min_duration = min_duration, plotting = False)\n",
    "#             else:\n",
    "#                 _, dur_1sec, _ = get_durations(six, 0, animal_id, min_duration = min_duration, plotting = False)\n",
    "\n",
    "#             print (animal_id, len(dur_1sec))\n",
    "#             res.append(len(dur_1sec))\n",
    "\n",
    "#         #\n",
    "#         plt.plot(min_durations/25., res, \n",
    "#                  color=clrs[animal_id],\n",
    "#                  linewidth=3, label = \"animal: \"+str(animal_id))\n",
    "\n",
    "#         #\n",
    "#         #break\n",
    "#     plt.ylim(bottom=1)\n",
    "#     #plt.xlim(0,7500/25)\n",
    "#     plt.xlim(0,1500/25)\n",
    "#     #plt.suptitle(\"# of segments of specific duration containing head + nose features\",fontsize=20)\n",
    "#     plt.suptitle(\"# of segments of specific duration containing all six features\",fontsize=20)\n",
    "#     #plt.semilogy()\n",
    "#     plt.xlabel(\"Duration of segment (Sec)\", fontsize=20)\n",
    "#     plt.ylabel(\"# of segments\", fontsize=20)\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #\n",
    "# def centre_and_align2_pairwise(data, centre_pt = 0, rotation_pt = 1):\n",
    "\n",
    "#     # centre the data on the nose\n",
    "#     # data[:,0] -= data[centre_pt,0]\n",
    "#     # data[:,1] -= data[centre_pt,1]\n",
    "#     #print (\"data; \", data.shape)\n",
    "\n",
    "#     translation_pt = data[0,centre_pt]\n",
    "#     data -= translation_pt\n",
    "    \n",
    "\n",
    "#     # get angle between +x axis and head location (i.e. 2nd position)\n",
    "#     # 2ND POSITION ALIGNMENT\n",
    "#     angle = -np.arctan2(*data[0,rotation_pt].T[::-1])-np.pi/2\n",
    "\n",
    "#     #print (\"agnel: \", angle, \"  translation pt: \", translation_pt, '  Rotation pt: ', rotation_pt)\n",
    "    \n",
    "#     # get rotation\n",
    "#     rotmat = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "#                        [np.sin(angle),  np.cos(angle)]])\n",
    "\n",
    "#     # Apply rotation to each row of m\n",
    "#     data_rot = []\n",
    "#     for p in range(data.shape[0]):\n",
    "#         data_rot.append((rotmat @ data[p].T).T)\n",
    "\n",
    "#     data_rot = np.array(data_rot)\n",
    "    \n",
    "#     idx = np.where(np.isnan(data_rot))\n",
    "#     if idx[0].shape[0]>0:\n",
    "#         print (data_rot)\n",
    "    \n",
    "#         print (\"data: \", data)\n",
    "#         return None\n",
    "    \n",
    "#     #print (\"data rot: \", data_rot.shape)\n",
    "#     # return rotated body, angle and translation pt\n",
    "#     return data_rot #, angle #, translation_pt\n",
    "\n",
    "\n",
    "\n",
    "# #    \n",
    "# def get_vectors(animal_id, vectors_idx, feature_ids, min_duration):\n",
    "\n",
    "#     root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "#     d = np.load(root_dir + '/animalID_'+\n",
    "#                 str(animal_id)+\n",
    "#                 '_alldata_imputed.npy')\n",
    "    \n",
    "#     print (d.shape)\n",
    "    \n",
    "#     # \n",
    "#     vecs = []\n",
    "#     for k in trange(vectors_idx.shape[0]):\n",
    "#         temp = d[vectors_idx[k][0]:vectors_idx[k][1]][:,feature_ids]\n",
    "#         idx = np.where(np.isnan(temp))\n",
    "        \n",
    "#         if idx[0].shape[0]>0:\n",
    "#             print (k, vectors_idx[k][0], vectors_idx[k][1])\n",
    "#             print (temp)\n",
    "#             break\n",
    "            \n",
    "#         vecs.append(temp)\n",
    "        \n",
    "#     vecs = np.array(vecs)\n",
    "    \n",
    "#     return vecs\n",
    "\n",
    "\n",
    "# # \n",
    "# def vectors_to_egocentric(vecs, animal_id,min_duration):\n",
    "    \n",
    "#     # print (\"vecs: \", vecs.shape)\n",
    "\n",
    "#     fname_out = '/home/cat/vecs_ego_animalID_'+str(animal_id)+\"_duration_\"+str(min_duration)+'.npy'\n",
    "#     if os.path.exists(fname_out)==False:\n",
    "#         vecs_ego = np.zeros(vecs.shape, 'float32')+np.nan\n",
    "\n",
    "#         # \n",
    "#         for s in trange(vecs.shape[0],  desc='Getting egocentric vectors', leave=True):\n",
    "\n",
    "#             vec = vecs[s]\n",
    "#             #print (\"vec: \", vec.shape)\n",
    "#             # centre and align data\n",
    "#             vecs_ego[s] = centre_and_align2_pairwise(vec)\n",
    "\n",
    "#         np.save(fname_out, vecs_ego)\n",
    "#     else:\n",
    "#         vecs_ego = np.load(fname_out)\n",
    "            \n",
    "#     return vecs_ego\n",
    "\n",
    "# def find_movements(vecs_ego):\n",
    "    \n",
    "#     print (\"vecs: \", vecs_ego.shape)\n",
    "\n",
    "\n",
    "#     min_quiet_n_frames = 2      # number of frames of static\n",
    "#     min_quiet_movement = 1\n",
    "    \n",
    "#     #\n",
    "#     min_velocity = 1             # minimum pixels to move to indicate movement intiaitins \n",
    "    \n",
    "    \n",
    "#     # \n",
    "#     ctr_q = 0\n",
    "#     idx_movement = []\n",
    "#     s = 0\n",
    "#     while s<vecs_ego.shape[0]:\n",
    "# #    for s in trange(vecs_ego.shape[0]):\n",
    "        \n",
    "#         # check if there is at least 10 frames quiet followed by high movement\n",
    "        \n",
    "#         # get location of noses\n",
    "#         noses = vecs_ego[s,:,0]\n",
    "#         #print (\"noses: \", noses.shape)\n",
    "        \n",
    "#         # get velocities\n",
    "#         temp = noses[1:]-noses[:-1]\n",
    "#         vel = np.linalg.norm(temp,axis=1)\n",
    "#         #print (\"vel: \", vel, vel.shape)\n",
    "        \n",
    "#         # find periods with at least 10 frames of low or no movement\n",
    "#         if np.max(vel[:min_quiet_n_frames])<=min_quiet_movement:\n",
    "#             ctr_q+=1\n",
    "            \n",
    "#             # require minimum movement in next frame\n",
    "#             if True:\n",
    "#                 if vel[min_quiet_n_frames]>=min_velocity:\n",
    "#                     idx_movement.append(s)\n",
    "#                     #vecs_movement.append(vecs_ego[s])\n",
    "#             else:\n",
    "#                 #vecs_movement.append(vecs_ego[s])\n",
    "#                 idx_movement.apppend(s)\n",
    "#         s+=1\n",
    "        \n",
    "        \n",
    "#     print (\"ctrq: \", ctr_q)\n",
    "#     idx_movement = np.array(idx_movement)\n",
    "        \n",
    "#     return idx_movement\n",
    "\n",
    "\n",
    "# def dim_red(X_pca):\n",
    "#     from sklearn import decomposition\n",
    "#     import sklearn\n",
    "\n",
    "#     # \n",
    "#     print (\"X_pca: \", X_pca.shape)\n",
    "\n",
    "#     #\n",
    "#     if False:\n",
    "#         pca = decomposition.PCA(n_components=3)\n",
    "\n",
    "#         X_pca = pca.fit_transform(X_pca)\n",
    "#         print (X_pca.shape)\n",
    "\n",
    "#     else:\n",
    "#         import umap\n",
    "#         umap = umap.UMAP(n_components=2,\n",
    "#                         init='random',\n",
    "#                         random_state=0)\n",
    "\n",
    "#         print (\"fitting umap\")\n",
    "#         #umap_ = umap.fit(vecs_pca[::10])\n",
    "#         umap_ = umap.fit(X_pca)\n",
    "\n",
    "#         print (\"transforming alldata\")\n",
    "#         X_pca = umap_.transform(X_pca)\n",
    "\n",
    "\n",
    "#     print (\"plotting: \", X_pca.shape)\n",
    "    \n",
    "#     return X_pca\n",
    "    \n",
    "\n",
    "# # \n",
    "# def make_pca_data(vecs_movement, angles):\n",
    "    \n",
    "#     vecs_nose = vecs_movement[:,:,0]\n",
    "#     print (\"vecs_nose: \", vecs_nose.shape)\n",
    "    \n",
    "#     X_pca = np.zeros((vecs_movement.shape[0], vecs_movement.shape[1],vecs_movement.shape[2]+1))\n",
    "    \n",
    "#     for k in range(X_pca.shape[0]):\n",
    "#         X_pca[k,:,:2] = vecs_nose[k]\n",
    "#         X_pca[k,:,2] = angles[k]\n",
    "        \n",
    "    \n",
    "#     print (X_pca.shape)\n",
    "    \n",
    "#     X_pca = X_pca.reshape(X_pca.shape[0],-1)\n",
    "    \n",
    "#     return X_pca \n",
    "\n",
    "# from numpy import arccos, array\n",
    "# from numpy.linalg import norm\n",
    "\n",
    "# # Note: returns angle in radians\n",
    "# def theta(v, w): \n",
    "#     return arccos(v.dot(w)/(norm(v)*norm(w)))\n",
    "\n",
    "\n",
    "# def get_angles(vecs, animal_id, min_duration):\n",
    "\n",
    "#     fname_out = '/home/cat/angles_ego_animalID_'+str(animal_id)+\"_duration_\"+str(min_duration)+'.npy'\n",
    "     \n",
    "#     if os.path.exists(fname_out)==False:\n",
    "#         angles = np.zeros((vecs.shape[0], \n",
    "#                            vecs.shape[1]),\n",
    "#                            'float32')+np.nan\n",
    "#         # \n",
    "#         for f in trange(vecs.shape[0],  desc='Getting angles', leave=True):\n",
    "\n",
    "#             # \n",
    "#             temp1 = vecs[f,0]\n",
    "#             temp1 = temp1[1] - temp1[0]\n",
    "#             angle1 = np.angle(complex(*(temp1)))\n",
    "#             for m in range(0,vecs.shape[1],1):\n",
    "\n",
    "#                 # \n",
    "#                 temp2 = vecs[f,m]\n",
    "#                 temp2 = temp2[1] - temp2[0]\n",
    "#                 angle2 = np.angle(complex(*(temp2)), deg=True)\n",
    "\n",
    "#                 angle = angle1-angle2\n",
    "#                 angles[f,m]=angle\n",
    "        \n",
    "#         np.save(fname_out, angles)\n",
    "#     else:\n",
    "#         angles = np.load(fname_out)\n",
    "        \n",
    "#     return angles\n",
    "\n",
    "\n",
    "# def get_angles3(vecs, vecs_ego, animal_id, min_duration):\n",
    "#     import math\n",
    "    \n",
    "#     root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "\n",
    "#     fname_out = root_dir + '/angles_ego_animalID_'+str(animal_id)+\"_duration_\"+str(min_duration)+'.npy'\n",
    "     \n",
    "#     if os.path.exists(fname_out)==False:\n",
    "#         angles = np.zeros((vecs.shape[0], \n",
    "#                            vecs.shape[1]),\n",
    "#                            'float32')+np.nan\n",
    "#         # \n",
    "#         print (\"vecs: \", vecs.shape)\n",
    "#         for f in trange(vecs.shape[0],  desc='Getting angles', leave=True):\n",
    "\n",
    "#             # \n",
    "#             temp1 = vecs[f,0]\n",
    "#             temp1 = temp1[1] - temp1[0]\n",
    "#             angle1 = np.angle(complex(*(temp1)))\n",
    "#             for m in range(0,vecs.shape[1],1):\n",
    "\n",
    "#                 # \n",
    "#                 # \n",
    "#                 temp2 = vecs[f,m]\n",
    "#                 temp2 = temp2[1] - temp2[0]\n",
    "                \n",
    "#                 angle = math.atan2(temp1[0]*temp2[1] - temp1[1]*temp2[0], \n",
    "#                               temp1[0]*temp2[0] + temp1[1]*temp2[1])\n",
    "\n",
    "#                 angles[f,m]=angle\n",
    "                \n",
    "#             break\n",
    "        \n",
    "#         return None\n",
    "    \n",
    "#         np.save(fname_out, angles)\n",
    "#     else:\n",
    "#         angles = np.load(fname_out)\n",
    "        \n",
    "\n",
    "#     return angles\n",
    "# # \n",
    "\n",
    "\n",
    "# def get_angles2(vecs, vecs_ego, animal_id, min_duration):\n",
    "#     import math\n",
    "    \n",
    "#     fname_out = '/home/cat/angles_ego_animalID_'+str(animal_id)+\"_duration_\"+str(min_duration)+'.npy'\n",
    "     \n",
    "#     if os.path.exists(fname_out)==False:\n",
    "#         angles = np.zeros((vecs.shape[0], \n",
    "#                            vecs.shape[1]),\n",
    "#                            'float32')+np.nan\n",
    "#         # \n",
    "#         for f in trange(vecs.shape[0],  desc='Getting angles', leave=True):\n",
    "\n",
    "#             # \n",
    "#             temp1 = vecs[f,0]\n",
    "#             temp1 = temp1[1] - temp1[0]\n",
    "#             angle1 = np.angle(complex(*(temp1)))\n",
    "#             for m in range(0,vecs.shape[1],1):\n",
    "\n",
    "#                 # \n",
    "#                 # \n",
    "#                 temp2 = vecs[f,m]\n",
    "#                 temp2 = temp2[1] - temp2[0]\n",
    "                \n",
    "#                 angle = math.atan2(temp1[0]*temp2[1] - temp1[1]*temp2[0], \n",
    "#                               temp1[0]*temp2[0] + temp1[1]*temp2[1])\n",
    "\n",
    "#                 angles[f,m]=angle\n",
    "        \n",
    "#         np.save(fname_out, angles)\n",
    "#     else:\n",
    "#         angles = np.load(fname_out)\n",
    "        \n",
    "\n",
    "#     return angles\n",
    "# # \n",
    "\n",
    "\n",
    "# def get_acceleration(vecs, animal_id, min_duration):\n",
    "    \n",
    "#     fname_out = '/home/cat/acceleration_ego_animalID_'+str(animal_id)+\"_duration_\"+str(min_duration)+'.npz'\n",
    "    \n",
    "#     acc_ap = np.zeros((vecs.shape[0],vecs.shape[1]-2),'float32')\n",
    "#     acc_ml = np.zeros((vecs.shape[0],vecs.shape[1]-2),'float32')\n",
    "    \n",
    "#     if os.path.exists(fname_out)==False:\n",
    "#         for f in trange(vecs.shape[0], desc='Getting acceleration'):\n",
    "\n",
    "#             vecs_nose = vecs[f,:,0]\n",
    "#             #print (\"vecs_nose: \", vecs_nose.shape)\n",
    "\n",
    "#             vel_ap = vecs_nose[1:,0] - vecs_nose[:-1,0]\n",
    "#             vel_ml = vecs_nose[1:,1] - vecs_nose[:-1,1]\n",
    "\n",
    "#             #\n",
    "#             aa_ap = vel_ap[1:]-vel_ap[:-1]\n",
    "#             aa_ml = vel_ml[1:]-vel_ml[:-1]\n",
    "            \n",
    "#             #print (\"aa_ap: \", aa_ap.shape)\n",
    "#             acc_ap[f]=aa_ap\n",
    "#             acc_ml[f]=aa_ml\n",
    "\n",
    "#         np.savez(fname_out,\n",
    "#                 acc_ap=acc_ap,\n",
    "#                 acc_ml=acc_ml)\n",
    "#     else:\n",
    "#         d = np.load(fname_out)\n",
    "#         acc_ap = d['acc_ap']\n",
    "#         acc_ml = d['acc_ml']\n",
    "   \n",
    "#     return acc_ap, acc_ml\n",
    "\n",
    "\n",
    "\n",
    "# def generate_egocentric_and_angles_data(animal_ids, min_duration):\n",
    "\n",
    "#     vecs_mov_array = []\n",
    "#     # \n",
    "#     for animal_id in animal_ids:\n",
    "\n",
    "#         #\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         two, six, headnose = get_lengths(animal_id)\n",
    "\n",
    "#         #\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         _,_,vectors_idx = get_durations(headnose, 0, \n",
    "#                                         animal_id, \n",
    "#                                         min_duration = min_duration, \n",
    "#                                         plotting = False)\n",
    "                \n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         feature_ids = np.arange(2)\n",
    "#         vecs = get_vectors(animal_id, vectors_idx, feature_ids, min_duration)\n",
    "\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         vecs_ego = vectors_to_egocentric(vecs, animal_id, min_duration)\n",
    "\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         angles = get_angles2(vecs, vecs_ego, animal_id, min_duration)\n",
    "\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         _,_ = get_acceleration(vecs_ego,  animal_id, min_duration)\n",
    "\n",
    "\n",
    "#         print (\"angles: \", angles.shape)\n",
    "\n",
    "#         print ('')\n",
    "\n",
    "# def plot_angle_acceleration_distributions(animal_ids, min_duration):\n",
    "#     fps = 25\n",
    "#     # \n",
    "#     vecs_mov_array = []\n",
    "#     for animal_id in animal_ids:\n",
    "        \n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         vecs = None                        # if this is already computed it is not required\n",
    "#         vecs_ego = vectors_to_egocentric(vecs, animal_id, min_duration)\n",
    "\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         angles = get_angles2(vecs, vecs_ego, animal_id, min_duration)\n",
    "\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         ##################################\n",
    "#         acc_ap, acc_ml = get_acceleration(vecs_ego,  animal_id, min_duration)\n",
    "\n",
    "#         ##################################\n",
    "#         ######### PLOT ANGLES ############\n",
    "#         ##################################\n",
    "#         plt.subplot(4,4,animal_id*4+1)\n",
    "\n",
    "#         print (\"angles: \", angles.shape)\n",
    "#         print ('acc: ', acc_ap.shape)\n",
    "#         width = 1\n",
    "#         rad_to_degree= 57.2958\n",
    "#         lims = 360\n",
    "#         bins = np.arange(-lims,lims+width, width)\n",
    "#         temp = angles.flatten()*fps*rad_to_degree\n",
    "\n",
    "\n",
    "#         y = np.histogram(temp, bins=bins)\n",
    "#         plt.plot(y[1][1:]-width/2, y[0],c='black')\n",
    "#         plt.semilogy()\n",
    "#         plt.ylim(bottom=1)\n",
    "#         plt.plot([0,0],[0,np.max(y[0])],'--')\n",
    "#         plt.plot([45,45],[0,np.max(y[0])],'--')\n",
    "#         plt.plot([-45,-45],[0,np.max(y[0])],'--')\n",
    "#         if animal_id==0:\n",
    "#             plt.title(\"angles (deg/sec) pdf\")\n",
    "#         plt.ylabel(\"Animal \"+str(animal_id))\n",
    "\n",
    "#         ##################################\n",
    "#         ######### PLOT ACC_AP ############\n",
    "#         ##################################\n",
    "#         plt.subplot(4,4,animal_id*4+2)\n",
    "#         lims = 0.5*fps\n",
    "#         width = .01*fps\n",
    "#         bins = np.arange(-lims,lims+width, width)\n",
    "#         temp = acc_ap.flatten()\n",
    "#         y = np.histogram(temp, bins=bins)\n",
    "#         plt.plot(y[1][1:]-width/2., y[0],c='blue')\n",
    "#         plt.semilogy()\n",
    "#         plt.ylim(bottom=1)\n",
    "#         plt.plot([0,0],[0,np.max(y[0])],'--')\n",
    "#         if animal_id==0:\n",
    "#             plt.title(\"acceleration ap (pix/sec) pdf\")\n",
    "\n",
    "#         ##################################\n",
    "#         ######### PLOT ACC_ML ############\n",
    "#         ##################################\n",
    "#         plt.subplot(4,4,animal_id*4+3)\n",
    "#         #width = 1\n",
    "#         bins = np.arange(-lims,lims+width, width)\n",
    "#         temp = acc_ml.flatten()\n",
    "#         y = np.histogram(temp, bins=bins)\n",
    "#         plt.plot(y[1][1:]-width/2, y[0],c='red')    \n",
    "#         plt.semilogy()\n",
    "#         plt.plot([0,0],[0,np.max(y[0])],'--')\n",
    "#         plt.ylim(bottom=1)\n",
    "#         if animal_id==0:\n",
    "#             plt.title(\"acceleration ml (pix/sec) pdf\")\n",
    "\n",
    "#         ##################################\n",
    "#         ###### PLOT ACC-OVERALL ##########\n",
    "#         ##################################\n",
    "#         plt.subplot(4,4,animal_id*4+4)\n",
    "#         width = 5\n",
    "#         lims = 500\n",
    "#         bins = np.arange(0,lims+width, width)\n",
    "#         temp1 = acc_ml.flatten()\n",
    "#         temp2 = acc_ap.flatten()\n",
    "#         temp3 = np.sqrt(acc_ml**2+acc_ap**2)*25\n",
    "\n",
    "#         #\n",
    "#         y = np.histogram(temp3, bins=bins)\n",
    "#         plt.plot(y[1][1:]-width/2, y[0],c='magenta')    \n",
    "#         plt.semilogy()\n",
    "#         #plt.semilogx()\n",
    "#         plt.plot([0,0],[0,np.max(y[0])],'--')\n",
    "#         plt.plot([35,35],[0,np.max(y[0])],'--')\n",
    "#         plt.plot([150,150],[0,np.max(y[0])],'--')\n",
    "#         plt.plot([245,245],[0,np.max(y[0])],'--')\n",
    "#         plt.ylim(bottom=1)\n",
    "#         if animal_id==0:\n",
    "#             plt.title(\"abs acceleration (pix/sec) pdf\")   \n",
    "\n",
    "#         print ('')\n",
    "        \n",
    "\n",
    "# def compute_discretized_angles_and_acceleration(animal_id, min_duration):\n",
    "#     fps = 25\n",
    "#     rad_to_degree= 57.2958\n",
    "\n",
    "#     # discretized thresholds for angles \n",
    "#     angles_thresh = [[-1E8, -45],\n",
    "#                      [-45,+45],\n",
    "#                      [+45,1E8]\n",
    "#                     ]\n",
    "    \n",
    "#     # discretized thresholds for acceleration \n",
    "# #     acc_thresh = [[0,30],\n",
    "# #                   [30,75],\n",
    "# #                   [75,150],\n",
    "# #                   [150,1E8]]\n",
    "    \n",
    "# #     acc_thresh = [[0,30],\n",
    "# #                   [30,1E8]]\n",
    "\n",
    "#     acc_thresh = [[0,30],\n",
    "#                   [30,75],\n",
    "#                   [75,1E8]]\n",
    "        \n",
    "#     #\n",
    "    \n",
    "#     #\n",
    "#     print (\"agnels thresholds: \", angles_thresh)\n",
    "    \n",
    "#     # \n",
    "#     print (\"accelaration thresholds: \", acc_thresh)\n",
    "    \n",
    "\n",
    "#     ##################################\n",
    "#     ##################################\n",
    "#     ##################################\n",
    "#     #vecs = None                        # if this is already computed it is not required\n",
    "#     vecs_ego = vectors_to_egocentric(vecs, animal_id, min_duration)\n",
    "\n",
    "#     ##################################\n",
    "#     ##################################\n",
    "#     ##################################\n",
    "#     angles = get_angles3(vecs, vecs_ego, animal_id, min_duration)\n",
    "#     #print (\"ANGLES: \", angles)\n",
    "\n",
    "#     ##################################\n",
    "#     ##################################\n",
    "#     ##################################\n",
    "#     acc_ap, acc_ml = get_acceleration(vecs_ego, animal_id, min_duration)\n",
    "    \n",
    "#     ##################################\n",
    "#     ###### SAVE CONTINOUS DATA #######\n",
    "#     ##################################    \n",
    "#     # process acceleration\n",
    "#     temp1 = acc_ml.flatten()\n",
    "#     temp2 = acc_ap.flatten()\n",
    "#     acc = np.sqrt(acc_ml**2+acc_ap**2)*25\n",
    "    \n",
    "#     print (\"angles and everything else: \", angles.shape, acc_ap.shape, acc_ml.shape)\n",
    "#     all_continuous = np.hstack((angles[:,2:], acc_ap, acc_ml))\n",
    "#     print (\"all_continuous: \", all_continuous.shape)\n",
    "#     # \n",
    "#     np.save('/home/cat/all_continuous_'+str(animal_id)+\n",
    "#             '_min_duration'+str(min_duration)+\n",
    "#             '.npy', all_continuous)    \n",
    "    \n",
    "#     ##################################\n",
    "#     ######### DISCRETIZE #############\n",
    "#     ##################################\n",
    "#     angles_discretized = np.zeros(angles.shape, 'float32')+np.nan\n",
    "#     for k in trange(angles.shape[0]):\n",
    "#         temp = angles[k]*fps*rad_to_degree\n",
    "        \n",
    "#         for a in range(len(angles_thresh)):\n",
    "#             idx = np.where(np.logical_and(\n",
    "#                                 temp>=angles_thresh[a][0],\n",
    "#                                 temp<angles_thresh[a][1],\n",
    "#                            ))[0]\n",
    "#             #temp[idx]=a\n",
    "            \n",
    "#             angles_discretized[k,idx]= a\n",
    "        \n",
    "        \n",
    "#     # discretize accelaration\n",
    "#     acc_discretized = np.zeros(acc.shape, 'float32')+np.nan\n",
    "#     for k in trange(acc.shape[0]):\n",
    "#         temp = acc[k]\n",
    "\n",
    "#         for a in range(len(acc_thresh)):\n",
    "#             idx = np.where(np.logical_and(\n",
    "#                                 temp>=acc_thresh[a][0],\n",
    "#                                 temp<acc_thresh[a][1],\n",
    "#                            ))[0]\n",
    "\n",
    "#             acc_discretized[k,idx]= a\n",
    "\n",
    "            \n",
    "#     # \n",
    "#     all_discretized = np.hstack((angles_discretized[:,2:], acc_discretized))\n",
    "    \n",
    "#     # \n",
    "#     np.save('/home/cat/all_discretized_'+str(animal_id)+\n",
    "#             '_min_duration'+str(min_duration)+\n",
    "#             '.npy', all_discretized)\n",
    "    \n",
    "    \n",
    "#     return (all_continuous,\n",
    "#             all_discretized,\n",
    "#             angles_discretized, \n",
    "#             acc_discretized, \n",
    "#             angles,\n",
    "#             acc,\n",
    "#             angles_thresh, \n",
    "#             acc_thresh)\n",
    "\n",
    "# # # \n",
    "# # def dim_red2(animal_id,\n",
    "# #              min_duration,\n",
    "# #              angles_thresh,\n",
    "# #              acc_thresh,\n",
    "# #              method,\n",
    "# #              subsample=5):\n",
    "    \n",
    "# #     #\n",
    "# #     from sklearn import decomposition\n",
    "# #     import sklearn\n",
    "    \n",
    "# #     #\n",
    "# #     root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "\n",
    "# #     fname_out = os.path.join(root_dir,\"DimRed_animalID_\"+str(animal_id)+\n",
    "# #                              \"_min_duration_\"+str(min_duration)+\n",
    "# #                              \"_method_\"+str(method)+\n",
    "# #                              \"_n_events_\"+str(len(angles_thresh))+\n",
    "# #                              '.npz'\n",
    "# #                             )\n",
    "    \n",
    "# #     #\n",
    "# #     if os.path.exists(fname_out)==False:\n",
    "\n",
    "# #         X_pca = np.zeros((angles_thresh.shape[0], \n",
    "# #                       acc_thresh.shape[1], 2), 'float32')\n",
    "# #         X_pca[:,:,0]=angles_thresh[:,2:]\n",
    "# #         X_pca[:,:,1]=acc_thresh\n",
    "# #         print (\"Xpca: \", X_pca.shape)\n",
    "\n",
    "# #         #\n",
    "# #         X_pca = X_pca.reshape(X_pca.shape[0],-1)\n",
    "# #         print (\"Xpca: \", X_pca.shape)\n",
    "\n",
    "\n",
    "# #         ###################################\n",
    "# #         ###################################\n",
    "# #         ###################################\n",
    "# #         if method==0:\n",
    "# #             pca = decomposition.PCA(n_components=3)\n",
    "\n",
    "# #             X_pca_fit = pca.fit_transform(X_pca)\n",
    "# #             print (X_pca_fit.shape)\n",
    "\n",
    "# #         elif method==1:\n",
    "# #             import umap\n",
    "# #             umap = umap.UMAP(n_components=2,\n",
    "# #                             init='random',\n",
    "# #                             random_state=0)\n",
    "\n",
    "# #             X_pca_subsampled = X_pca[::subsample]\n",
    "# #             print (\"fitting umap on\" ,X_pca_subsampled.shape)\n",
    "# #             umap_ = umap.fit(X_pca_subsampled)\n",
    "\n",
    "# #             print (\"transforming alldata \", X_pca.shape)\n",
    "# #             X_pca_fit = umap_.transform(X_pca)\n",
    "\n",
    "# #         # retransform X_pca to regular shape\n",
    "# #         X_pca = X_pca.reshape(X_pca.shape[0], -1, 2)\n",
    "    \n",
    "# #         # \n",
    "# #         np.savez(fname_out,\n",
    "# #                  X_pca_fit = X_pca_fit,\n",
    "# #                  X_pca = X_pca,\n",
    "# #                  animal_id = animal_id,\n",
    "# #                  min_duration = min_duration,\n",
    "# #                  method = method,\n",
    "# #                  subsample = subsample,\n",
    "# #                 )\n",
    "# #     else:\n",
    "# #         data = np.load(fname_out,allow_pickle=True)\n",
    "# #         X_pca_fit = data[\"X_pca_fit\"]\n",
    "# #         X_pca = data['X_pca']\n",
    "    \n",
    "    \n",
    "# #     return X_pca_fit, X_pca\n",
    "\n",
    "\n",
    "\n",
    "# # CLUSTER STEP\n",
    "# def cluster_and_visualize(X_pca_fit, \n",
    "#                           X_pca, \n",
    "#                           n_clusters=2, \n",
    "#                           cluster_id = None):\n",
    "    \n",
    "#     #clrs = ['black','blue','red','green', 'magenta','brown','cyan','yellow',\n",
    "#     #        'pink','olive']\n",
    "       \n",
    "#     import matplotlib.cm as cm\n",
    "#     from matplotlib.colors import Normalize\n",
    "\n",
    "        \n",
    "#     # use GaussianMixture clusters\n",
    "#     if n_clusters is not None:\n",
    "#         from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#         gmm = GaussianMixture(n_components=n_clusters).fit(X_pca_fit)\n",
    "#         labels = gmm.predict(X_pca_fit)\n",
    "#         print(np.unique(labels))\n",
    "#           # PLOT STEP\n",
    "#         n_clusters = np.unique(labels).shape[0]\n",
    "#         print (\"n_clusters: \", n_clusters, np.unique(labels))\n",
    "#         colors = plt.cm.tab20(np.linspace(0,1,20))\n",
    "\n",
    "#     # use metadata\n",
    "#     else:\n",
    "#         labels = []\n",
    "# #         for k in trange(X_pca.shape[0], desc='Making colors'):\n",
    "# #             #print (X_pca[k].shape)\n",
    "# #             temp = X_pca[k]\n",
    "# #             ave_angle = np.median(temp[:,0])\n",
    "# #             ave_acc = np.median(temp[:,1])\n",
    "            \n",
    "# #             labels.append(ave_angle+ave_acc*3)\n",
    "    \n",
    "            \n",
    "# #         labels = np.int32(labels)\n",
    "# #         print (\"labels: \", labels)\n",
    "  \n",
    "#         #  \n",
    "#         if True:\n",
    "#             medians = np.mean(X_pca, axis=1)\n",
    "#             print (\"means: \", medians.shape)\n",
    "            \n",
    "#             labels = np.log(medians[:,0]+medians[:,1]*3)\n",
    "#             print (\"Labels log: \", labels)\n",
    "                        \n",
    "#             labels = (labels-np.min(labels))/(np.max(labels)-np.min(labels))\n",
    "#             print (\"Labels norm: \", labels)\n",
    "            \n",
    "#             labels = np.abs(np.log(labels+0.000001))\n",
    "#             print (\"labels: \", labels)\n",
    "            \n",
    "#             labels = (labels-np.min(labels))/(np.max(labels)-np.min(labels))\n",
    "#             print (\"labels: \", labels)\n",
    "            \n",
    "#             labels = np.around(labels, 1)\n",
    "#             labels = 1-labels\n",
    "#             print (\"Final labels: \", labels)\n",
    "            \n",
    "#             cmap = cm.viridis\n",
    "\n",
    "#             colors = plt.cm.viridis(np.linspace(0,1,100))\n",
    "\n",
    "#             ctr=0\n",
    "#             for k in np.unique(labels):\n",
    "#                 plt.subplot(2,3,ctr+1)\n",
    "\n",
    "#                 idx = np.where(labels==k)[0]\n",
    "#                 print (\"k: \", k, idx.shape)\n",
    "#                 plt.scatter(X_pca_fit[idx,0],\n",
    "#                             X_pca_fit[idx,1],\n",
    "#                             alpha=.1,\n",
    "#                             edgecolor='black',\n",
    "#                             #color = colors[labels]\n",
    "#                             color = cmap(labels[idx])\n",
    "#                             #color = colors[labels[idx]]\n",
    "#                        )\n",
    "#                 ctr+=1\n",
    "#                 plt.xlim(-15,25)\n",
    "#                 plt.ylim(-20,30)\n",
    "\n",
    "\n",
    "#             return idx, labels\n",
    "        \n",
    "        \n",
    "#         else: \n",
    "#             medians = np.median(X_pca, axis=1)\n",
    "#             labels = np.log(medians[:,0]+medians[:,1]*3)\n",
    "#             print (\"labels: \", labels, np.unique(labels))\n",
    "#             labels = (labels-np.min(labels))/(np.max(labels)-np.min(labels))\n",
    "#             print (\"labels: \", labels, np.unique(labels))\n",
    "#             labels = np.int32(labels*99)\n",
    "\n",
    "#             cmap = cm.tab20c\n",
    "#             colors = plt.cm.viridis(np.linspace(0,1,100))\n",
    "#             #norm = Normalize(vmin=np.min(labels), vmax=np.max(labels))\n",
    "            \n",
    "#             fig=plt.figure()\n",
    "#             ctr=0\n",
    "#             for k in np.unique(labels):\n",
    "#                 plt.subplot(2,3,ctr+1)\n",
    "\n",
    "#                 idx = np.where(labels==k)[0]\n",
    "#                 plt.scatter(X_pca_fit[idx,0],\n",
    "#                         X_pca_fit[idx,1],\n",
    "#                         alpha=.1,\n",
    "#                         edgecolor='black',\n",
    "#                         #color = colors[labels]\n",
    "#                         #color = cmap(labels)\n",
    "#                         color = colors[labels[idx]]\n",
    "#                        )\n",
    "#                 ctr+=1\n",
    "#                 plt.xlim(-15,25)\n",
    "#                 plt.ylim(-20,30)\n",
    "            \n",
    "#             return idx, labels\n",
    "\n",
    "#     # \n",
    "#     if cluster_id is None:\n",
    "#         fig=plt.figure()\n",
    "\n",
    "#         for id_ in np.unique(labels):\n",
    "#             idx = np.where(labels==id_)[0]\n",
    "#             print (\"cluster: \", id_, \" has # events: \", idx.shape[0])\n",
    "\n",
    "#             plt.scatter(X_pca_fit[idx,0],\n",
    "#                         X_pca_fit[idx,1],\n",
    "#                         alpha=.1,\n",
    "#                         edgecolor='black',\n",
    "#                         color = colors[labels[idx]]\n",
    "#                        )\n",
    "\n",
    "#     # \n",
    "#     print (\"PLOTTING...\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     return idx, labels\n",
    "\n",
    "\n",
    "               \n",
    "# # \n",
    "\n",
    "\n",
    "# def get_normalized_histograms(angles_discretized,\n",
    "#                               acc_discretized):\n",
    "    \n",
    "#     print (\"COMPUTING Klaus normalized histograms\")\n",
    "#     #\n",
    "#     bins = np.arange(np.unique(angles_discretized).shape[0]+1)-0.5\n",
    "#     print (\"BINS angles: \", bins)\n",
    "#     angles_hist = np.zeros((angles_discretized.shape[0], \n",
    "#                              bins.shape[0]-1), 'float32')\n",
    "#     for k in trange(angles_discretized.shape[0]):\n",
    "#         angles_hist[k] = np.histogram(angles_discretized[k], bins = bins)[0]\n",
    "   \n",
    "#     # normalize\n",
    "#     angles_hist = angles_hist / np.sum(angles_hist[0])\n",
    "\n",
    "#     #\n",
    "#     bins = np.arange(np.unique(acc_discretized).shape[0]+1)-0.5\n",
    "#     print (\"BINS accel: \", bins)\n",
    "#     acc_hist = np.zeros((acc_discretized.shape[0], \n",
    "#                              bins.shape[0]-1), 'float32')\n",
    "#     for k in trange(acc_discretized.shape[0]):\n",
    "#         acc_hist[k] = np.histogram(acc_discretized[k], bins = bins)[0]\n",
    "      \n",
    "#     # normalize\n",
    "#     acc_hist = acc_hist / np.sum(acc_hist[0])\n",
    "    \n",
    "#     #\n",
    "#     print (\"Angles histograms: \", angles_hist.shape)\n",
    "#     print (\"Accel histograms: \", acc_hist.shape)\n",
    "#     print (\"\")\n",
    "    \n",
    "#     return  angles_hist, acc_hist\n",
    "\n",
    "\n",
    "# def get_unique_angles_accs(angles_hist, acc_hist):\n",
    "#     # \n",
    "#     ang_unique = np.unique(angles_hist, axis=0)\n",
    "#     print (\"Unique angles: \", ang_unique.shape)\n",
    "\n",
    "#     acc_hist = np.around(acc_hist, 3)\n",
    "#     acc_unique = np.unique(acc_hist, axis=0)\n",
    "#     print (\"Unique accel: \", acc_unique.shape)\n",
    "\n",
    "#     # \n",
    "#     all_hist = np.hstack((angles_hist, acc_hist))\n",
    "#     all_unique = np.unique(all_hist, axis=0)\n",
    "\n",
    "#     print (\"# possible combinations: \", acc_unique.shape[0]*ang_unique.shape[0])\n",
    "#     print (\"# of actual combinations: \", all_unique.shape)\n",
    "    \n",
    "    \n",
    "#     print (\"normalized angles histograms: \", angles_hist[50:52])\n",
    "#     print (\"ang unique: \", ang_unique[:2])\n",
    "#     print (\"acc unique: \", acc_unique[:2])\n",
    "\n",
    "#     return ang_unique, acc_unique, all_unique\n",
    "\n",
    "# # \n",
    "# def compute_emd_normalized_unique_angle_acc(ang_unique, acc_unique):\n",
    "    \n",
    "#     # \n",
    "#     emd_angle=np.zeros((ang_unique.shape[0],ang_unique.shape[0]))\n",
    "#     for i in range(ang_unique.shape[0]):\n",
    "#         for p in range(i+1, ang_unique.shape[0],1):\n",
    "#             emd_angle[i,p] = scipy.stats.wasserstein_distance(ang_unique[i], \n",
    "#                                                               ang_unique[p])\n",
    "            \n",
    "#     # normalize by n-1\n",
    "#     emd_angle = emd_angle / (ang_unique.shape[1]-1)\n",
    "\n",
    "#     #\n",
    "#     emd_acc=np.zeros((acc_unique.shape[0],acc_unique.shape[0]))\n",
    "#     for i in range(acc_unique.shape[0]):\n",
    "#         for p in range(i+1, acc_unique.shape[0],1):\n",
    "#             emd_acc[i,p] = scipy.stats.wasserstein_distance(acc_unique[i], \n",
    "#                                                             acc_unique[p])\n",
    "            \n",
    "#     # normalize by n-1\n",
    "#     emd_acc = emd_acc / (acc_unique.shape[1]-1)\n",
    "    \n",
    "    \n",
    "#     return emd_angle, emd_acc\n",
    "\n",
    "\n",
    "# def get_index(array, feature_array):\n",
    "    \n",
    "#     idx = np.sum(np.equal(feature_array, array), axis=1)\n",
    "#     idx = np.where(idx==len(array))[0]\n",
    "\n",
    "#     return idx\n",
    "\n",
    "\n",
    "# def compute_S_matrix_parallel(indexes,\n",
    "#                                all_unique, \n",
    "#                                ang_unique, \n",
    "#                                acc_unique, \n",
    "#                                n_split,\n",
    "#                                min_duration, \n",
    "#                                emd_angle_unique_norm, \n",
    "#                                emd_acc_unique_norm):\n",
    "\n",
    "#     root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "\n",
    "#     fname_out = os.path.join(root_dir,\"S_unique\",\n",
    "#                              \"minDuration\"+str(min_duration)+\"_S_unique_index\"+str(indexes[0])+\"_nsplit_\"+str(n_split)+'.npz')\n",
    "    \n",
    "#     if os.path.exists(fname_out)==False:\n",
    "        \n",
    "#         # \n",
    "# #         (emd_angle_unique_norm, \n",
    "# #          emd_acc_unique_norm) = compute_emd_normalized_unique_angle_acc(ang_unique, \n",
    "# #                                                                         acc_unique)\n",
    "\n",
    "#         #\n",
    "#         S_ang = np.zeros((len(indexes),all_unique.shape[0]),'float32') #+np.nan\n",
    "#         S_acc = np.zeros((len(indexes),all_unique.shape[0]),'float32') #+np.nan\n",
    "\n",
    "#         #for p in trange(all_unique.shape[0]):\n",
    "#         for ctrp, p in enumerate(indexes):\n",
    "\n",
    "#             # find outer loop all feature match to ang unique features\n",
    "#             idx1_ang = get_index(all_unique[p,:3], \n",
    "#                                    ang_unique)\n",
    "#             # same for acceleration\n",
    "#             idx1_acc = get_index(all_unique[p,3:], \n",
    "#                                    acc_unique)\n",
    "\n",
    "#             # loop over inner dimensions\n",
    "#             for q in range(p+1, all_unique.shape[0],1):\n",
    "\n",
    "#                 # get index of current frame for angle\n",
    "#                 idx2_ang = get_index(all_unique[q,:3], \n",
    "#                                      ang_unique)\n",
    "#                 # \n",
    "#                 S_ang[ctrp,q] = emd_angle_unique_norm[idx1_ang,idx2_ang] \n",
    "\n",
    "#                 # get index for acceleration\n",
    "#                 idx2_acc = get_index(all_unique[q,3:], \n",
    "#                                      acc_unique)\n",
    "\n",
    "#                 # print (idx1_ang,idx2_ang,idx1_acc,idx2_acc )\n",
    "#                 # \n",
    "#                 S_acc[ctrp,q] = emd_acc_unique_norm[idx1_acc,idx2_acc] \n",
    "\n",
    "#         # compute sum and normalize\n",
    "#         S = -((S_ang + S_acc)/2)**2\n",
    "\n",
    "#         # \n",
    "#         np.savez(fname_out,\n",
    "#                  S = S,\n",
    "#                  indexes = indexes)\n",
    "         \n",
    "    \n",
    "\n",
    "\n",
    "# def reconstruct_S_unique_from_files(all_unique, min_duration):\n",
    "\n",
    "#     root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "#     path = os.path.join(root_dir, \"S_unique\", \n",
    "#                         \"minDuration\"+str(min_duration)+\"_S_unique*.npz\")\n",
    "#     print (\"path: \", path)\n",
    "#     fnames = glob.glob(path)\n",
    "#     print (\"Fnames: \", fnames)\n",
    "\n",
    "   \n",
    "#     #S_unique = np.zeros((all_unique.shape[0],all_unique.shape[0]),'float32') #+np.nan\n",
    "#     S_unique = np.zeros((all_unique.shape[0],\n",
    "#                          all_unique.shape[0]),'float32') #+np.nan\n",
    "\n",
    "#     for fname in tqdm(fnames):\n",
    "#         # \n",
    "#         data = np.load(fname, allow_pickle=True)\n",
    "        \n",
    "#         # \n",
    "#         S = data['S']\n",
    "#         indexes = data['indexes']\n",
    "        \n",
    "#         #\n",
    "#         S_unique[indexes] = S\n",
    "\n",
    "#     #     \n",
    "#     S_unique_symmetric = S_unique + S_unique.T - np.diag(np.diag(S_unique))\n",
    "     \n",
    "#     return S_unique_symmetric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######## PREPROCESS DATA #############\n",
    "######################################\n",
    "animal_id = 0\n",
    "min_duration = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... done\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "######### GET PCA DATA ###########\n",
    "##################################\n",
    "\n",
    "# \n",
    "(all_continuous,\n",
    "all_discretized,\n",
    "angles_discretized, \n",
    "acc_discretized, \n",
    "angles,\n",
    "acc,\n",
    "angles_thresh, \n",
    "acc_thresh) =  compute_discretized_angles_and_acceleration(animal_id, \n",
    "                                                           min_duration)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = np.load('/media/cat/1TB/dan/cohort1/slp/alldata_fixed.npy')\n",
    "\n",
    "# print (d.shape)\n",
    "# for k in range(4):\n",
    "#     np.save('/media/cat/1TB/dan/cohort1/slp/animalID_'+str(k)+\"_alldata_fixed.npy\",\n",
    "#            d[:,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTING Klaus normalized histograms\n",
      "BINS angles:  [-0.5  0.5  1.5  2.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 264580/264580 [00:06<00:00, 42706.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINS accel:  [-0.5  0.5  1.5  2.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 264580/264580 [00:06<00:00, 43815.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angles histograms:  (264580, 3)\n",
      "Accel histograms:  (264580, 3)\n",
      "\n",
      "Unique angles:  (35, 3)\n",
      "Unique accel:  (28, 3)\n",
      "# possible combinations:  980\n",
      "# of actual combinations:  (518, 6)\n",
      "normalized angles histograms:  [[0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "ang unique:  [[0.    0.    1.   ]\n",
      " [0.    0.125 0.875]]\n",
      "acc unique:  [[0.    0.    1.   ]\n",
      " [0.    0.167 0.833]]\n",
      "all_unique:  (518, 6)\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "######## COMPUTE HISTOGRAMS ##########\n",
    "######################################\n",
    "#\n",
    "angles_hist, acc_hist = get_normalized_histograms(angles_discretized,\n",
    "                                                  acc_discretized)\n",
    "\n",
    "\n",
    "def get_unique_angles_accs(angles_hist, acc_hist):\n",
    "    # \n",
    "    ang_unique = np.unique(angles_hist, axis=0)\n",
    "    print (\"Unique angles: \", ang_unique.shape)\n",
    "\n",
    "    acc_hist = np.around(acc_hist, 3)\n",
    "    acc_unique = np.unique(acc_hist, axis=0)\n",
    "    print (\"Unique accel: \", acc_unique.shape)\n",
    "\n",
    "    # \n",
    "    all_hist = np.hstack((angles_hist, acc_hist))\n",
    "    all_unique = np.unique(all_hist, axis=0)\n",
    "\n",
    "    print (\"# possible combinations: \", acc_unique.shape[0]*ang_unique.shape[0])\n",
    "    print (\"# of actual combinations: \", all_unique.shape)\n",
    "    \n",
    "    \n",
    "    print (\"normalized angles histograms: \", angles_hist[50:52])\n",
    "    print (\"ang unique: \", ang_unique[:2])\n",
    "    print (\"acc unique: \", acc_unique[:2])\n",
    "\n",
    "    return ang_unique, acc_unique, all_unique\n",
    "\n",
    "#\n",
    "ang_unique, acc_unique, all_unique = get_unique_angles_accs(angles_hist, \n",
    "                                                            acc_hist)\n",
    "\n",
    "print (\"all_unique: \", all_unique.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    1.    0.    0.    1.   ]\n",
      " [0.    0.    1.    0.167 0.167 0.667]\n",
      " [0.    0.    1.    0.333 0.5   0.167]\n",
      " ...\n",
      " [1.    0.    0.    0.167 0.167 0.667]\n",
      " [1.    0.    0.    0.333 0.333 0.333]\n",
      " [1.    0.    0.    0.667 0.    0.333]]\n"
     ]
    }
   ],
   "source": [
    "print (all_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 3)\n",
      "(28, 3)\n",
      "(518, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [00:00<00:00, 507170.98it/s]\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function compute_S_matrix_parallel at 0x7f46171fcd30>: it's not the same object as klaus_emd_tools.compute_S_matrix_parallel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6fcf100f9898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mn_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mindexes_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_unique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[:32]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     parmap.map(compute_S_matrix_parallel, indexes_split,\n\u001b[0m\u001b[1;32m     19\u001b[0m                \u001b[0mall_unique\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                \u001b[0mang_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/parmap/parmap.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(function, iterable, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m        \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpm_pbar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_map_or_starmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/parmap/parmap.py\u001b[0m in \u001b[0;36m_map_or_starmap\u001b[0;34m(function, iterable, args, kwargs, map_or_starmap)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0m_do_pbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclose_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/umap/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/umap/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    535\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                         \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/umap/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/umap/lib/python3.8/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function compute_S_matrix_parallel at 0x7f46171fcd30>: it's not the same object as klaus_emd_tools.compute_S_matrix_parallel"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "######## GENERATE S MATRIX ########\n",
    "###################################\n",
    "#\n",
    "print (ang_unique.shape)\n",
    "print (acc_unique.shape)\n",
    "print (all_unique.shape)      \n",
    "\n",
    "\n",
    "(emd_angle_unique_norm, \n",
    "   emd_acc_unique_norm) = compute_emd_normalized_unique_angle_acc(ang_unique, acc_unique)\n",
    "\n",
    "\n",
    "#     \n",
    "if True:\n",
    "    n_split = 200\n",
    "    indexes_split = np.array_split(np.arange(all_unique.shape[0]),n_split)#[:32]\n",
    "    parmap.map(compute_S_matrix_parallel, indexes_split,\n",
    "               all_unique, \n",
    "               ang_unique, acc_unique, n_split,\n",
    "               min_duration, \n",
    "               emd_angle_unique_norm, \n",
    "               emd_acc_unique_norm,\n",
    "               pm_processes= 8,\n",
    "               pm_pbar = True)\n",
    "      \n",
    "else:\n",
    "    res = []\n",
    "    for k in range(all_unique.shape[0]):\n",
    "        res.append(compute_S_matrix_parallel([k], all_unique, ang_unique, acc_unique,\n",
    "                                   min_duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5089, 6)\n",
      "path:  /media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique*.npz\n",
      "Fnames:  ['/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1482_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4014_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4039_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index442_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3139_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1222_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1846_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1300_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3889_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4289_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1924_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1794_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4714_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index312_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1066_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2028_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1170_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1534_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1144_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index78_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1092_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2236_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1638_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4864_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index416_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index832_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index650_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4389_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3914_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4989_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4264_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2514_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4414_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4339_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1872_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3364_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index5064_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3414_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2639_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2314_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1248_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2739_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4614_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3389_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4064_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3264_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4514_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2764_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index936_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index130_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index5039_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1612_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3764_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index26_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2414_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4739_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index728_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1274_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2339_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4139_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index962_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3989_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index234_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1768_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1820_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index104_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1716_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2689_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3214_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index156_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1378_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2002_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index676_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index286_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2864_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2989_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4964_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1040_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1560_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1508_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4639_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3714_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1690_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4939_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index598_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1898_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index390_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2439_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2158_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2914_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2939_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3089_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3689_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1352_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3339_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4314_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1196_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3114_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3664_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4814_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4214_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1014_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2132_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2210_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4114_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3814_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index338_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3489_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2489_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3789_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2814_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2184_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1456_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1664_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index52_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index884_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3314_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2614_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index624_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3014_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index572_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2714_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3839_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3289_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4189_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3939_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index702_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index910_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1742_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1404_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3039_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3464_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1326_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index208_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4839_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3539_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3739_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1976_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2789_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2054_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2262_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3514_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3589_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4489_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4239_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3564_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4464_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2889_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2288_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2539_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4689_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4789_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3189_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3639_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index988_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2839_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index780_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index260_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3239_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2106_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2389_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4889_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2080_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4364_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index520_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4164_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1430_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1950_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2464_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1586_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4664_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2589_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3964_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4089_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3864_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index1118_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index754_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4564_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index182_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index858_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3164_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3614_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index806_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3439_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2364_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index0_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index494_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index5014_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2664_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index3064_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index364_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4439_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2564_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4914_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4539_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4589_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index468_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index546_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index4764_nsplit_200.npz', '/media/cat/1TB/dan/cohort1/slp/S_unique/minDuration15_S_unique_index2964_nsplit_200.npz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [00:00<00:00, 873.81it/s]\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "# \n",
    "min_duration = 15\n",
    "print (all_unique.shape)\n",
    "\n",
    "S_uniq_sym = reconstruct_S_unique_from_files(all_unique, min_duration)\n",
    "    \n",
    "plt.imshow(S_uniq_sym, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201 200 199 ... 201 201 201]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "import numpy as np\n",
    "\n",
    "X = S_uniq_sym\n",
    "\n",
    "clustering = AffinityPropagation(random_state=5).fit(X)\n",
    "\n",
    "print (clustering.labels_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Y: \n",
      "Computing Z: \n"
     ]
    }
   ],
   "source": [
    "# Compute and plot dendrogram.\n",
    "import scipy.cluster.hierarchy as sch\n",
    "fig = plt.figure()\n",
    "axdendro = fig.add_axes([0.09,0.1,0.2,0.8])\n",
    "\n",
    "# \n",
    "D = S_uniq_sym\n",
    "\n",
    "# \n",
    "print (\"Computing Y: \")\n",
    "Y = sch.linkage(D, method='centroid')\n",
    "\n",
    "print (\"Computing Z: \")\n",
    "Z = sch.dendrogram(Y, orientation='right')\n",
    "axdendro.set_xticks([])\n",
    "axdendro.set_yticks([])\n",
    "\n",
    "# Plot distance matrix.\n",
    "axmatrix = fig.add_axes([0.3,0.1,0.6,0.8])\n",
    "index = Z['leaves']\n",
    "D = D[index,:]\n",
    "D = D[:,index]\n",
    "im = axmatrix.matshow(D, aspect='auto', origin='lower')\n",
    "axmatrix.set_xticks([])\n",
    "axmatrix.set_yticks([])\n",
    "\n",
    "# Plot colorbar.\n",
    "axcolor = fig.add_axes([0.91,0.1,0.02,0.8])\n",
    "plt.colorbar(im, cax=axcolor)\n",
    "\n",
    "# Display and save figure.\n",
    "fig.show()\n",
    "fig.savefig('dendrogram.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5088, 4)\n"
     ]
    }
   ],
   "source": [
    "print (Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89985, 4, 14, 2)\n"
     ]
    }
   ],
   "source": [
    "d = np.load('/media/cat/1TB/dan/cohort1/slp/2020_3_16_07_55_55_775234_compressed.npy')\n",
    "print (d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "######## SUBSAMPLE DATA ###########\n",
    "#########################################\n",
    "# # \n",
    "\n",
    "# def compute_S_matrix2(all_histograms, subsample=10):\n",
    "    \n",
    "#     all_hist_sub = all_histograms[::subsample]\n",
    "    \n",
    "#     print (\"all hist sub: \", all_hist_sub.shape)\n",
    "    \n",
    "#     s_matrix = np.zeros((all_hist_sub.shape[0], all_hist_sub.shape[0]), dtype=np.float32)\n",
    "#     for i in trange(all_hist_sub.shape[0]):\n",
    "#             for p in range(i+1, all_hist_sub.shape[0],1):\n",
    "#                 s_matrix[i,p] = scipy.stats.wasserstein_distance(all_hist_sub[i], \n",
    "#                                                                  all_hist_sub[p])\n",
    "\n",
    "#     ## normalize by n-1\n",
    "#     #emd_angle = emd_angle / ang_unique.shape[1]\n",
    "\n",
    "#     return s_matrix\n",
    "    \n",
    "# print (all_histograms.shape)\n",
    "\n",
    "# s_matrix = compute_S_matrix2(all_histograms, subsample = 50)\n",
    "\n",
    "# print (\"S_matrix: \", s_matrix.shape)\n",
    "# # (emd_angle_unique_norm, \n",
    "# #  emd_acc_unique_norm) = compute_emd_normalized_unique_angle_acc(ang_unique, \n",
    "# #                                                                 acc_unique)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting umap on (5089, 5089)\n",
      "noise mean:  9.998707467602347e-06\n",
      "noise:  (5089, 5089)\n",
      "(5089, 2)\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "#data = np.load('/home/cat/S_uniq_sym.npy')\n",
    "#X_subsampled = data\n",
    "\n",
    "X_subsampled = S_uniq_sym.copy()\n",
    "\n",
    "print (\"fitting umap on\" ,X_subsampled.shape)\n",
    "if True:\n",
    "    noise = np.random.rand(X_subsampled.shape[0], X_subsampled.shape[1])/50000.\n",
    "    print (\"noise mean: \", noise.mean())\n",
    "\n",
    "    #for k in range(6):\n",
    "    #    plt.subplot(2,3,k+1)\n",
    "    #    \n",
    "    #    \n",
    "    #    \n",
    "    print (\"noise: \", noise.shape)\n",
    "    X_subsampled += noise\n",
    "\n",
    "umap = umap.UMAP(n_components=2,\n",
    "                 n_neighbors=20,\n",
    "                 min_dist = 0.1,\n",
    "                 init='random',\n",
    "                 random_state=0)\n",
    "\n",
    "# \n",
    "X_fit = umap.fit_transform(X_subsampled)\n",
    "\n",
    "# \n",
    "print (X_fit.shape)\n",
    "plt.scatter(X_fit[:,0], X_fit[:,1],\n",
    "            s=10,\n",
    "            alpha=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "############################################\n",
    "############################################\n",
    "x_flat = X_subsampled.flatten()\n",
    "\n",
    "fig = plt.figure()\n",
    "y = np.histogram(x_flat, bins = np.arange(-.05,0,0.001))\n",
    "plt.plot(y[1][1:],y[0])\n",
    "#plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17355, 2)\n"
     ]
    }
   ],
   "source": [
    "X_fit = np.load('/home/cat/S_uniqu_sym_X_fit_0.1_200_original_17355Frames.npy')\n",
    "print (X_fit.shape)\n",
    "\n",
    "plt.scatter(X_fit[:,0], X_fit[:,1],\n",
    "            s=10,\n",
    "            alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/cat/S_uniqu_sym_X_fit_0.1_200.npy', X_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len  39\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/home/cat/cluster.npz', allow_pickle=True)\n",
    "locs = data['cluster_locs']\n",
    "print (\"len \", len(locs))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax=plt.subplot(121)\n",
    "idx = np.random.choice(np.arange(len(locs)), len(locs), replace=False)\n",
    "for ctr,k in enumerate(idx):\n",
    "    plt.scatter(locs[k][:,0],\n",
    "                locs[k][:,1],\n",
    "                color = 'blue')\n",
    "\n",
    "ax=plt.subplot(122)\n",
    "\n",
    "colors = plt.cm.gist_ncar(np.linspace(0,1,len(locs)))\n",
    "idx = np.random.choice(np.arange(len(locs)), len(locs), replace=False)\n",
    "for ctr,k in enumerate(idx):\n",
    "    plt.scatter(locs[k][:,0],\n",
    "                locs[k][:,1],\n",
    "                color = colors[ctr])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1:  (5449, 2)\n",
      "X2:  (5449, 5449)\n",
      "Converged after 96 iterations.\n",
      "FINISHED:  [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "import numpy as np\n",
    "\n",
    "# \n",
    "X1 = X_fit[::5] #[:30*18]\n",
    "print (\"X1: \", X1.shape)\n",
    "\n",
    "X2 = scipy.spatial.distance.cdist(X1, X1, metric='cosine')\n",
    "print (\"X2: \", X2.shape)\n",
    "\n",
    "clustering = AffinityPropagation(affinity='precomputed', \n",
    "                                 preference=-1.1,\n",
    "                                 random_state=5, \n",
    "                                 verbose=True).fit(X2)\n",
    "\n",
    "#\n",
    "print (\"FINISHED: \", np.unique(clustering.labels_))\n",
    "\n",
    "# \n",
    "fig = plt.figure()\n",
    "plt.scatter(X_fit[:,0], X_fit[:,1],\n",
    "            c='red',\n",
    "            edgecolor='black',\n",
    "            alpha=.1)\n",
    "\n",
    "labels = clustering.labels_\n",
    "plt.scatter(X1[:,0], X1[:,1],\n",
    "            c=labels,\n",
    "            edgecolor='black',\n",
    "            alpha=1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# print (\"all data: \", all_unique.shape)\n",
    "\n",
    "# # \n",
    "# def cleanup_data(all_unique, min_occurance = 10, max_occurance = 1E4):\n",
    "\n",
    "#     all_clean = all_unique.copy()\n",
    "    \n",
    "#     frames_unique, indexes, counts_all  = np.unique(all_unique, axis=0, \n",
    "#                                                 return_counts=True, \n",
    "#                                                 return_index=True)\n",
    "    \n",
    "#     # \n",
    "#     print (\"uinque feature vectors: \", frames_unique.shape, \" Counts: \", counts_all.shape, \"  all indexes: \", indexes.shape)\n",
    "#     print (\"unique features: \", frames_unique)\n",
    "#     print (\"counts_all:\", counts_all)\n",
    "#     print (\"indexes: \", indexes)\n",
    "    \n",
    "#     # \n",
    "#     cts_unique = np.unique(counts_all)#[::-1]\n",
    "#     print (\" unique counts: \", cts_unique)\n",
    "\n",
    "#     ##################################\n",
    "#     ##################################\n",
    "#     ##################################\n",
    "#     for c in cts_unique:\n",
    "#         idx3 = np.where(counts==c)[0]\n",
    "#         n_counts.append(idx3.shape[0])\n",
    "\n",
    "#         if c<min_occurance or c>max_occurance:\n",
    "#             id4 = np.where(counts_all==c)[0]\n",
    "#             print (\" # of unique frames that occur \", c, \" times is: \", idx3.shape)\n",
    "#             #print (\"ID4: \", id4)\n",
    "            \n",
    "#             for id_ in id4:\n",
    "#                 frame = frames_unique[id_]\n",
    "\n",
    "#                 id4 = np.sum(np.equal(all_clean, frame),axis=1)\n",
    "#                 id4 = np.where(id4==len(frame))[0]\n",
    "\n",
    "#                 all_clean = np.delete(all_clean,id4,axis=0)\n",
    "\n",
    "\n",
    "#     ##################################\n",
    "#     ##################################\n",
    "#     ################################## \n",
    "#     frames_unique, indexes, counts_all  = np.unique(all_clean, axis=0, \n",
    "#                                                     return_counts=True, \n",
    "#                                                     return_index=True)\n",
    "    \n",
    "#     # \n",
    "#     print (\"uinque feature vectors: \", frames_unique.shape, \" Counts: \", counts_all.shape, \"  all indexes: \", indexes.shape)\n",
    "#     print (\"unique features: \", frames_unique)\n",
    "#     print (\"counts_all:\", counts_all)\n",
    "#     print (\"indexes: \", indexes)\n",
    "    \n",
    "#     # \n",
    "#     cts_unique = np.unique(counts_all)#[::-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # \n",
    "#     return all_clean, cts_unique\n",
    "\n",
    "# ##############################################\n",
    "# ##############################################\n",
    "# ##############################################\n",
    "# min_occurance = 1\n",
    "# max_occurance = 100\n",
    "# all_clean, cts_unique = cleanup_data(all_unique, min_occurance, max_occurance)\n",
    "\n",
    "# print(\"ALL CLENA: \", all_clean.shape)\n",
    "\n",
    "# plt.plot(cts_unique)\n",
    "# plt.xlabel(\"Number of frames\")\n",
    "# plt.ylabel(\"Number of occurances\")\n",
    "# #plt.xlim(left=-100)\n",
    "# plt.ylim(bottom=1)\n",
    "# plt.semilogy()\n",
    "# plt.semilogx()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDM CALCULATIONS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ang_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dc8d6eacc0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m######## COMPUTE EMD ON UNIQUE FEATURE WISE PAIRS ########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m##########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0memd_angle_unique_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memd_acc_unique_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_emd_normalized_unique_angle_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mang_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_unique\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ang_unique' is not defined"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "######## COMPUTE EMD ON UNIQUE FEATURE WISE PAIRS ########\n",
    "########################################################## \n",
    "emd_angle_unique_norm, emd_acc_unique_norm = compute_emd_normalized_unique_angle_acc(ang_unique, acc_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(325, 3)\n",
      "(706, 4)\n",
      "(17355, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "203it [23:53,  7.06s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17355, 17355)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cat/anaconda3/envs/gerbil/lib/python3.6/site-packages/matplotlib/image.py:446: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)\n",
      "/home/cat/anaconda3/envs/gerbil/lib/python3.6/site-packages/matplotlib/image.py:453: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_min = np.float64(newmin)\n",
      "/home/cat/anaconda3/envs/gerbil/lib/python3.6/site-packages/matplotlib/image.py:458: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_max = np.float64(newmax)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [00:03<00:00, 60.03it/s]\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "######## RECONSTRUCT AND SYMMETRIZE S_UNIQUE #######\n",
    "####################################################\n",
    "\n",
    "def reconstruct_S_unique(res, all_unique, indexes_split):\n",
    "\n",
    "    root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "    fname_out = os.path.join(root_dir, \"S_unique_symmetric.npy\")\n",
    "    \n",
    "    S_unique = np.zeros((all_unique.shape[0],all_unique.shape[0]),'float32') #+np.nan\n",
    "\n",
    "    for k in range(len(indexes_split)):\n",
    "        S_unique[indexes_split[k]] = res[k]\n",
    "\n",
    "    print (S_unique.shape)\n",
    "\n",
    "    S_unique_symmetric = S_unique + S_unique.T - np.diag(np.diag(S_unique))\n",
    "\n",
    "    np.save(fname_out, S_unique_symmetric)\n",
    "    \n",
    "    return S_unique_symmetric\n",
    "\n",
    "\n",
    "def reconstruct_S_unique_from_files(all_unique):\n",
    "\n",
    "    root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "    fnames = glob.glob(os.path.join(root_dir, \"S_unique\")+\"/*.npz\")\n",
    "    \n",
    "   \n",
    "    #S_unique = np.zeros((all_unique.shape[0],all_unique.shape[0]),'float32') #+np.nan\n",
    "    S_unique = np.zeros((17355,\n",
    "                         17355),'float32') #+np.nan\n",
    "\n",
    "    for fname in tqdm(fnames):\n",
    "        # \n",
    "        data = np.load(fname, allow_pickle=True)\n",
    "        \n",
    "        # \n",
    "        S = data['S']\n",
    "        indexes = data['indexes']\n",
    "        \n",
    "        #\n",
    "        S_unique[indexes] = S\n",
    "\n",
    "    #     \n",
    "    S_unique_symmetric = S_unique + S_unique.T - np.diag(np.diag(S_unique))\n",
    "     \n",
    "    return S_unique_symmetric\n",
    "\n",
    "\n",
    "# \n",
    "all_unique = None\n",
    "S_uniq_sym = reconstruct_S_unique_from_files(all_unique)\n",
    "\n",
    "    \n",
    "plt.imshow(S_uniq_sym, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# clusters:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315]\n",
      "0 idx:  (25,)\n",
      "1 idx:  (47,)\n",
      "2 idx:  (58,)\n",
      "3 idx:  (56,)\n",
      "4 idx:  (58,)\n",
      "5 idx:  (48,)\n",
      "6 idx:  (57,)\n",
      "7 idx:  (98,)\n",
      "8 idx:  (87,)\n",
      "9 idx:  (102,)\n",
      "10 idx:  (56,)\n",
      "11 idx:  (71,)\n",
      "12 idx:  (47,)\n",
      "13 idx:  (55,)\n",
      "14 idx:  (107,)\n",
      "15 idx:  (94,)\n",
      "16 idx:  (43,)\n",
      "17 idx:  (29,)\n",
      "18 idx:  (55,)\n",
      "19 idx:  (65,)\n",
      "20 idx:  (43,)\n",
      "21 idx:  (34,)\n",
      "22 idx:  (65,)\n",
      "23 idx:  (15,)\n",
      "24 idx:  (27,)\n",
      "25 idx:  (67,)\n",
      "26 idx:  (100,)\n",
      "27 idx:  (57,)\n",
      "28 idx:  (21,)\n",
      "29 idx:  (38,)\n",
      "30 idx:  (77,)\n",
      "31 idx:  (51,)\n",
      "32 idx:  (103,)\n",
      "33 idx:  (85,)\n",
      "34 idx:  (46,)\n",
      "35 idx:  (38,)\n",
      "36 idx:  (77,)\n",
      "37 idx:  (47,)\n",
      "38 idx:  (68,)\n",
      "39 idx:  (68,)\n",
      "40 idx:  (34,)\n",
      "41 idx:  (47,)\n",
      "42 idx:  (66,)\n",
      "43 idx:  (32,)\n",
      "44 idx:  (54,)\n",
      "45 idx:  (51,)\n",
      "46 idx:  (90,)\n",
      "47 idx:  (38,)\n",
      "48 idx:  (114,)\n",
      "49 idx:  (133,)\n",
      "50 idx:  (43,)\n",
      "51 idx:  (50,)\n",
      "52 idx:  (24,)\n",
      "53 idx:  (17,)\n",
      "54 idx:  (18,)\n",
      "55 idx:  (17,)\n",
      "56 idx:  (20,)\n",
      "57 idx:  (55,)\n",
      "58 idx:  (74,)\n",
      "59 idx:  (23,)\n",
      "60 idx:  (59,)\n",
      "61 idx:  (72,)\n",
      "62 idx:  (39,)\n",
      "63 idx:  (94,)\n",
      "64 idx:  (60,)\n",
      "65 idx:  (37,)\n",
      "66 idx:  (65,)\n",
      "67 idx:  (50,)\n",
      "68 idx:  (84,)\n",
      "69 idx:  (38,)\n",
      "70 idx:  (21,)\n",
      "71 idx:  (42,)\n",
      "72 idx:  (67,)\n",
      "73 idx:  (40,)\n",
      "74 idx:  (12,)\n",
      "75 idx:  (39,)\n",
      "76 idx:  (68,)\n",
      "77 idx:  (48,)\n",
      "78 idx:  (77,)\n",
      "79 idx:  (62,)\n",
      "80 idx:  (78,)\n",
      "81 idx:  (69,)\n",
      "82 idx:  (18,)\n",
      "83 idx:  (26,)\n",
      "84 idx:  (20,)\n",
      "85 idx:  (20,)\n",
      "86 idx:  (50,)\n",
      "87 idx:  (64,)\n",
      "88 idx:  (23,)\n",
      "89 idx:  (17,)\n",
      "90 idx:  (52,)\n",
      "91 idx:  (17,)\n",
      "92 idx:  (36,)\n",
      "93 idx:  (28,)\n",
      "94 idx:  (51,)\n",
      "95 idx:  (47,)\n",
      "96 idx:  (47,)\n",
      "97 idx:  (57,)\n",
      "98 idx:  (42,)\n",
      "99 idx:  (93,)\n",
      "100 idx:  (66,)\n",
      "101 idx:  (28,)\n",
      "102 idx:  (29,)\n",
      "103 idx:  (65,)\n",
      "104 idx:  (17,)\n",
      "105 idx:  (53,)\n",
      "106 idx:  (72,)\n",
      "107 idx:  (61,)\n",
      "108 idx:  (42,)\n",
      "109 idx:  (28,)\n",
      "110 idx:  (85,)\n",
      "111 idx:  (19,)\n",
      "112 idx:  (49,)\n",
      "113 idx:  (16,)\n",
      "114 idx:  (51,)\n",
      "115 idx:  (70,)\n",
      "116 idx:  (59,)\n",
      "117 idx:  (86,)\n",
      "118 idx:  (82,)\n",
      "119 idx:  (72,)\n",
      "120 idx:  (93,)\n",
      "121 idx:  (38,)\n",
      "122 idx:  (16,)\n",
      "123 idx:  (43,)\n",
      "124 idx:  (96,)\n",
      "125 idx:  (78,)\n",
      "126 idx:  (37,)\n",
      "127 idx:  (51,)\n",
      "128 idx:  (56,)\n",
      "129 idx:  (22,)\n",
      "130 idx:  (57,)\n",
      "131 idx:  (20,)\n",
      "132 idx:  (70,)\n",
      "133 idx:  (28,)\n",
      "134 idx:  (54,)\n",
      "135 idx:  (39,)\n",
      "136 idx:  (79,)\n",
      "137 idx:  (61,)\n",
      "138 idx:  (42,)\n",
      "139 idx:  (42,)\n",
      "140 idx:  (46,)\n",
      "141 idx:  (35,)\n",
      "142 idx:  (24,)\n",
      "143 idx:  (60,)\n",
      "144 idx:  (28,)\n",
      "145 idx:  (10,)\n",
      "146 idx:  (26,)\n",
      "147 idx:  (66,)\n",
      "148 idx:  (76,)\n",
      "149 idx:  (62,)\n",
      "150 idx:  (64,)\n",
      "151 idx:  (56,)\n",
      "152 idx:  (59,)\n",
      "153 idx:  (27,)\n",
      "154 idx:  (64,)\n",
      "155 idx:  (30,)\n",
      "156 idx:  (39,)\n",
      "157 idx:  (94,)\n",
      "158 idx:  (47,)\n",
      "159 idx:  (74,)\n",
      "160 idx:  (20,)\n",
      "161 idx:  (54,)\n",
      "162 idx:  (41,)\n",
      "163 idx:  (63,)\n",
      "164 idx:  (45,)\n",
      "165 idx:  (73,)\n",
      "166 idx:  (65,)\n",
      "167 idx:  (66,)\n",
      "168 idx:  (70,)\n",
      "169 idx:  (39,)\n",
      "170 idx:  (37,)\n",
      "171 idx:  (32,)\n",
      "172 idx:  (73,)\n",
      "173 idx:  (43,)\n",
      "174 idx:  (31,)\n",
      "175 idx:  (26,)\n",
      "176 idx:  (43,)\n",
      "177 idx:  (59,)\n",
      "178 idx:  (32,)\n",
      "179 idx:  (48,)\n",
      "180 idx:  (45,)\n",
      "181 idx:  (66,)\n",
      "182 idx:  (84,)\n",
      "183 idx:  (13,)\n",
      "184 idx:  (22,)\n",
      "185 idx:  (48,)\n",
      "186 idx:  (39,)\n",
      "187 idx:  (55,)\n",
      "188 idx:  (70,)\n",
      "189 idx:  (39,)\n",
      "190 idx:  (60,)\n",
      "191 idx:  (36,)\n",
      "192 idx:  (29,)\n",
      "193 idx:  (6,)\n",
      "194 idx:  (37,)\n",
      "195 idx:  (12,)\n",
      "196 idx:  (85,)\n",
      "197 idx:  (65,)\n",
      "198 idx:  (65,)\n",
      "199 idx:  (71,)\n",
      "200 idx:  (77,)\n",
      "201 idx:  (25,)\n",
      "202 idx:  (56,)\n",
      "203 idx:  (60,)\n",
      "204 idx:  (13,)\n",
      "205 idx:  (54,)\n",
      "206 idx:  (10,)\n",
      "207 idx:  (42,)\n",
      "208 idx:  (102,)\n",
      "209 idx:  (41,)\n",
      "210 idx:  (62,)\n",
      "211 idx:  (26,)\n",
      "212 idx:  (61,)\n",
      "213 idx:  (42,)\n",
      "214 idx:  (63,)\n",
      "215 idx:  (81,)\n",
      "216 idx:  (64,)\n",
      "217 idx:  (62,)\n",
      "218 idx:  (59,)\n",
      "219 idx:  (51,)\n",
      "220 idx:  (59,)\n",
      "221 idx:  (70,)\n",
      "222 idx:  (71,)\n",
      "223 idx:  (77,)\n",
      "224 idx:  (31,)\n",
      "225 idx:  (16,)\n",
      "226 idx:  (68,)\n",
      "227 idx:  (58,)\n",
      "228 idx:  (19,)\n",
      "229 idx:  (69,)\n",
      "230 idx:  (48,)\n",
      "231 idx:  (54,)\n",
      "232 idx:  (39,)\n",
      "233 idx:  (23,)\n",
      "234 idx:  (77,)\n",
      "235 idx:  (51,)\n",
      "236 idx:  (25,)\n",
      "237 idx:  (79,)\n",
      "238 idx:  (48,)\n",
      "239 idx:  (57,)\n",
      "240 idx:  (54,)\n",
      "241 idx:  (22,)\n",
      "242 idx:  (68,)\n",
      "243 idx:  (82,)\n",
      "244 idx:  (70,)\n",
      "245 idx:  (16,)\n",
      "246 idx:  (48,)\n",
      "247 idx:  (59,)\n",
      "248 idx:  (34,)\n",
      "249 idx:  (43,)\n",
      "250 idx:  (75,)\n",
      "251 idx:  (49,)\n",
      "252 idx:  (39,)\n",
      "253 idx:  (93,)\n",
      "254 idx:  (56,)\n",
      "255 idx:  (47,)\n",
      "256 idx:  (85,)\n",
      "257 idx:  (33,)\n",
      "258 idx:  (59,)\n",
      "259 idx:  (37,)\n",
      "260 idx:  (55,)\n",
      "261 idx:  (107,)\n",
      "262 idx:  (51,)\n",
      "263 idx:  (67,)\n",
      "264 idx:  (59,)\n",
      "265 idx:  (36,)\n",
      "266 idx:  (40,)\n",
      "267 idx:  (60,)\n",
      "268 idx:  (59,)\n",
      "269 idx:  (57,)\n",
      "270 idx:  (61,)\n",
      "271 idx:  (83,)\n",
      "272 idx:  (67,)\n",
      "273 idx:  (76,)\n",
      "274 idx:  (19,)\n",
      "275 idx:  (71,)\n",
      "276 idx:  (30,)\n",
      "277 idx:  (73,)\n",
      "278 idx:  (78,)\n",
      "279 idx:  (44,)\n",
      "280 idx:  (60,)\n",
      "281 idx:  (54,)\n",
      "282 idx:  (104,)\n",
      "283 idx:  (103,)\n",
      "284 idx:  (111,)\n",
      "285 idx:  (65,)\n",
      "286 idx:  (136,)\n",
      "287 idx:  (101,)\n",
      "288 idx:  (68,)\n",
      "289 idx:  (55,)\n",
      "290 idx:  (59,)\n",
      "291 idx:  (76,)\n",
      "292 idx:  (43,)\n",
      "293 idx:  (77,)\n",
      "294 idx:  (95,)\n",
      "295 idx:  (62,)\n",
      "296 idx:  (101,)\n",
      "297 idx:  (101,)\n",
      "298 idx:  (68,)\n",
      "299 idx:  (37,)\n",
      "300 idx:  (96,)\n",
      "301 idx:  (89,)\n",
      "302 idx:  (66,)\n",
      "303 idx:  (65,)\n",
      "304 idx:  (80,)\n",
      "305 idx:  (64,)\n",
      "306 idx:  (129,)\n",
      "307 idx:  (79,)\n",
      "308 idx:  (56,)\n",
      "309 idx:  (57,)\n",
      "310 idx:  (51,)\n",
      "311 idx:  (16,)\n",
      "312 idx:  (96,)\n",
      "313 idx:  (81,)\n",
      "314 idx:  (74,)\n",
      "315 idx:  (48,)\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "######## CLUSTER S_UNIQUE #######\n",
    "#################################\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import numpy as np\n",
    "# X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "#               [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "X = S_uniq_sym #[:30*18]\n",
    "# print (\"X: \", X.shape)\n",
    "# clustering = AffinityPropagation(random_state=5).fit(X)\n",
    "\n",
    "# #\n",
    "#clustering\n",
    "\n",
    "#\n",
    "cluster_ids = np.unique(clustering.labels_)\n",
    "print (\"# clusters: \", np.unique(cluster_ids))\n",
    "\n",
    "# \n",
    "S_unique_clustered = np.zeros(X.shape, 'float32')\n",
    "\n",
    "ctr=0\n",
    "for c in cluster_ids:\n",
    "    idx = np.where(clustering.labels_==c)[0]\n",
    "    print (c,\"idx: \", idx.shape)\n",
    "    S_unique_clustered[ctr:ctr+idx.shape[0]] = S_uniq_sym[idx]\n",
    "    ctr+=idx.shape[0]\n",
    "\n",
    "    \n",
    "# ax2=plt.subplot(121)\n",
    "# plt.imshow(X, aspect='auto')\n",
    "\n",
    "\n",
    "\n",
    "# ax2=plt.subplot(122)\n",
    "# plt.imshow(S_unique_clustered, aspect='auto')\n",
    "# plt.show()\n",
    "\n",
    "# \n",
    "# clustering.predict([[0, 0], [4, 4]])\n",
    "\n",
    "# \n",
    "#print (clustering.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################################\n",
    "# ###############################################\n",
    "###############################################\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_fit[:,0], X_fit[:,1], X_fit[:,2],\n",
    "           c = clustering.labels_)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "######## PREDICT S_ALL #########\n",
    "################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "######## VISUALIZE S_ALL + CLUSTER IDS #########\n",
    "################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "# #\n",
    "# res = hist_rows\n",
    "# for k in trange(hist_rows.shape[0]):\n",
    "#     for p in range(k+1, hist_rows.shape[0],1):\n",
    "#         temp1 = hist_rows[k]\n",
    "#         temp2 = hist_rows[p]\n",
    "#         #print (\"temp1: \", temp1. shape, temp2.shape)\n",
    "#         res = scipy.stats.wasserstein_distance(temp1, \n",
    "#                                                temp2)\n",
    "        \n",
    "        \n",
    "def edm_parallel(idx, data_in):\n",
    "    \n",
    "    res = []\n",
    "    for i in idx:\n",
    "        for p in range(i+1, hist_rows.shape[0],1):\n",
    "            res.append(scipy.stats.wasserstein_distance(hist_rows[i], \n",
    "                                                        hist_rows[k]))\n",
    "    return res\n",
    "                  \n",
    "#                       \n",
    "import parmap\n",
    "                       \n",
    "if True:\n",
    "    idx_split = np.array_split(np.arange(hist_rows.shape[0]),100)\n",
    "    res = parmap.map(edm_parallel, \n",
    "                     idx_split, \n",
    "                     hist_rows[:,:3],\n",
    "                     pm_processes= 8,\n",
    "                     pm_pbar=True)\n",
    "    \n",
    "    \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
