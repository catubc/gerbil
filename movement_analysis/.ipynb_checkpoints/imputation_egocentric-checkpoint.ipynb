{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# \n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import trange\n",
    "import parmap\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "#import umap\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "# \n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn.experimental\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# \n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "# \n",
    "\n",
    "import sleap\n",
    "\n",
    "from Imputation import Impute, CentreBody\n",
    "\n",
    "from Imputation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ... median filtering ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 8099.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ... rejecting outliers....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 23/23 [00:00<00:00, 7574.51it/s]\n"
     ]
    }
   ],
   "source": [
    "##################################################   \n",
    "############# GENERATE EGOCENTRIC DATA ###########   \n",
    "##################################################\n",
    "cb = CentreBody()\n",
    "cb.parallel = True\n",
    "#cb.root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "cb.root_dir = '/media/cat/256GB/dan/slp'\n",
    "\n",
    "#\n",
    "#cb.process_slp()\n",
    "\n",
    "#\n",
    "cb.get_fnames()\n",
    "\n",
    "# median filter data\n",
    "cb.filter_data()\n",
    "\n",
    "# \n",
    "cb.reject_outliers()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DONE Generating 2-point ground truth datasets for imputation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################   \n",
    "####### GENERATE PAIR-WISE CENTRE DATASETS #######   \n",
    "################################################## \n",
    "\n",
    "# \n",
    "cb.parallel = True\n",
    "\n",
    "# for each recording centre/align every pair of data\n",
    "#     TODO: make a single file otherwise generating 15 files per recording\n",
    "cb.centre_and_align_all_pairs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   note: pipeline currently applied only to Cohort1 March 16th datasets   note: pipeline currently applied only to Cohort1 March 16th datasets   note: pipeline currently applied only to Cohort1 March 16th datasets   note: pipeline currently applied only to Cohort1 March 16th datasets   note: pipeline currently applied only to Cohort1 March 16th datasets   note: pipeline currently applied only to Cohort1 March 16th datasets   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (218641, 6, 2)\n",
      " male:                                DATA SIZES: (94647, 6, 2)\n",
      "\n",
      " female [n_samples, n_featres, xy]:  pup1:                                (218641, 6, 2)(132861, 6, 2)\n",
      "\n",
      " male:                               pup2:                               (94647, 6, 2) \n",
      "(176982, 6, 2)\n",
      " pup1:                               (132861, 6, 2)Raw Data: \n",
      " (218641, 6, 2)\n",
      " pup2:                              X_train:   (176982, 6, 2)(218641, 12)\n",
      "\n",
      "fitting...animal id:  0 Raw Data:   body centre:   (218641, 6, 2)0\n",
      " X_train:   rotation pt:   (218641, 12)2\n",
      "\n",
      "fitting...animal id:  0   body centre:  1   rotation pt:  3\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (218641, 6, 2)\n",
      " male:                               (94647, 6, 2) DATA SIZES: \n",
      "\n",
      " pup1:                               (132861, 6, 2)\n",
      " female [n_samples, n_featres, xy]:  (218641, 6, 2) pup2:                              \n",
      " (176982, 6, 2) male:                              \n",
      " (94647, 6, 2)\n",
      "Raw Data:   pup1:                              (218641, 6, 2) \n",
      "(132861, 6, 2)X_train: \n",
      " (218641, 12)\n",
      "fitting...animal id:  pup2:                                0(176982, 6, 2) \n",
      "  body centre:  0   rotation pt: Raw Data:   1(218641, 6, 2)\n",
      "\n",
      "X_train:  (218641, 12)\n",
      "fitting...animal id:  0   body centre:  0   rotation pt:  5\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (218641, 6, 2)\n",
      " male:                               (94647, 6, 2)\n",
      " DATA SIZES: \n",
      " pup1:                               (132861, 6, 2) female [n_samples, n_featres, xy]: \n",
      " (218641, 6, 2)\n",
      " pup2:                                male:                              (176982, 6, 2) \n",
      "(94647, 6, 2) DATA SIZES: \n",
      "\n",
      " pup1:                              Raw Data:   (218641, 6, 2)(132861, 6, 2)\n",
      "\n",
      "X_train:  female [n_samples, n_featres, xy]:   (218641, 12)(218641, 6, 2) DATA SIZES: \n",
      "\n",
      " pup2:                              \n",
      " fitting...animal id:  male:                              (176982, 6, 2)  \n",
      "0(94647, 6, 2) \n",
      "  body centre:  female [n_samples, n_featres, xy]: Raw Data:    pup1:                               1(218641, 6, 2) (218641, 6, 2)\n",
      " (132861, 6, 2)\n",
      "\n",
      "  rotation pt: X_train:  male:                                 (218641, 12)(94647, 6, 2)2\n",
      "\n",
      " pup2:                              \n",
      "fitting...animal id:   (176982, 6, 2) pup1:                              0\n",
      " (132861, 6, 2) \n",
      "  body centre:  Raw Data: 1  pup2:                               (218641, 6, 2) \n",
      "  rotation pt: (176982, 6, 2)X_train:  \n",
      " 4(218641, 12)\n",
      "\n",
      "fitting...animal id: Raw Data:   0(218641, 6, 2) \n",
      "  body centre: X_train:   0(218641, 12)\n",
      " fitting...animal id:   rotation pt:   40\n",
      "   body centre:  0   rotation pt:  3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f82a5161e0a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# this will generate 4 animals x 15 pairwise models = 60 models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_imputation_models_all_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/gerbil/movement_analysis/Imputation.py\u001b[0m in \u001b[0;36mgenerate_imputation_models_all_pairs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                        \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                        pm_processes= 8)\n\u001b[0m\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cat/4TBSSD/anaconda3/envs/gerbil/lib/python3.7/site-packages/parmap/parmap.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(function, iterable, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m        \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpm_pbar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_map_or_starmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cat/4TBSSD/anaconda3/envs/gerbil/lib/python3.7/site-packages/parmap/parmap.py\u001b[0m in \u001b[0;36m_map_or_starmap\u001b[0;34m(function, iterable, args, kwargs, map_or_starmap)\u001b[0m\n\u001b[1;32m    246\u001b[0m                                          repeat(kwargs)),\n\u001b[1;32m    247\u001b[0m                                     chunksize)\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclose_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cat/4TBSSD/anaconda3/envs/gerbil/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cat/4TBSSD/anaconda3/envs/gerbil/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cat/4TBSSD/anaconda3/envs/gerbil/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cat/4TBSSD/anaconda3/envs/gerbil/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#################################################        \n",
    "#### TRAIN PAIRWISE MODELS MISSING FEATURES #####\n",
    "#################################################                       \n",
    "\n",
    "# pass cb object into Impute object\n",
    "I = Impute(cb)\n",
    "I.parallel = True\n",
    "\n",
    "# Select best model; \n",
    "#  [\"BayesianRidge\",          0\n",
    "#   \"DecisionTreeRegressor\",  1 \n",
    "#   \"ExtraTreesRegressor\",    2  <--- best performing when tested on female data; but large data, 2.5GB /model\n",
    "#   \"KNeighborsRegressor\"]    3  <--- 2nd best\n",
    "#  VAE also evaluated; but not coded\n",
    "I.model_type = 2 \n",
    "\n",
    "# this will generate 4 animals x 15 pairwise models = 60 models\n",
    "I.generate_imputation_models_all_pairs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#########################################################    \n",
    "#### LOAD TEST DATA AND PREDICT USING SPECIFIC MODEL ####\n",
    "#########################################################\n",
    "# select a particular animal\n",
    "I = Impute(cb)\n",
    "I.parallel = True\n",
    "I.n_cores = 16\n",
    "I.model_type = 0 \n",
    "\n",
    "#\n",
    "I.animals_selected = [0]\n",
    "I.generate_random_drops = True     # flag used to indicate \n",
    "\n",
    "#\n",
    "# I.predict_imputation_ground_truth_all_pairs()\n",
    "\n",
    "# plot violion plots of errors\n",
    "plot_errors(I)\n",
    "\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[0, 2]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[0, 3]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[0, 4]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[0, 5]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[1, 2]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[1, 3]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[1, 4]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[1, 5]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[2, 3]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[2, 4]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[2, 5]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[3, 4]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[3, 5]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "[4, 5]\n",
      "   note: pipeline currently applied only to Cohort1 March 16th datasets\n",
      " DATA SIZES: \n",
      " female [n_samples, n_featres, xy]:  (2069710, 6, 2)\n",
      " male:                               (2069710, 6, 2)\n",
      " pup1:                               (2069710, 6, 2)\n",
      " pup2:                               (2069710, 6, 2)\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#########################################################    \n",
    "#### LOAD REAL DATA AND PREDICT USING SPECIFIC MODEL ####\n",
    "#########################################################\n",
    "# select a particular animal\n",
    "I = Impute(cb)\n",
    "I.parallel = False\n",
    "I.n_cores = 8\n",
    "I.model_type = 2 \n",
    "\n",
    "# \n",
    "I.animals_selected = [0]\n",
    "I.generate_random_drops = False     # flag used to indicate \n",
    "\n",
    "# \n",
    "I.predict_novel_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################    \n",
    "#### REASSEMBLE PREDICTIONS ####\n",
    "################################\n",
    "\n",
    "# goal is to replaces nans with aggregate but only when the anchor and rotation point (f1 and f2) are close enough\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################    \n",
    "#### UNSHIFT/UNROTATE RESULTS ##\n",
    "################################\n",
    "\n",
    "# load the original shifts/translations and apply them to the predictions fixed traces\n",
    "\n",
    "# visualize before and after results in video form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # load filtered + outliner triaged data with Nans removed\n",
    "# I.cb.load_processed_data(f1, f2,\n",
    "#                          data_type=0,\n",
    "#                          remove_nans=True)\n",
    "        \n",
    "        \n",
    "# I.load_models()\n",
    "\n",
    "\n",
    "# # test against ground truth/cleand data \n",
    "# I.fname_dropout = '/home/cat/feats_dropout.tsv'\n",
    "# res, idx_drop = I.predict_imputation_ground_truth(drops=np.arange(3,6,1)) # drops = 'fixed' or None\n",
    "\n",
    "# # check missing features in the data\n",
    "# # I.calculate_missing_features()\n",
    "\n",
    "# # \n",
    "# # I.plot_imputation_results(features_array, animal_id, idx_test, res, idx_drop)\n",
    "# plt.suptitle(\"animal \"+str(animal_id)+ \" imputed vs. ground truth\",fontsize=20)\n",
    "\n",
    "# # \n",
    "# I.evaluate_imputation_error(features_array, animal_id, res, idx_train, idx_test)\n",
    "# plt.suptitle(\"animal \"+str(animal_id)+ \"  Egocentric (fixed nose) errors (pixels)\",fontsize=20)\n",
    "# #plt.suptitle(\"animal \"+str(animal_id)+ \" NON-Egocentric (fixed nose) errors (pixels)\",fontsize=20)\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 5 4]\n",
      " [0 5 1]\n",
      " [2 4 0]\n",
      " ...\n",
      " [0 4 1]\n",
      " [4 5 0]\n",
      " [4 1 5]]\n",
      "(109565,)\n",
      "(109445,)\n",
      "(44043,)\n",
      "[[5 1 2]\n",
      " [1 2 0]\n",
      " [4 1 2]\n",
      " ...\n",
      " [1 5 2]\n",
      " [5 2 1]\n",
      " [1 4 2]]\n"
     ]
    }
   ],
   "source": [
    "# select some random drops for each\n",
    "drops = np.load('/home/cat/temp_f1f2_drops.npy')\n",
    "print (drops)\n",
    "\n",
    "idx1 = np.where(drops==1)[0]\n",
    "idx2 = np.where(drops==2)[0]\n",
    "print (idx1.shape)\n",
    "print (idx2.shape)\n",
    "\n",
    "idx3 = np.intersect1d(idx1, idx2)\n",
    "print (idx3.shape)\n",
    "print (drops[idx3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "random_sample() takes at most 1 positional argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9a00fd32ea32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.sample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.random_sample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: random_sample() takes at most 1 positional argument (2 given)"
     ]
    }
   ],
   "source": [
    "idx = np.random.sample(np.arange(6), 1)\n",
    "print (idx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9f4b180bc87c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# compute the error vs. ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_imputation_multivariate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# VAE CODE NOT USED NOW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/gerbil/movement_analysis/Imputation.py\u001b[0m in \u001b[0;36mevaluate_imputation_multivariate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0mtdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mtdiff\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m                     \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compute the error vs. ground truth\n",
    "I.evaluate_imputation_multivariate()\n",
    "\n",
    "\n",
    "# VAE CODE NOT USED NOW \n",
    "# make_vae_data()\n",
    "\n",
    "# df1 = evaluate_imputation_vae()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218641, 12)\n",
      "(1093205, 12)\n",
      "(218641, 12)\n",
      "(218641, 12)\n",
      "(1093205, 12)\n",
      "(218641, 12)\n",
      "match_drop: [0 1 2 3 4 5]\n",
      "(5, 12)\n",
      "match_drop: [0 1 2 3 4 5]\n",
      "(5, 12)\n",
      "match_drop: [0 1 2 3 4 5]\n",
      "(5, 12)\n",
      "match_drop: [0 1 2 3 4 5]\n",
      "(5, 12)\n",
      "match_drop: [0 1 2 3 4 5]\n",
      "(5, 12)\n",
      "match_drop: [0 1 2 3 4 5]\n",
      "(5, 12)\n",
      "match_drop: [0 1 2 3 4 5]\n",
      "(5, 12)\n",
      "match_drop: [0 1 2 3 4 5]\n",
      "(5, 12)\n",
      "match_drop: [0 1 2 3 4 5]\n",
      "(5, 12)\n",
      "match_drop: [0 1 2 3 4 5]\n",
      "(5, 12)\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "dropped_features= [0,1,2]\n",
    "I.plot_vae_scatter(dropped_features)  \n",
    "    \n",
    "#        \n",
    "I.plot_multiple_imputation_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "from scipy.spatial import cKDTree\n",
    "import joblib\n",
    "\n",
    "def knn_triage(th, pca_wf):\n",
    "    tree = cKDTree(pca_wf)\n",
    "    dist, ind = tree.query(pca_wf, k=6)\n",
    "    dist = np.sum(dist, 1)\n",
    "    idx_keep1 = dist <= np.percentile(dist, th)\n",
    "    return idx_keep1\n",
    "\n",
    "\n",
    "\n",
    "# Fit the PCA object, but do not transform the data\n",
    "for k in range(4):\n",
    "    ax=plt.subplot(2,2,k+1)\n",
    "    \n",
    "    temp = features_array[k]\n",
    "    d = []\n",
    "    clrs = []\n",
    "    for p in range(len(temp)):\n",
    "        d.append(temp[p])\n",
    "        clrs.extend(np.zeros(temp[p].shape[0])+p)\n",
    "    \n",
    "    clrs = np.array(clrs)\n",
    "    d = np.vstack(d)\n",
    "    print (\"D: \", d.shape)\n",
    "    d = d.reshape(d.shape[0],-1)\n",
    "    continue\n",
    "    #d = sklearn.preprocessing.normalize(d)\n",
    "\n",
    "    # remove 1% of outliers\n",
    "    if True:\n",
    "        th = 95  # % of data to keep\n",
    "        idx_keep = knn_triage(th, d)\n",
    "        print (\" d before traige: \", d.shape)\n",
    "        d = d[idx_keep]\n",
    "        print (\" d after traige: \", d.shape)\n",
    "        clrs = clrs[idx_keep]\n",
    "    \n",
    "    \n",
    "    if False:\n",
    "        pca = PCA(2)\n",
    "\n",
    "        print (\"... data into pca: \", d.shape)\n",
    "\n",
    "        feats_pca = pca.fit_transform(d)\n",
    "        print (feats_pca.shape)\n",
    "\n",
    "        # \n",
    "        plt.scatter(feats_pca[::5,0],\n",
    "           feats_pca[::5,1],\n",
    "            #c=np.arange(feats_pca.shape[0])[::5],\n",
    "            c=clrs[::5],\n",
    "            alpha=.05)\n",
    "        \n",
    "    if True:\n",
    "        \n",
    "#         import gpumap\n",
    "#         #from sklearn.datasets import load_digits\n",
    "\n",
    "#         #digits = load_digits()\n",
    "#         print (\"Data into gpumap: \", d.shape)\n",
    "#         feats_pca = gpumap.GPUMAP().fit_transform(d)\n",
    "#         print (\"Data out of gpumap: \", feats_pca.shape)\n",
    "\n",
    "        import umap\n",
    "    \n",
    "        umap = umap.UMAP(n_components=2,\n",
    "                        init='random',\n",
    "                        random_state=0)\n",
    "        \n",
    "        d = d[::2]\n",
    "        clrs = clrs[::2]\n",
    "        \n",
    "        print (\"... data into umap: \", d.shape)\n",
    "        \n",
    "        if False:\n",
    "            umap_ = umap.fit(d) #[::10])\n",
    "            feats_pca = umap_.transform(d)\n",
    "        else:\n",
    "            feats_pca = umap.fit_transform(d) #[::10])\n",
    "        \n",
    "        \n",
    "            # remove 1% of outliers\n",
    "        if True:\n",
    "            th = 90  # % of data to keep\n",
    "            idx_keep = knn_triage(th, feats_pca)\n",
    "            print (\" d before traige: \", feats_pca.shape)\n",
    "            feats_pca = feats_pca[idx_keep]\n",
    "            print (\" d after traige: \", feats_pca.shape)\n",
    "            clrs = clrs[idx_keep]\n",
    "        \n",
    "        plt.scatter(feats_pca[:,0],\n",
    "               feats_pca[:,1],\n",
    "                #c=np.arange(feats_pca.shape[0])[::5],\n",
    "                c=clrs,\n",
    "                alpha=.05)\n",
    "    if False:\n",
    "        \n",
    "        #from openTSNE import TSNE\n",
    "        #print (\"... data into tsne: \", d.shape)\n",
    "        #feats_pca = TSNE().fit(d)\n",
    "        \n",
    "        \n",
    "        from fastTSNE import TSNE\n",
    "\n",
    "        tsne = TSNE(\n",
    "            n_components=2, perplexity=30, learning_rate=100, early_exaggeration=12,\n",
    "            n_jobs=4, \n",
    "            #angle=0.5, \n",
    "            initialization='random', metric='euclidean',\n",
    "            n_iter=750, early_exaggeration_iter=250, neighbors='exact',\n",
    "            negative_gradient_method='bh', min_num_intervals=10,\n",
    "            #ints_in_inverval=2, \n",
    "            #late_exaggeration_iter=100, \n",
    "            #late_exaggeration=4,\n",
    "        )\n",
    "        \n",
    "        # \n",
    "        feats_pca = tsne.fit(d)\n",
    "\n",
    "        print (\" output: \", feats_pca.shape)\n",
    "\n",
    "\n",
    "        plt.scatter(feats_pca[:,0],\n",
    "            feats_pca[:,1],\n",
    "            #c=np.arange(feats_pca.shape[0])[::5],\n",
    "            c=clrs,\n",
    "            alpha=.05)\n",
    "\n",
    "    # \n",
    "    plt.title(\"Animal:\"+str(k))\n",
    "    \n",
    "    \n",
    "plt.suptitle(\"Static vertically aligned postures\",fontsize=20)\n",
    "plt.show()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.57  4.58  6.43  8.56]\n"
     ]
    }
   ],
   "source": [
    "lens = [218641, 94647, 132861, 176982]\n",
    "\n",
    "lens = np.array(lens)\n",
    "print (np.round(lens/(23*89900)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.]\n",
      " [ 6. 12.]\n",
      " [ 3.  6.]]\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "############### IMPUTE MISSING DATA #############\n",
    "#################################################\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n",
    "X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n",
    "# the model learns that the second feature is double the first\n",
    "\n",
    "print(np.round(imp.transform(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "########## FEATURIZE BEHAVIOR CHUNKS #########\n",
    "##############################################\n",
    "from sklearn import decomposition\n",
    "import sklearn\n",
    "\n",
    "fig = plt.figure()\n",
    "X_all = []\n",
    "n_events = []\n",
    "for animal_id in animal_ids:\n",
    "    X = X4[animal_id].copy()\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    print (X.shape)\n",
    "    X_all.append(X)\n",
    "    n_events.append(X.shape[0])\n",
    "\n",
    "#     \n",
    "X_all = np.vstack(X_all)\n",
    "print (X_all.shape)\n",
    "X = sklearn.preprocessing.normalize(X_all)\n",
    "\n",
    "#\n",
    "if True:\n",
    "    pca = decomposition.PCA(n_components=3)\n",
    "\n",
    "    X_pca = pca.fit_transform(X_all)\n",
    "    print (X_pca.shape)\n",
    "    \n",
    "if False:\n",
    "    import umap\n",
    "    umap = umap.UMAP(n_components=2,\n",
    "                    init='random',\n",
    "                    random_state=0)\n",
    "\n",
    "    umap_ = umap.fit(X_all[::10])\n",
    "\n",
    "    X_pca = umap_.transform(X_all)\n",
    "        \n",
    "\n",
    "print (\"plotting: \", X_pca.shape)\n",
    "\n",
    "\n",
    "print (n_events)\n",
    "fig=plt.figure()\n",
    "for k in range(4):\n",
    "    ax = plt.subplot(2,2,k+1)\n",
    "    start = np.int32(n_events[:k]).sum()\n",
    "    end = np.int32(n_events[:k+1]).sum()\n",
    "    print (start, end)\n",
    "    plt.scatter(X_pca[start:end,0],\n",
    "                X_pca[start:end,1],\n",
    "               alpha=.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
