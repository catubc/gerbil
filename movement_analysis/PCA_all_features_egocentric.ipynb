{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# \n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import trange\n",
    "import parmap\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "#import umap\n",
    "\n",
    "# \n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ... median filtering ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 792.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ... rejecting outliers....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 23/23 [00:00<00:00, 3217.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ... center and aligning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 23/23 [00:00<00:00, 2067.05it/s]\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "############ FILTER DATA #############\n",
    "######################################\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "\n",
    "class CentreBody():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "       # \n",
    "       self.node_names = ['nose',          # 0\n",
    "                      'lefteye',       # 1\n",
    "                      'righteye',      # 2\n",
    "                      'leftear',       # 3\n",
    "                      'rightear',      # 4\n",
    "                      'spine1',        # 5\n",
    "                      'spine2',        # 6\n",
    "                      'spine3',        # 7\n",
    "                      'spine4',        # 8\n",
    "                      'spine5',        # 9\n",
    "                      'tail1',         # 10\n",
    "                      'tail2',         # 11\n",
    "                      'tail3',         # 12\n",
    "                      'tail4']         # 13\n",
    "\n",
    "       self.feature_ids = np.array([0,5,6,7,8,9])       \n",
    "        \n",
    "        \n",
    "    def get_fnames(self):\n",
    "\n",
    "        self.fnames = glob.glob(self.root_dir+\"/*_compressed.npy\")\n",
    "\n",
    "    \n",
    "    def filter_data(self):\n",
    "        \n",
    "        print (\"  ... median filtering ...\")\n",
    "\n",
    "            \n",
    "        if self.parallel:\n",
    "            parmap.map(self.filter_data1, self.fnames,\n",
    "                      pm_processes=8,\n",
    "                      pm_pbar=True)\n",
    "        else:\n",
    "            for fname in fnames:\n",
    "                pass\n",
    "\n",
    "    def filter_data1(self, fname):    \n",
    "\n",
    "        fname_out = fname.replace('.npy','_median_filtered.npy')\n",
    "        if os.path.exists(fname_out)==False:\n",
    "            data = np.load(fname)\n",
    "\n",
    "            data_filtered = data.copy()\n",
    "            for a in range(data.shape[1]):\n",
    "                for f in range(data.shape[2]):\n",
    "                    for l in range(data.shape[3]):\n",
    "                        data_filtered[:,a,f,l] = filter_data2(data[:,a,f,l])\n",
    "\n",
    "            np.save(fname_out, data_filtered)\n",
    "\n",
    "\n",
    "    def filter_data2(self, x, width=25):\n",
    "\n",
    "        # replace data with previous \n",
    "        for k in range(1000):\n",
    "            idx = np.where(np.isnan(x))[0]\n",
    "            if idx.shape[0]==0:\n",
    "                break\n",
    "\n",
    "            if idx[0]==0:\n",
    "                idx=idx[1:]\n",
    "            x[idx] = x[idx-1]\n",
    "\n",
    "        x = scipy.ndimage.median_filter(x, width=25)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def reject_outliers2(self, x,y,\n",
    "                        max_dist_pairwise,\n",
    "                        max_dist_all=100):  # number of deviations away\n",
    "\n",
    "        ''' Function returns indexes for which [x,y] array points are close to at least 2 other points\n",
    "\n",
    "            Goal: to generate very clean data which has 6 body features well connected for downstream analysis\n",
    "\n",
    "        '''\n",
    "\n",
    "        # method 2: explicitly reject points that are > max distance from nearest 2 points\n",
    "        temp = np.vstack((x,y))\n",
    "        dists = scipy.spatial.distance.cdist(temp.T, temp.T)\n",
    "\n",
    "        # first check points inside the array to ensure they have 2 close neighbours\n",
    "        # if they don't, remove them so other points can't be connected to them.\n",
    "        idx_far = []\n",
    "        for k in range(1,temp.shape[1]-1,1):\n",
    "            #idx = np.where(dists[k]<=max_dist_pairwise)[0]\n",
    "            temp = dists[k]\n",
    "            if np.abs(temp[k]-temp[k-1])>max_dist_pairwise or np.abs(temp[k]-temp[k+1])>max_dist_pairwise:\n",
    "                idx_far.append(k)\n",
    "                dists[:,k]= 1E3\n",
    "\n",
    "        # check start and end points to ensure they have nearby val\n",
    "        if np.abs(dists[0,1])>max_dist_pairwise:\n",
    "            idx_far.append(0)\n",
    "            #print (dists[0], 'excluded ', 0)\n",
    "\n",
    "        if np.abs(dists[dists.shape[1]-1,dists.shape[1]-2])>max_dist_pairwise:\n",
    "            idx_far.append(dists.shape[1]-1)\n",
    "            #print (dists[0], 'excluded ', dists.shape[1]-1)\n",
    "\n",
    "\n",
    "        x[idx_far] = np.nan\n",
    "        y[idx_far] = np.nan\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def reject_outliers1(self, fname, feature_ids, max_dist):\n",
    "        \n",
    "        fname2 = fname.replace('.npy','_median_filtered.npy')\n",
    "        \n",
    "        fname_out = fname2.replace('.npy','_outliers.npy')\n",
    "        \n",
    "        if os.path.exists(fname_out)==False:\n",
    "            \n",
    "            data = np.load(fname2)\n",
    "            for f in range(0,data.shape[0],1):\n",
    "\n",
    "                for k in range(data.shape[1]):\n",
    "                    # \n",
    "                    x = data[f,k,feature_ids,0]\n",
    "                    y = data[f,k,feature_ids,1]\n",
    "\n",
    "                    x, y = self.reject_outliers2(x,y, max_dist)\n",
    "\n",
    "                    data[f,k,feature_ids,0] = x\n",
    "                    data[f,k,feature_ids,1] = y\n",
    "\n",
    "            np.save(fname_out, data)\n",
    "\n",
    "    def reject_outliers(self, max_dist=40):\n",
    "        \n",
    "        print (\"  ... rejecting outliers....\")\n",
    "        \n",
    "        self.fnames = glob.glob(self.root_dir+\"/*_compressed.npy\")\n",
    "\n",
    "        if self.parallel:\n",
    "            parmap.map(self.reject_outliers1, self.fnames, \n",
    "                       self.feature_ids,\n",
    "                       max_dist,\n",
    "                       pm_processes=8,\n",
    "                       pm_pbar=True)\n",
    "        else:\n",
    "            for fname in fnames:\n",
    "                pass\n",
    "\n",
    "\n",
    "    def centre_and_align2(self, data, frame, centre_pt=0):\n",
    "\n",
    "        if True:\n",
    "            # centre the data on the nose\n",
    "            data[:,0] -= data[centre_pt,0]\n",
    "            data[:,1] -= data[centre_pt,1]\n",
    "\n",
    "            # get angle between +x axis and head location (i.e. 2nd position)\n",
    "            t = -np.arctan2(*data[1].T[::-1])-np.pi/2\n",
    "\n",
    "            # get rotation\n",
    "            rotmat = np.array([[np.cos(t), -np.sin(t)], \n",
    "                               [np.sin(t),  np.cos(t)]])\n",
    "\n",
    "            # Apply rotation to each row of m\n",
    "            m2 = (rotmat @ data.T).T\n",
    "\n",
    "            return m2\n",
    "\n",
    "    #     # use PCA alignment:\n",
    "    #     else: \n",
    "    #         # Fit the PCA object, but do not transform the data\n",
    "    #         pca = PCA(2)\n",
    "    #         try:\n",
    "    #             pca.fit(data)\n",
    "    #         except:\n",
    "    #             print (\"frame: \", frame, \"  data: crash:\", data)\n",
    "    #             return None\n",
    "\n",
    "    #         # pca.components_ : array, shape (n_components, n_features)\n",
    "    #         # cos theta\n",
    "    #         ct = pca.components_[0, 0]\n",
    "    #         # sin theta\n",
    "    #         st = pca.components_[0, 1]\n",
    "\n",
    "    #         # One possible value of theta that lies in [0, pi]\n",
    "    #         t = np.arccos(ct)\n",
    "\n",
    "    #         t+=np.pi/2.\n",
    "\n",
    "    #         # If t is in quadrant 1, rotate CLOCKwise by t\n",
    "    #         if ct > 0 and st > 0:\n",
    "    #             t *= -1\n",
    "    #         # If t is in Q2, rotate COUNTERclockwise by the complement of theta\n",
    "    #         elif ct < 0 and st > 0:\n",
    "    #             t = np.pi - t\n",
    "    #         # If t is in Q3, rotate CLOCKwise by the complement of theta\n",
    "    #         elif ct < 0 and st < 0:\n",
    "    #             t = -(np.pi - t)\n",
    "    #         # If t is in Q4, rotate COUNTERclockwise by theta, i.e., do nothing\n",
    "    #         elif ct > 0 and st < 0:\n",
    "    #             pass\n",
    "\n",
    "    #         # Manually build the ccw rotation matrix\n",
    "    #         rotmat = np.array([[np.cos(t), -np.sin(t)], \n",
    "    #                            [np.sin(t),  np.cos(t)]])\n",
    "\n",
    "    #         # Apply rotation to each row of m\n",
    "    #         m2 = (rotmat @ data.T).T\n",
    "\n",
    "    #         # Center the rotated point cloud at (0, 0)\n",
    "    #         m2 -= m2.mean(axis=0)\n",
    "\n",
    "\n",
    "    #         # make sure data faces up\n",
    "    #         if m2[0,1]<m2[1,1]:\n",
    "    #             m2[:,1] = m2[:,1][::-1]\n",
    "\n",
    "        #return m2\n",
    "    \n",
    "    def centre_and_align(self):\n",
    "        \n",
    "        print (\"  ... center and aligning ...\")\n",
    "\n",
    "        self.fnames = glob.glob(self.root_dir+\"/*_compressed.npy\")\n",
    "\n",
    "        if self.parallel:\n",
    "            parmap.map(self.centre_and_align1, self.fnames, \n",
    "                       self.feature_ids,\n",
    "                       pm_processes=8,\n",
    "                       pm_pbar=True)\n",
    "        else:\n",
    "            for fname in fnames:\n",
    "                pass\n",
    "            \n",
    "\n",
    "    def centre_and_align1(self, fname,\n",
    "                         feature_ids):\n",
    "\n",
    "        fname2 = fname.replace('.npy','_median_filtered_outliers.npy')\n",
    "        \n",
    "        fname_out = fname2.replace('.npy','_centre_aligned.npy')\n",
    "        #fname_out_good_only = fname2.replace('.npy','_centre_aligned.npy')\n",
    "        \n",
    "        if os.path.exists(fname_out)==False:\n",
    "            \n",
    "            data = np.load(fname2)\n",
    "\n",
    "            # \n",
    "            centre_pt = 0\n",
    "\n",
    "            features_full = np.zeros((data.shape[0],data.shape[1],feature_ids.shape[0],2), \n",
    "                                          'float32')+np.nan\n",
    "\n",
    "            features_array = []\n",
    "            for k in range(4):\n",
    "                features_array.append([])\n",
    "\n",
    "            for f in range(0,data.shape[0],1):\n",
    "\n",
    "                # loop over each animal\n",
    "                for k in range(data.shape[1]):\n",
    "\n",
    "                    x = data[f,k,feature_ids,0]\n",
    "                    y = data[f,k,feature_ids,1]\n",
    "\n",
    "                    idx = np.where(np.isnan(x))[0]\n",
    "                    if idx.shape[0]==0:\n",
    "                        #print (f, k, x.shape)\n",
    "\n",
    "                        locs = np.vstack((x,y)).T\n",
    "\n",
    "                        # centre and align data\n",
    "                        locs_pca = self.centre_and_align2(locs,f,centre_pt)\n",
    "\n",
    "                        if locs_pca is not None:\n",
    "                            idx = np.where(np.isnan(locs_pca))[0]\n",
    "\n",
    "                            if idx.shape[0]>0:\n",
    "                                continue\n",
    "                                \n",
    "                            features_full[f,k] = locs_pca\n",
    "                            features_array[k].append(locs_pca)\n",
    "\n",
    "            np.save(fname_out, features_full)\n",
    "\n",
    "            \n",
    "import sklearn.experimental\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "#print (len(features_array))\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_imputation_error(features_array, animal_id, res, idx_train, idx_test):\n",
    "    \n",
    "    temp = np.vstack(features_array[animal_id])\n",
    "    fig=plt.figure()\n",
    "    ax=plt.subplot()\n",
    "    \n",
    "    diff = np.abs(temp[idx_test]-res)\n",
    "    #print (\"diff: \", diff.shape)\n",
    "    \n",
    "    errors = []\n",
    "    for k in range(diff.shape[1]):\n",
    "        errors.append([])\n",
    "    \n",
    "    for k in range(diff.shape[0]):\n",
    "        for p in range(diff.shape[1]):\n",
    "            temp = diff[k,p]\n",
    "            #print (temp)\n",
    "            tdiff = np.linalg.norm(temp)\n",
    "            if tdiff>0:\n",
    "                errors[p].append(tdiff)\n",
    "\n",
    "    t =[]\n",
    "    for k in range(len(errors)):\n",
    "        temp = errors[k]\n",
    "        pad = np.zeros(100000-len(errors[k]),'float32')+np.nan\n",
    "        temp = np.concatenate((temp, pad))\n",
    "        t.append(temp)\n",
    "        \n",
    "    data = np.array(t).T\n",
    "    #print (data.shape)\n",
    "    columns = ['nose','spine1','spine2', 'spine3', 'spine4', 'spine5']\n",
    "    #columns = ['errors']\n",
    "    df = pd.DataFrame(data, columns = columns)\n",
    "\n",
    "    #print (\"DF: \", df)\n",
    "\n",
    "    # plot\n",
    "    ax=plt.subplot(2,1,1)\n",
    "    sns.violinplot(data=df) #x=df['spine2'])\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.ylabel(\" pixel error\")\n",
    "    \n",
    "    ax=plt.subplot(2,1,2)\n",
    "    plt.title(\"Zoom\",fontsize=20)\n",
    "    sns.violinplot(data=df) #x=df['spine2'])\n",
    "    plt.ylabel(\" pixel error\")\n",
    "    plt.ylim(0,50)\n",
    "            \n",
    "        \n",
    "# \n",
    "def generate_imputation_model(features_array, animal_id, idx=None):\n",
    "\n",
    "    temp = np.vstack(features_array[animal_id])\n",
    "    print (temp.shape)\n",
    "\n",
    "    X_all = temp.reshape(temp.shape[0],-1)\n",
    "    print (\"X_all: \", X_all.shape)\n",
    "\n",
    "    if False:\n",
    "        split = 0.9\n",
    "        idx = np.random.choice(np.arange(X_all.shape[0]), int(X_all.shape[0]*split),replace=False)\n",
    "        idx_not = np.delete(np.arange(X_all.shape[0]),idx)\n",
    "\n",
    "    if idx is not None: \n",
    "        #\n",
    "        X_train = X_all[idx]\n",
    "\n",
    "    #\n",
    "    print (\"fitting...\")\n",
    "    imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "    imp.fit(X_train)\n",
    "    print (\"done\")\n",
    "    \n",
    "    return imp\n",
    "\n",
    "def predict_imputation(imp, features_array, animal_id, drops='fixed', idx_test=None, n_drops = 3):\n",
    "    \n",
    "    # \n",
    "    temp = np.vstack(features_array[animal_id])\n",
    "    print (temp.shape)\n",
    "\n",
    "    X_all = temp.reshape(temp.shape[0],-1)\n",
    "\n",
    "    # select frames to predict\n",
    "    if idx_test is not None:\n",
    "        X_test = X_all[idx_test]\n",
    "    \n",
    "    # do drop outs in the test set:\n",
    "    if True:\n",
    "        X_test = X_test.reshape(-1,6,2)\n",
    "        idx_drop = np.zeros((n_drops, X_test.shape[0]),'int32')\n",
    "        for k in range(idx_drop.shape[1]):#n_drops):\n",
    "            if drops=='fixed':\n",
    "                idx_drop[:,k] = [1,3,5] #np.random.choice(np.arange(6),n_drops,replace=False)\n",
    "            else:\n",
    "                idx_drop[:,k] = np.random.choice(np.arange(6),n_drops,replace=False)\n",
    "                \n",
    "                \n",
    "        print (\"idx drop: \", idx_drop[0].shape)\n",
    "        for k in range(len(idx_drop)):\n",
    "            for p in range(idx_drop[k].shape[0]):\n",
    "                X_test[p,idx_drop[k][p]]=np.nan\n",
    "    else:\n",
    "        # just evaluate the data as is...\n",
    "        pass\n",
    "\n",
    "    # TEST STEP\n",
    "    X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "    res = imp.transform(X_test).reshape(-1,6,2)\n",
    "    #print (\"Res: \", res.shape)\n",
    "\n",
    "    return res, idx_drop\n",
    "\n",
    "    \n",
    "cb = CentreBody()\n",
    "cb.parallel = True\n",
    "cb.root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "cb.get_fnames()\n",
    "\n",
    "# median filter data\n",
    "cb.filter_data()\n",
    "\n",
    "# \n",
    "cb.reject_outliers()\n",
    "\n",
    "#\n",
    "cb.centre_and_align()\n",
    "\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:02<00:00, 10.78it/s]\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "############## STACK ALL THE DATA ############\n",
    "##############################################\n",
    "\n",
    "# \n",
    "file_order = [\n",
    "\n",
    "'2020-3-16_11-56-56-704655',  # day time starts correct day\n",
    "'2020-3-16_12-57-12-418305',\n",
    "'2020-3-16_01-57-27-327194',\n",
    "'2020-3-16_02-57-41-995158',\n",
    "'2020-3-16_03-57-56-902379',\n",
    "'2020-3-16_04-58-11-998956',\n",
    "'2020-3-16_05-58-27-193818',\n",
    "'2020-3-16_06-58-43-678014',\n",
    "'2020-3-16_07-59-00-362242',\n",
    "'2020-3-16_08-59-17-534732',\n",
    "'2020-3-16_09-59-34-731308',\n",
    "'2020-3-16_10-59-50-448686',\n",
    "\n",
    "'2020-3-16_12-54-07-193951',  # night time of previous day though\n",
    "'2020-3-16_01-54-23-358257',\n",
    "'2020-3-16_02-54-39-170978',\n",
    "'2020-3-16_03-54-54-231226',\n",
    "'2020-3-16_04-55-09-841582',\n",
    "'2020-3-16_05-55-25-305681',\n",
    "'2020-3-16_06-55-40-714236',\n",
    "'2020-3-16_07-55-55-775234',\n",
    "'2020-3-16_08-56-11-096689',\n",
    "'2020-3-16_09-56-26-362091',\n",
    "'2020-3-16_10-56-41-406701',\n",
    "]\n",
    "\n",
    "# stack the postures for each animal\n",
    "features_array = []\n",
    "for k in range(4):\n",
    "    features_array.append([])\n",
    "    \n",
    "# \n",
    "from tqdm import tqdm\n",
    "for file in tqdm(file_order):\n",
    "    \n",
    "    if True:\n",
    "        fname = glob.glob(os.path.join(cb.root_dir,file+\"*_centre_aligned.npy\").replace(\"-\",\"_\"))[0]\n",
    "        d3 = np.load(fname)\n",
    "    if False:\n",
    "        fname = glob.glob(os.path.join(cb.root_dir,file+\"*_median_filtered_outliers.npy\").replace(\"-\",\"_\"))[0]    \n",
    "    \n",
    "        d3 = np.load(fname)\n",
    "        d3 = d3[:,:,np.array([0,5,6,7,8,9])]  \n",
    "        \n",
    "    if False:\n",
    "        fname = glob.glob(os.path.join(cb.root_dir,file+\"*_median_filtered.npy\").replace(\"-\",\"_\"))[0]    \n",
    "    \n",
    "        d3 = np.load(fname)\n",
    "        d3 = d3[:,:,np.array([0,5,6,7,8,9])]       \n",
    "\n",
    "        \n",
    "        \n",
    "    #print (d3.shape)\n",
    "\n",
    "    # loop over animals and keep only complete data (i.e. 6 pts)\n",
    "    for k in range(4):\n",
    "        # find nans and delete any frame that is missing even a single value for that animal\n",
    "        idx = np.where(np.isnan(d3[:,k]))\n",
    "        ids = np.unique(idx[0])\n",
    "    \n",
    "        idx_all = np.arange(d3.shape[0])\n",
    "        idx_good = np.delete(idx_all, ids)\n",
    "#        print (idx_good.shape)\n",
    "        \n",
    "        features_array[k].append(d3[idx_good,k])\n",
    "\n",
    "        \n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218641, 6, 2)\n",
      "X_all:  (218641, 12)\n",
      "fitting...\n",
      "done\n",
      "(218641, 6, 2)\n",
      "idx drop:  (21865,)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "def plot_imputation_results(features_array, animal_id, idx_test, res, idx_drop):\n",
    "    \n",
    "    # \n",
    "    labels = ['n','s1','s2','s3','s4','s5']\n",
    "    \n",
    "    # grab the selected data:\n",
    "    temp = np.vstack(features_array[animal_id])\n",
    "    #print (\"temp: \", temp.shape)\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    shift = 0\n",
    "    for k in range(10):\n",
    "        plt.subplot(2,5,k+1)\n",
    "\n",
    "        ############ PLOT GROUND TRUTH ##############\n",
    "        id2 = np.random.choice(idx_test,1)[0]\n",
    "        #print (id2)\n",
    "        \n",
    "        #print (temp[id2].shape)\n",
    "        plt.scatter(temp[id2,:,0],\n",
    "                    temp[id2,:,1],\n",
    "                    c='blue',\n",
    "                    s=np.arange(1,7,1)[::-1]*20,\n",
    "                    alpha=.7,\n",
    "                    edgecolor='black', label='truth')\n",
    "\n",
    "        #\n",
    "        id3 = np.where(idx_test==id2)[0]\n",
    "        #print (id3)\n",
    "\n",
    "        ############ PLOT IMPUTED LOCS ##############\n",
    "        plt.scatter(res[id3,:,0]+shift,\n",
    "                    res[id3,:,1],\n",
    "                    c='red',\n",
    "                    s=np.arange(1,7,1)[::-1]*20,\n",
    "                    alpha=.7,\n",
    "                    edgecolor='black', label='imputed')\n",
    "\n",
    "        if True: #k==0:\n",
    "            for p in range(6):\n",
    "#                 plt.text(res[id3,p,0],\n",
    "#                          res[id3,p,1],labels[p])\n",
    "                plt.text(temp[id2,p,0],\n",
    "                         temp[id2,p,1],labels[p])\n",
    "\n",
    "        # draw lines\n",
    "        for p in range(len(idx_drop)):\n",
    "            #print (\"connectgin: \", p, idx_drop[p][id3])\n",
    "            #print (\"idx_drop full: \", np.array(idx_drop).shape)\n",
    "            plt.plot([temp[id2,idx_drop[p][id3],0], res[id3,idx_drop[p][id3],0]+shift],\n",
    "                     [temp[id2,idx_drop[p][id3],1], res[id3,idx_drop[p][id3],1]],\n",
    "                     '--',c='black')\n",
    "\n",
    "        if k==0:\n",
    "            plt.legend(fontsize=8)\n",
    "\n",
    "        plt.title(\"frame id: \"+str(id2) + \"\\ndrops: \"+str(np.array(idx_drop)[:,id3]),fontsize=8)\n",
    "        \n",
    "        x1 = (np.max(np.abs(temp[id2,:,0])), \n",
    "                             np.max(np.abs(res[id3,:,0])),\n",
    "                             np.max(np.abs(temp[id2,:,1])), \n",
    "                             np.max(np.abs(res[id3,:,1])))\n",
    "        #print (\"x1: \", x1)\n",
    "\n",
    "        max_ = np.max(x1)*1.2\n",
    "        #plt.xlim(-max_, max_+shift)\n",
    "        #plt.xlim(-100, 200)\n",
    "        plt.ylim(-max_,10)\n",
    "        ax.set_aspect('equal', 'datalim')\n",
    "        #print ('')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "animal_id = 0\n",
    "\n",
    "#\n",
    "n_frames = np.vstack(features_array[animal_id]).shape[0]\n",
    "split = 0.9\n",
    "idx_train = np.random.choice(np.arange(n_frames),int(n_frames*split),replace=False)\n",
    "imp = generate_imputation_model(features_array, animal_id, idx_train)\n",
    "\n",
    "# \n",
    "idx_test = np.delete(np.arange(n_frames), idx_train)\n",
    "res, idx_drop = predict_imputation(imp, features_array, animal_id, drops=None, idx_test=idx_test, n_drops = 3)\n",
    "\n",
    "#\n",
    "plot_imputation_results(features_array, animal_id, idx_test, res, idx_drop)\n",
    "plt.suptitle(\"animal \"+str(animal_id)+ \" imputed vs. ground truth\",fontsize=20)\n",
    "\n",
    "# \n",
    "evaluate_imputation_error(features_array, animal_id, res, idx_train, idx_test)\n",
    "#plt.suptitle(\"Clean data only \",fontsize=20)\n",
    "plt.suptitle(\"animal \"+str(animal_id)+ \"  Egocentric (fixed nose) errors (pixels)\",fontsize=20)\n",
    "#plt.suptitle(\"animal \"+str(animal_id)+ \" NON-Egocentric (fixed nose) errors (pixels)\",fontsize=20)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff:  (10000, 6, 2)\n",
      "(10000, 6)\n",
      "DF:        nose    spine1     spine2     spine3     spine4     spine5\n",
      "0      NaN  8.053196  15.022601  20.459671  17.801094  20.735806\n",
      "1      NaN  6.009436   7.212002   7.303462  28.604742  53.004890\n",
      "2      NaN  1.747940   9.411833   8.524810   3.233682   9.551811\n",
      "3      NaN  4.166721   2.071897  13.535925  25.377176  37.943233\n",
      "4      NaN  0.956593   8.782355   2.688964  14.177631   5.399886\n",
      "...    ...       ...        ...        ...        ...        ...\n",
      "9995   NaN       NaN        NaN        NaN        NaN        NaN\n",
      "9996   NaN       NaN        NaN        NaN        NaN        NaN\n",
      "9997   NaN       NaN        NaN        NaN        NaN        NaN\n",
      "9998   NaN       NaN        NaN        NaN        NaN        NaN\n",
      "9999   NaN       NaN        NaN        NaN        NaN        NaN\n",
      "\n",
      "[10000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#         \n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nose</th>\n",
       "      <th>spine1</th>\n",
       "      <th>spine2</th>\n",
       "      <th>spine3</th>\n",
       "      <th>spine4</th>\n",
       "      <th>spine5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.818413</td>\n",
       "      <td>4.814587</td>\n",
       "      <td>24.999798</td>\n",
       "      <td>25.545969</td>\n",
       "      <td>19.456072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.858467</td>\n",
       "      <td>4.052742</td>\n",
       "      <td>19.418888</td>\n",
       "      <td>24.486452</td>\n",
       "      <td>19.967733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.557945</td>\n",
       "      <td>8.433762</td>\n",
       "      <td>24.798075</td>\n",
       "      <td>24.866077</td>\n",
       "      <td>56.824924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.740223</td>\n",
       "      <td>8.106142</td>\n",
       "      <td>26.428474</td>\n",
       "      <td>25.756119</td>\n",
       "      <td>56.594051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.175694</td>\n",
       "      <td>7.933312</td>\n",
       "      <td>17.512733</td>\n",
       "      <td>42.814701</td>\n",
       "      <td>63.951710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nose    spine1    spine2     spine3     spine4     spine5\n",
       "0      NaN  3.818413  4.814587  24.999798  25.545969  19.456072\n",
       "1      NaN  0.858467  4.052742  19.418888  24.486452  19.967733\n",
       "2      NaN  4.557945  8.433762  24.798075  24.866077  56.824924\n",
       "3      NaN  0.740223  8.106142  26.428474  25.756119  56.594051\n",
       "4      NaN  2.175694  7.933312  17.512733  42.814701  63.951710\n",
       "...    ...       ...       ...        ...        ...        ...\n",
       "9995   NaN       NaN       NaN        NaN        NaN        NaN\n",
       "9996   NaN       NaN       NaN        NaN        NaN        NaN\n",
       "9997   NaN       NaN       NaN        NaN        NaN        NaN\n",
       "9998   NaN       NaN       NaN        NaN        NaN        NaN\n",
       "9999   NaN       NaN       NaN        NaN        NaN        NaN\n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "from scipy.spatial import cKDTree\n",
    "import joblib\n",
    "\n",
    "def knn_triage(th, pca_wf):\n",
    "    tree = cKDTree(pca_wf)\n",
    "    dist, ind = tree.query(pca_wf, k=6)\n",
    "    dist = np.sum(dist, 1)\n",
    "    idx_keep1 = dist <= np.percentile(dist, th)\n",
    "    return idx_keep1\n",
    "\n",
    "\n",
    "\n",
    "# Fit the PCA object, but do not transform the data\n",
    "for k in range(4):\n",
    "    ax=plt.subplot(2,2,k+1)\n",
    "    \n",
    "    temp = features_array[k]\n",
    "    d = []\n",
    "    clrs = []\n",
    "    for p in range(len(temp)):\n",
    "        d.append(temp[p])\n",
    "        clrs.extend(np.zeros(temp[p].shape[0])+p)\n",
    "    \n",
    "    clrs = np.array(clrs)\n",
    "    d = np.vstack(d)\n",
    "    print (\"D: \", d.shape)\n",
    "    d = d.reshape(d.shape[0],-1)\n",
    "    continue\n",
    "    #d = sklearn.preprocessing.normalize(d)\n",
    "\n",
    "    # remove 1% of outliers\n",
    "    if True:\n",
    "        th = 95  # % of data to keep\n",
    "        idx_keep = knn_triage(th, d)\n",
    "        print (\" d before traige: \", d.shape)\n",
    "        d = d[idx_keep]\n",
    "        print (\" d after traige: \", d.shape)\n",
    "        clrs = clrs[idx_keep]\n",
    "    \n",
    "    \n",
    "    if False:\n",
    "        pca = PCA(2)\n",
    "\n",
    "        print (\"... data into pca: \", d.shape)\n",
    "\n",
    "        feats_pca = pca.fit_transform(d)\n",
    "        print (feats_pca.shape)\n",
    "\n",
    "        # \n",
    "        plt.scatter(feats_pca[::5,0],\n",
    "           feats_pca[::5,1],\n",
    "            #c=np.arange(feats_pca.shape[0])[::5],\n",
    "            c=clrs[::5],\n",
    "            alpha=.05)\n",
    "        \n",
    "    if True:\n",
    "        \n",
    "#         import gpumap\n",
    "#         #from sklearn.datasets import load_digits\n",
    "\n",
    "#         #digits = load_digits()\n",
    "#         print (\"Data into gpumap: \", d.shape)\n",
    "#         feats_pca = gpumap.GPUMAP().fit_transform(d)\n",
    "#         print (\"Data out of gpumap: \", feats_pca.shape)\n",
    "\n",
    "        import umap\n",
    "    \n",
    "        umap = umap.UMAP(n_components=2,\n",
    "                        init='random',\n",
    "                        random_state=0)\n",
    "        \n",
    "        d = d[::2]\n",
    "        clrs = clrs[::2]\n",
    "        \n",
    "        print (\"... data into umap: \", d.shape)\n",
    "        \n",
    "        if False:\n",
    "            umap_ = umap.fit(d) #[::10])\n",
    "            feats_pca = umap_.transform(d)\n",
    "        else:\n",
    "            feats_pca = umap.fit_transform(d) #[::10])\n",
    "        \n",
    "        \n",
    "            # remove 1% of outliers\n",
    "        if True:\n",
    "            th = 90  # % of data to keep\n",
    "            idx_keep = knn_triage(th, feats_pca)\n",
    "            print (\" d before traige: \", feats_pca.shape)\n",
    "            feats_pca = feats_pca[idx_keep]\n",
    "            print (\" d after traige: \", feats_pca.shape)\n",
    "            clrs = clrs[idx_keep]\n",
    "        \n",
    "        plt.scatter(feats_pca[:,0],\n",
    "               feats_pca[:,1],\n",
    "                #c=np.arange(feats_pca.shape[0])[::5],\n",
    "                c=clrs,\n",
    "                alpha=.05)\n",
    "    if False:\n",
    "        \n",
    "        #from openTSNE import TSNE\n",
    "        #print (\"... data into tsne: \", d.shape)\n",
    "        #feats_pca = TSNE().fit(d)\n",
    "        \n",
    "        \n",
    "        from fastTSNE import TSNE\n",
    "\n",
    "        tsne = TSNE(\n",
    "            n_components=2, perplexity=30, learning_rate=100, early_exaggeration=12,\n",
    "            n_jobs=4, \n",
    "            #angle=0.5, \n",
    "            initialization='random', metric='euclidean',\n",
    "            n_iter=750, early_exaggeration_iter=250, neighbors='exact',\n",
    "            negative_gradient_method='bh', min_num_intervals=10,\n",
    "            #ints_in_inverval=2, \n",
    "            #late_exaggeration_iter=100, \n",
    "            #late_exaggeration=4,\n",
    "        )\n",
    "        \n",
    "        # \n",
    "        feats_pca = tsne.fit(d)\n",
    "\n",
    "        print (\" output: \", feats_pca.shape)\n",
    "\n",
    "\n",
    "        plt.scatter(feats_pca[:,0],\n",
    "            feats_pca[:,1],\n",
    "            #c=np.arange(feats_pca.shape[0])[::5],\n",
    "            c=clrs,\n",
    "            alpha=.05)\n",
    "\n",
    "    # \n",
    "    plt.title(\"Animal:\"+str(k))\n",
    "    \n",
    "    \n",
    "plt.suptitle(\"Static vertically aligned postures\",fontsize=20)\n",
    "plt.show()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.57  4.58  6.43  8.56]\n"
     ]
    }
   ],
   "source": [
    "lens = [218641, 94647, 132861, 176982]\n",
    "\n",
    "lens = np.array(lens)\n",
    "print (np.round(lens/(23*89900)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.]\n",
      " [ 6. 12.]\n",
      " [ 3.  6.]]\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "############### IMPUTE MISSING DATA #############\n",
    "#################################################\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n",
    "X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n",
    "# the model learns that the second feature is double the first\n",
    "\n",
    "print(np.round(imp.transform(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "########## FEATURIZE BEHAVIOR CHUNKS #########\n",
    "##############################################\n",
    "from sklearn import decomposition\n",
    "import sklearn\n",
    "\n",
    "fig = plt.figure()\n",
    "X_all = []\n",
    "n_events = []\n",
    "for animal_id in animal_ids:\n",
    "    X = X4[animal_id].copy()\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    print (X.shape)\n",
    "    X_all.append(X)\n",
    "    n_events.append(X.shape[0])\n",
    "\n",
    "#     \n",
    "X_all = np.vstack(X_all)\n",
    "print (X_all.shape)\n",
    "X = sklearn.preprocessing.normalize(X_all)\n",
    "\n",
    "#\n",
    "if True:\n",
    "    pca = decomposition.PCA(n_components=3)\n",
    "\n",
    "    X_pca = pca.fit_transform(X_all)\n",
    "    print (X_pca.shape)\n",
    "    \n",
    "if False:\n",
    "    import umap\n",
    "    umap = umap.UMAP(n_components=2,\n",
    "                    init='random',\n",
    "                    random_state=0)\n",
    "\n",
    "    umap_ = umap.fit(X_all[::10])\n",
    "\n",
    "    X_pca = umap_.transform(X_all)\n",
    "        \n",
    "\n",
    "print (\"plotting: \", X_pca.shape)\n",
    "\n",
    "\n",
    "print (n_events)\n",
    "fig=plt.figure()\n",
    "for k in range(4):\n",
    "    ax = plt.subplot(2,2,k+1)\n",
    "    start = np.int32(n_events[:k]).sum()\n",
    "    end = np.int32(n_events[:k+1]).sum()\n",
    "    print (start, end)\n",
    "    plt.scatter(X_pca[start:end,0],\n",
    "                X_pca[start:end,1],\n",
    "               alpha=.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
