{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# \n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import trange\n",
    "import parmap\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "#import umap\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "# \n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn.experimental\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# \n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ... median filtering ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 21624.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ... rejecting outliers....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 23/23 [00:00<00:00, 12110.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ... center and aligning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 26737.53it/s]\n",
      "100%|██████████| 23/23 [00:01<00:00, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DATA SIZES: \n",
      " female:  (218641, 6, 2)\n",
      " male:  (94647, 6, 2)\n",
      " pup1:  (132861, 6, 2)\n",
      " pup2:  (176982, 6, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "############ FILTER DATA #############\n",
    "######################################\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "\n",
    "class CentreBody():\n",
    "    \n",
    "    def __init__(self):\n",
    "         \n",
    "        # \n",
    "        self.node_names = ['nose',          # 0\n",
    "                          'lefteye',       # 1\n",
    "                          'righteye',      # 2\n",
    "                          'leftear',       # 3\n",
    "                          'rightear',      # 4\n",
    "                          'spine1',        # 5\n",
    "                          'spine2',        # 6\n",
    "                          'spine3',        # 7\n",
    "                          'spine4',        # 8\n",
    "                          'spine5',        # 9\n",
    "                          'tail1',         # 10\n",
    "                          'tail2',         # 11\n",
    "                          'tail3',         # 12\n",
    "                          'tail4']         # 13\n",
    "\n",
    "        self.feature_ids = np.array([0,5,6,7,8,9])       \n",
    "        \n",
    "        \n",
    "    def get_fnames(self):\n",
    "\n",
    "        self.fnames = glob.glob(self.root_dir+\"/*_compressed.npy\")\n",
    "\n",
    "    \n",
    "    def filter_data(self):\n",
    "        \n",
    "        print (\"  ... median filtering ...\")\n",
    "\n",
    "            \n",
    "        if self.parallel:\n",
    "            parmap.map(self.filter_data1, self.fnames,\n",
    "                      pm_processes=8,\n",
    "                      pm_pbar=True)\n",
    "        else:\n",
    "            for fname in fnames:\n",
    "                pass\n",
    "\n",
    "    def filter_data1(self, fname):    \n",
    "\n",
    "        fname_out = fname.replace('.npy','_median_filtered.npy')\n",
    "        if os.path.exists(fname_out)==False:\n",
    "            data = np.load(fname)\n",
    "\n",
    "            data_filtered = data.copy()\n",
    "            # loop over animals\n",
    "            for a in range(data.shape[1]):\n",
    "                # loop over feature\n",
    "                for f in range(data.shape[2]):\n",
    "                    # loop over each x,y\n",
    "                    for l in range(data.shape[3]):\n",
    "                        data_filtered[:,a,f,l] = filter_data2(data[:,a,f,l])\n",
    "\n",
    "            np.save(fname_out, data_filtered)\n",
    "\n",
    "\n",
    "    def filter_data2(self, x, width=25):\n",
    "\n",
    "        # replace data with previous value until all nans are replacesd\n",
    "        for k in range(1000):\n",
    "            idx = np.where(np.isnan(x))[0]\n",
    "            if idx.shape[0]==0:\n",
    "                break\n",
    "\n",
    "            if idx[0]==0:\n",
    "                idx=idx[1:]\n",
    "            x[idx] = x[idx-1]\n",
    "\n",
    "        \n",
    "        x = scipy.ndimage.median_filter(x, width=width)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def reject_outliers2(self, x,y,\n",
    "                        max_dist_pairwise,\n",
    "                        max_dist_all=100):  # number of deviations away\n",
    "\n",
    "        ''' Function returns indexes for which [x,y] array points are close to at least 2 other points\n",
    "\n",
    "            Goal 1: to generate very clean data which has 6 body features well connected for downstream analysis and imputation\n",
    "\n",
    "            Goal 2: to remove outliers and leave small clusters of feautres only\n",
    "            \n",
    "        '''\n",
    "\n",
    "        # method 2: explicitly reject points that are > max distance from nearest 2 points\n",
    "        temp = np.vstack((x,y))\n",
    "        dists = scipy.spatial.distance.cdist(temp.T, temp.T)\n",
    "\n",
    "        # first check points inside the array to ensure they have 2 close neighbours\n",
    "        # if they don't, remove them so other points can't be connected to them.\n",
    "        idx_far = []\n",
    "        for k in range(1,temp.shape[1]-1,1):\n",
    "            #idx = np.where(dists[k]<=max_dist_pairwise)[0]\n",
    "            temp = dists[k]\n",
    "            if np.abs(temp[k]-temp[k-1])>max_dist_pairwise or np.abs(temp[k]-temp[k+1])>max_dist_pairwise:\n",
    "                idx_far.append(k)\n",
    "                dists[:,k]= 1E3\n",
    "\n",
    "        # check start and end points to ensure they have nearby val\n",
    "        if np.abs(dists[0,1])>max_dist_pairwise:\n",
    "            idx_far.append(0)\n",
    "            #print (dists[0], 'excluded ', 0)\n",
    "\n",
    "        if np.abs(dists[dists.shape[1]-1,dists.shape[1]-2])>max_dist_pairwise:\n",
    "            idx_far.append(dists.shape[1]-1)\n",
    "            #print (dists[0], 'excluded ', dists.shape[1]-1)\n",
    "\n",
    "\n",
    "        x[idx_far] = np.nan\n",
    "        y[idx_far] = np.nan\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def reject_outliers1(self, fname, feature_ids, max_dist):\n",
    "        \n",
    "        fname2 = fname.replace('.npy','_median_filtered.npy')\n",
    "        \n",
    "        fname_out = fname2.replace('.npy','_outliers.npy')\n",
    "        \n",
    "        if os.path.exists(fname_out)==False:\n",
    "            \n",
    "            data = np.load(fname2)\n",
    "            for f in range(0,data.shape[0],1):\n",
    "\n",
    "                for k in range(data.shape[1]):\n",
    "                    # \n",
    "                    x = data[f,k,feature_ids,0]\n",
    "                    y = data[f,k,feature_ids,1]\n",
    "\n",
    "                    x, y = self.reject_outliers2(x,y, max_dist)\n",
    "\n",
    "                    data[f,k,feature_ids,0] = x\n",
    "                    data[f,k,feature_ids,1] = y\n",
    "\n",
    "            np.save(fname_out, data)\n",
    "\n",
    "    def reject_outliers(self, max_dist=40):\n",
    "        \n",
    "        print (\"  ... rejecting outliers....\")\n",
    "        \n",
    "        self.fnames = glob.glob(self.root_dir+\"/*_compressed.npy\")\n",
    "\n",
    "        if self.parallel:\n",
    "            parmap.map(self.reject_outliers1, self.fnames, \n",
    "                       self.feature_ids,\n",
    "                       max_dist,\n",
    "                       pm_processes=8,\n",
    "                       pm_pbar=True)\n",
    "        else:\n",
    "            for fname in fnames:\n",
    "                pass\n",
    "\n",
    "\n",
    "    def centre_and_align2(self, data, frame, centre_pt=0):\n",
    "\n",
    "        if True:\n",
    "            # centre the data on the nose\n",
    "            data[:,0] -= data[centre_pt,0]\n",
    "            data[:,1] -= data[centre_pt,1]\n",
    "\n",
    "            # get angle between +x axis and head location (i.e. 2nd position)\n",
    "            t = -np.arctan2(*data[1].T[::-1])-np.pi/2\n",
    "\n",
    "            # get rotation\n",
    "            rotmat = np.array([[np.cos(t), -np.sin(t)], \n",
    "                               [np.sin(t),  np.cos(t)]])\n",
    "\n",
    "            # Apply rotation to each row of m\n",
    "            m2 = (rotmat @ data.T).T\n",
    "\n",
    "            return m2\n",
    "\n",
    "    \n",
    "    def centre_and_align(self):\n",
    "        \n",
    "        print (\"  ... center and aligning ...\")\n",
    "\n",
    "        self.fnames = glob.glob(self.root_dir+\"/*_compressed.npy\")\n",
    "\n",
    "        if self.parallel:\n",
    "            parmap.map(self.centre_and_align1, self.fnames, \n",
    "                       self.feature_ids,\n",
    "                       pm_processes=8,\n",
    "                       pm_pbar=True)\n",
    "        else:\n",
    "            for fname in fnames:\n",
    "                pass\n",
    "            \n",
    "\n",
    "    def centre_and_align1(self, fname,\n",
    "                         feature_ids):\n",
    "\n",
    "        fname2 = fname.replace('.npy','_median_filtered_outliers.npy')\n",
    "        \n",
    "        fname_out = fname2.replace('.npy','_centre_aligned.npy')\n",
    "        #fname_out_good_only = fname2.replace('.npy','_centre_aligned.npy')\n",
    "        \n",
    "        if os.path.exists(fname_out)==False:\n",
    "            \n",
    "            data = np.load(fname2)\n",
    "\n",
    "            # \n",
    "            centre_pt = 0\n",
    "\n",
    "            features_full = np.zeros((data.shape[0],data.shape[1],feature_ids.shape[0],2), \n",
    "                                          'float32')+np.nan\n",
    "\n",
    "            features_array = []\n",
    "            for k in range(4):\n",
    "                features_array.append([])\n",
    "\n",
    "            for f in range(0,data.shape[0],1):\n",
    "\n",
    "                # loop over each animal\n",
    "                for k in range(data.shape[1]):\n",
    "\n",
    "                    x = data[f,k,feature_ids,0]\n",
    "                    y = data[f,k,feature_ids,1]\n",
    "\n",
    "                    idx = np.where(np.isnan(x))[0]\n",
    "                    if idx.shape[0]==0:\n",
    "                        #print (f, k, x.shape)\n",
    "\n",
    "                        locs = np.vstack((x,y)).T\n",
    "\n",
    "                        # centre and align data\n",
    "                        locs_pca = self.centre_and_align2(locs,f,centre_pt)\n",
    "\n",
    "                        if locs_pca is not None:\n",
    "                            idx = np.where(np.isnan(locs_pca))[0]\n",
    "\n",
    "                            if idx.shape[0]>0:\n",
    "                                continue\n",
    "                                \n",
    "                            features_full[f,k] = locs_pca\n",
    "                            features_array[k].append(locs_pca)\n",
    "\n",
    "            np.save(fname_out, features_full)\n",
    "\n",
    "    def load_processed_data(self, data_type=1, remove_nans=True):\n",
    "        \n",
    "        ''' data_type can be 3 types:\n",
    "            0: filtered, outlier cleaned, centred and rotated on the nose/head\n",
    "            1: filtered, outlier cleaned <- currently used\n",
    "            2: filtered only <- very noisy data with many outlier vals\n",
    "        \n",
    "            remove_nans - cleans up the data so only full body axis data is used\n",
    "            True: loading clean data for model training\n",
    "            False: loading all data for prediction\n",
    "        \n",
    "        '''\n",
    "            \n",
    "        March16_file_order = [\n",
    "\n",
    "        '2020-3-16_11-56-56-704655',  # day time starts correct day\n",
    "        '2020-3-16_12-57-12-418305',\n",
    "        '2020-3-16_01-57-27-327194',\n",
    "        '2020-3-16_02-57-41-995158',\n",
    "        '2020-3-16_03-57-56-902379',\n",
    "        '2020-3-16_04-58-11-998956',\n",
    "        '2020-3-16_05-58-27-193818',\n",
    "        '2020-3-16_06-58-43-678014',\n",
    "        '2020-3-16_07-59-00-362242',\n",
    "        '2020-3-16_08-59-17-534732',\n",
    "        '2020-3-16_09-59-34-731308',\n",
    "        '2020-3-16_10-59-50-448686',\n",
    "\n",
    "        '2020-3-16_12-54-07-193951',  # night time of previous day though\n",
    "        '2020-3-16_01-54-23-358257',\n",
    "        '2020-3-16_02-54-39-170978',\n",
    "        '2020-3-16_03-54-54-231226',\n",
    "        '2020-3-16_04-55-09-841582',\n",
    "        '2020-3-16_05-55-25-305681',\n",
    "        '2020-3-16_06-55-40-714236',\n",
    "        '2020-3-16_07-55-55-775234',\n",
    "        '2020-3-16_08-56-11-096689',\n",
    "        '2020-3-16_09-56-26-362091',\n",
    "        '2020-3-16_10-56-41-406701',\n",
    "        ]\n",
    "\n",
    "        # stack the postures for each animal for the day \n",
    "        features_array = []\n",
    "        for k in range(4):\n",
    "            features_array.append([])\n",
    "\n",
    "        for file in tqdm(March16_file_order):\n",
    "\n",
    "            # this data is filtered, outlier triaged + centred to nose and aligned to face up\n",
    "            if data_type==0:\n",
    "                fname = glob.glob(os.path.join(cb.root_dir,file+\"*_centre_aligned.npy\").replace(\"-\",\"_\"))[0]\n",
    "                d3 = np.load(fname)\n",
    "\n",
    "            # this data is filtered and outlier triaged\n",
    "            if data_type==1:\n",
    "                fname = glob.glob(os.path.join(cb.root_dir,file+\"*_median_filtered_outliers.npy\").replace(\"-\",\"_\"))[0]    \n",
    "\n",
    "                d3 = np.load(fname)\n",
    "                d3 = d3[:,:,self.feature_ids]  \n",
    "\n",
    "            # this data is not outlier traiged; - very poor results/bad to work with\n",
    "            if data_type==2:\n",
    "                fname = glob.glob(os.path.join(cb.root_dir,file+\"*_median_filtered.npy\").replace(\"-\",\"_\"))[0]    \n",
    "\n",
    "                d3 = np.load(fname)\n",
    "                d3 = d3[:,:,self.feature_ids]       \n",
    "\n",
    "            #print (\"d3: \", d3.shape)\n",
    "            # loop over animals and keep only complete data (i.e. 6 pts)\n",
    "            for k in range(4):\n",
    "                \n",
    "                # find nans and delete any frame that is missing even a single value for that animal\n",
    "                if remove_nans:\n",
    "\n",
    "                    idx = np.where(np.isnan(d3[:,k]))\n",
    "                    ids, counts = np.unique(idx[0], return_counts=True)\n",
    "#                     print (k, \"Coutns: \", counts.shape, counts)\n",
    "#                     print (k, \"ids: \", ids.shape, \" idx: \", idx[0].shape, idx[0])\n",
    "                    \n",
    "                    idx_all = np.arange(d3.shape[0])\n",
    "                    idx_good = np.delete(idx_all, ids)\n",
    "\n",
    "                    temp = d3[idx_good,k]\n",
    "                \n",
    "                    features_array[k].append(temp)\n",
    "                else:\n",
    "                    features_array[k].append(d3[:,k])\n",
    "\n",
    "                    \n",
    "        # return stacked feature array\n",
    "        self.features_array = features_array\n",
    "\n",
    "    \n",
    "cb = CentreBody()\n",
    "cb.parallel = True\n",
    "cb.root_dir = '/media/cat/1TB/dan/cohort1/slp/'\n",
    "cb.get_fnames()\n",
    "\n",
    "# median filter data\n",
    "cb.filter_data()\n",
    "\n",
    "# \n",
    "cb.reject_outliers()\n",
    "\n",
    "#\n",
    "cb.centre_and_align()\n",
    "\n",
    "########################################\n",
    "# data_type:  0 - filtered, outlier triaged + centred to nose and aligned to face up\n",
    "#             1 - filtered, outlier triaged \n",
    "#             2 - filtered\n",
    "cb.load_processed_data(data_type=0, \n",
    "                       remove_nans=True)\n",
    "\n",
    "print (\" DATA SIZES: \")\n",
    "print (\" female: \", np.vstack(cb.features_array[0]).shape)\n",
    "print (\" male: \", np.vstack(cb.features_array[1]).shape)\n",
    "print (\" pup1: \", np.vstack(cb.features_array[2]).shape)\n",
    "print (\" pup2: \", np.vstack(cb.features_array[3]).shape)\n",
    "\n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data:  (218641, 6, 2)\n",
      "(218641, 12)\n",
      "... predting data ...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "################ IMPUTATION STEP ##################\n",
    "####################################################\n",
    "\n",
    "# \n",
    "class Impute():\n",
    "    \n",
    "    def __init__(self, root_dir):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    #   \n",
    "    def evaluate_imputation_error(self,features_array, animal_id, res, idx_train, idx_test):\n",
    "\n",
    "        temp = np.vstack(features_array[animal_id])\n",
    "        temp = temp-temp[:,0][:,None]\n",
    "\n",
    "        fig=plt.figure()\n",
    "        ax=plt.subplot()\n",
    "\n",
    "        diff = np.abs(temp[idx_test]-res)\n",
    "        #print (\"diff: \", diff.shape)\n",
    "\n",
    "        errors = []\n",
    "        for k in range(diff.shape[1]):\n",
    "            errors.append([])\n",
    "\n",
    "        for k in range(diff.shape[0]):\n",
    "            for p in range(diff.shape[1]):\n",
    "                temp = diff[k,p]\n",
    "                #print (temp)\n",
    "                tdiff = np.linalg.norm(temp)\n",
    "                if tdiff>0:\n",
    "                    errors[p].append(tdiff)\n",
    "\n",
    "        t =[]\n",
    "        for k in range(len(errors)):\n",
    "            temp = errors[k]\n",
    "            pad = np.zeros(100000-len(errors[k]),'float32')+np.nan\n",
    "            temp = np.concatenate((temp, pad))\n",
    "            t.append(temp)\n",
    "\n",
    "        data = np.array(t).T\n",
    "        #print (data.shape)\n",
    "        columns = ['nose','spine1','spine2', 'spine3', 'spine4', 'spine5']\n",
    "        #columns = ['errors']\n",
    "        df = pd.DataFrame(data, columns = columns)\n",
    "\n",
    "        #print (\"DF: \", df)\n",
    "\n",
    "        # plot\n",
    "        ax=plt.subplot(2,1,1)\n",
    "        sns.violinplot(data=df) #x=df['spine2'])\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.ylabel(\" pixel error\")\n",
    "\n",
    "        ax=plt.subplot(2,1,2)\n",
    "        plt.title(\"Zoom\",fontsize=20)\n",
    "        sns.violinplot(data=df) #x=df['spine2'])\n",
    "        plt.ylabel(\" pixel error\")\n",
    "        plt.ylim(0,50)\n",
    "\n",
    "         \n",
    "    # \n",
    "    def predict_imputation_ground_truth(self, \n",
    "                                        drops=None, \n",
    "                                        idx_test=None, \n",
    "                                        n_drops = 3):\n",
    "\n",
    "        '''  Function used to test the prediction between the ground truth complete body axis and dropout data\n",
    "        \n",
    "            Need to set the 'remove_nans' flag about to True to get clean model data \n",
    "            \n",
    "            Need to set the idx_test to ~90% of data and run validation on the 10% holdout.\n",
    "        \n",
    "        '''\n",
    "            \n",
    "        # \n",
    "\n",
    "        \n",
    "        if self.fname_dropout is not None:\n",
    "            X_test = np.loadtxt(self.fname_dropout)\n",
    "            print (X_test.shape)\n",
    "            \n",
    "            idx_drop = []\n",
    "            \n",
    "        # GENERATE RANDOM DROPS\n",
    "        else:\n",
    "            # \n",
    "            print (\"LOADING head-egocentric clean data\")\n",
    "            self.cb.load_processed_data(data_type=0, remove_nans=True)\n",
    "            temp = np.vstack(self.cb.features_array[self.animal_id])\n",
    "            print (temp.shape)\n",
    "\n",
    "            print (\"Centering to head only <----------- NEEDS UPDATE FOR OTHER EGOCENTRIC DATA\")\n",
    "            temp = temp-temp[:,0][:,None]\n",
    "\n",
    "            X_all = temp.reshape(temp.shape[0],-1)\n",
    "\n",
    "            # select frames to predict\n",
    "            if idx_test is not None:\n",
    "                X_test = X_all[idx_test]\n",
    "            else:\n",
    "                X_test = X_all\n",
    "\n",
    "            # do drop outs in the test set:\n",
    "            X_test = X_test.reshape(-1,6,2)\n",
    "            idx_drop = np.zeros((n_drops, X_test.shape[0]),'int32')\n",
    "            for k in range(idx_drop.shape[1]):#n_drops):\n",
    "                if drops is None:\n",
    "                    idx_drop[:,k] = np.random.choice(np.arange(6),n_drops,replace=False)\n",
    "                else:\n",
    "                    idx_drop[:,k] = drops #np.random.choice(np.arange(6),n_drops,replace=False)\n",
    "\n",
    "\n",
    "            print (\"idx drop: \", idx_drop[0].shape)\n",
    "            for k in range(len(idx_drop)):\n",
    "                for p in range(idx_drop[k].shape[0]):\n",
    "                    X_test[p,idx_drop[k][p]]=np.nan\n",
    "\n",
    "            # \n",
    "            X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "\n",
    "        \n",
    "        # TEST STEP\n",
    "        #res = imp.transform(X_test).reshape(-1,6,2)\n",
    "        print (\"... predting data ...\")\n",
    "        #res = self.models[self.animal_id][0].transform(X_test).reshape(-1,6,2)\n",
    "        res = self.model.transform(X_test).reshape(-1,6,2)\n",
    "        #print (\"Res: \", res.shape)\n",
    "\n",
    "        self.res = res\n",
    "        self.drop = idx_drop\n",
    "        \n",
    "        return res, idx_drop\n",
    "\n",
    "    #\n",
    "    def load_models(self):\n",
    "        \n",
    "        body_centre = 0\n",
    "        # \n",
    "        \n",
    "        #fname_in = self.root_dir+\"imputation_animal_id\"+str(self.animal_id)+\"_body_centre\"+str(p)+\".pckl\"\n",
    "        fname_in = os.path.join(self.root_dir,\n",
    "                                \"model_type\"+str(self.model_type)+\n",
    "                                \"_imputation_animal_id\"+str(self.animal_id)+\n",
    "                                \"_body_centre\"+str(body_centre)+\".pckl\")\n",
    "        \n",
    "        with open(fname_in, \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    #\n",
    "    def calculate_missing_features(self):\n",
    "        \n",
    "        ''' Function used to grab statistisc of missing data\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        animal_ids = np.arange(4)\n",
    "\n",
    "        idxc_array = []\n",
    "        for animal_id in tqdm(animal_ids):\n",
    "            I.animal_id = animal_id\n",
    "            idxc_array.append([])\n",
    "            for max_drops in range(7):\n",
    "\n",
    "                # \n",
    "                temp = np.vstack(self.cb.features_array[self.animal_id])\n",
    "\n",
    "                # find which frames have that feature intact\n",
    "                idx = np.where(np.isnan(temp))\n",
    "                ids, counts = np.unique(idx[0], return_counts=True)\n",
    "\n",
    "                # these are the frames where at least max_drops worth of frames are present\n",
    "                idxc = np.where(counts<=max_drops*2)[0]\n",
    "\n",
    "                # these are frames where all 6 features are available.\n",
    "                idx_all = np.arange(temp.shape[0])\n",
    "                idx_good = np.delete(idx_all, ids)\n",
    "                \n",
    "                idxc_array[animal_id].append(idxc.shape[0] + idx_good.shape[0])\n",
    "        \n",
    "        self.idxc_array = idxc_array\n",
    "        print (self.idxc_array)\n",
    "        ax=plt.subplot()\n",
    "\n",
    "        t = np.arange(6,-1,-1)\n",
    "        for k in range(len(self.idxc_array)):\n",
    "            plt.plot(t, np.array(self.idxc_array[k])/2069710., label=\" animal: \"+str(k))\n",
    "            #plt.plot(t, I.idxc_array[k], label=\" animal: \"+str(k))\n",
    "\n",
    "        plt.legend()\n",
    "        plt.ylim(0,1)\n",
    "        plt.xlabel(\"min # of present features \", fontsize=20)\n",
    "        ax.ticklabel_format(useOffset=False)\n",
    "        ax.ticklabel_format(style='plain')\n",
    "        ax.set_ylabel(\"% of total # of frames in 23hr period\",\n",
    "                     fontsize=20)\n",
    "        plt.plot([3,3],[0,1],'--')\n",
    "        #plt.xlim(-0.1,6.1)\n",
    "        plt.show()\n",
    "        \n",
    "    #\n",
    "    \n",
    "    \n",
    "    \n",
    "    def plot_imputation_results(self, features_array, animal_id, idx_test, res, idx_drop):\n",
    "\n",
    "        # \n",
    "        labels = ['n','s1','s2','s3','s4','s5']\n",
    "\n",
    "        # grab the selected data:\n",
    "        temp = np.vstack(features_array[animal_id])\n",
    "        temp = temp-temp[:,0][:,None]\n",
    "\n",
    "        #print (\"temp: \", temp.shape)\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes()\n",
    "        shift = 0\n",
    "        for k in range(10):\n",
    "            plt.subplot(2,5,k+1)\n",
    "\n",
    "            ############ PLOT GROUND TRUTH ##############\n",
    "            id2 = np.random.choice(idx_test,1)[0]\n",
    "            #print (id2)\n",
    "\n",
    "            #print (temp[id2].shape)\n",
    "            print (temp[id2,:,0])\n",
    "            plt.scatter(temp[id2,:,0],\n",
    "                        temp[id2,:,1],\n",
    "                        c='blue',\n",
    "                        s=np.arange(1,7,1)[::-1]*20,\n",
    "                        alpha=.7,\n",
    "                        edgecolor='black', label='truth')\n",
    "\n",
    "            #\n",
    "            id3 = np.where(idx_test==id2)[0]\n",
    "            #print (id3)\n",
    "\n",
    "            ############ PLOT IMPUTED LOCS ##############\n",
    "            plt.scatter(res[id3,:,0]+shift,\n",
    "                        res[id3,:,1],\n",
    "                        c='red',\n",
    "                        s=np.arange(1,7,1)[::-1]*20,\n",
    "                        alpha=.7,\n",
    "                        edgecolor='black', label='imputed')\n",
    "\n",
    "            if True: #k==0:\n",
    "                for p in range(6):\n",
    "    #                 plt.text(res[id3,p,0],\n",
    "    #                          res[id3,p,1],labels[p])\n",
    "                    plt.text(temp[id2,p,0],\n",
    "                             temp[id2,p,1],labels[p])\n",
    "\n",
    "            # draw lines\n",
    "            for p in range(len(idx_drop)):\n",
    "                #print (\"connectgin: \", p, idx_drop[p][id3])\n",
    "                #print (\"idx_drop full: \", np.array(idx_drop).shape)\n",
    "                plt.plot([temp[id2,idx_drop[p][id3],0], res[id3,idx_drop[p][id3],0]+shift],\n",
    "                         [temp[id2,idx_drop[p][id3],1], res[id3,idx_drop[p][id3],1]],\n",
    "                         '--',c='black')\n",
    "\n",
    "            if k==0:\n",
    "                plt.legend(fontsize=8)\n",
    "\n",
    "            plt.title(\"frame id: \"+str(id2) + \"\\ndrops: \"+str(np.array(idx_drop)[:,id3]),fontsize=8)\n",
    "\n",
    "            x1 = (np.max(np.abs(temp[id2,:,0])), \n",
    "                                 np.max(np.abs(res[id3,:,0])),\n",
    "                                 np.max(np.abs(temp[id2,:,1])), \n",
    "                                 np.max(np.abs(res[id3,:,1])))\n",
    "            #print (\"x1: \", x1)\n",
    "\n",
    "            max_ = np.max(x1)*1.2\n",
    "            #plt.xlim(-max_, max_+shift)\n",
    "            plt.xlim(-200, 200)\n",
    "            plt.ylim(-200,200)\n",
    "            #plt.ylim(-max_,10)\n",
    "            ax.set_aspect('equal', 'datalim')\n",
    "            #print ('')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def predict_imputation_new_data(self, max_drops=3):\n",
    "        \n",
    "        ''' Function used to impute missing data\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # NED\n",
    "        \n",
    "        print (\"NEED TO LOAD NOnNAN DATA FIRST\")\n",
    "        self.cb.load_processed_data(data_type=0, remove_nans=False)\n",
    "        \n",
    "        # \n",
    "        all_locs = np.vstack(self.cb.features_array[self.animal_id])\n",
    "\n",
    "        # find which frames have that feature intact\n",
    "        idx = np.where(np.isnan(all_locs))\n",
    "        ids, counts = np.unique(idx[0], return_counts=True)\n",
    "        \n",
    "        #idx_all = np.arange(all_locs.shape[0])\n",
    "        #idx_good = np.delete(idx_all, ids)\n",
    "\n",
    "        # find frame Ids where at most max_drops of features are missing\n",
    "        idxc = np.where(counts<=max_drops*2)[0]\n",
    "        \n",
    "        # \n",
    "        X_all = all_locs[idxc]\n",
    "        print (\"X_all: \", X_all.shape)\n",
    "        \n",
    "        # loop over each feature\n",
    "        res = np.zeros((6,X_all.shape[0],\n",
    "                              X_all.shape[1],\n",
    "                              X_all.shape[2]),'float32')+np.nan\n",
    "        for k in range(6):\n",
    "            \n",
    "            # find frames where current feature is present so we can apply model to it\n",
    "            idx_feature = np.where(np.isnan(X_all[:,k,0])==False)[0]\n",
    "            \n",
    "            X_test = X_all[idx_feature]\n",
    "\n",
    "            # load model\n",
    "            temp_model = self.models[self.animal_id][k]\n",
    "            #print (k, temp_model)\n",
    "\n",
    "            # centre the data on the selected feature\n",
    "            offsets = X_test[:,k]\n",
    "            X_test = X_test-offsets[:,None]\n",
    "            \n",
    "            #print (\"XTES: \", X_test.shape)\n",
    "\n",
    "            # format the data\n",
    "            X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "        \n",
    "            # run mprediction\n",
    "            r = temp_model.transform(X_test).reshape(-1,6,2)\n",
    "            print (\"Result: \", r.shape)\n",
    "            \n",
    "            # add offset back in\n",
    "            r = r + offsets[:,None]\n",
    "            #\n",
    "            res[k,idx_feature] = r\n",
    "\n",
    "        \n",
    "        # average over all models \n",
    "        print (\" res: \", res.shape)\n",
    "        #res_ave = np.nanmean(res,axis=0)\n",
    "        \n",
    "        # use only front body prediction averages\n",
    "        #res_ave = np.nanmedian(res[:3],axis=0)\n",
    "        #print (\" res_ave; \", res_ave.shape)\n",
    "        \n",
    "        res_ave = res[0]\n",
    "        \n",
    "        return res_ave, all_locs, idxc\n",
    "\n",
    "    \n",
    "        # \n",
    "    def generate_imputation_models(self):\n",
    "        \n",
    "        \n",
    "        from sklearn.linear_model import BayesianRidge\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.ensemble import ExtraTreesRegressor\n",
    "        from sklearn.neighbors import KNeighborsRegressor\n",
    "        \n",
    "        self.model_type_names = [\"BayesianRidge\",\n",
    "                                 \"DecisionTreeRegressor\",\n",
    "                                 \"ExtraTreesRegressor\",\n",
    "                                 \"KNeighborsRegressor\"]\n",
    "        \n",
    "        for animal_id in self.animal_ids:\n",
    "\n",
    "            # \n",
    "            temp = np.vstack(self.cb.features_array[animal_id])\n",
    "            print (\"Raw Data: \", temp.shape)\n",
    "\n",
    "            # \n",
    "            body_centres = [self.body_centre]\n",
    "            \n",
    "            for k in body_centres:\n",
    "\n",
    "                fname_out = self.root_dir+\"model_type\"+str(self.model_type)+\"_imputation_animal_id\"+str(animal_id)+\"_body_centre\"+str(k)+\".pckl\"\n",
    "                if os.path.exists(fname_out)==False:\n",
    "                    \n",
    "                    # centre the data on the particular feature being trained on\n",
    "                    # TODO: try further aligning to 2 points; might give more stable results...\n",
    "                    temp = temp-temp[:,k][:,None]\n",
    "\n",
    "                    X_train = temp.reshape(temp.shape[0],-1)\n",
    "                    print (\"X_train: \", X_train.shape)\n",
    "\n",
    "                    #\n",
    "                    estimators = [BayesianRidge(),\n",
    "                    DecisionTreeRegressor(max_features='sqrt', random_state=0),\n",
    "                    ExtraTreesRegressor(n_estimators=10, random_state=0),\n",
    "                    KNeighborsRegressor(n_neighbors=15)]\n",
    "\n",
    "                    print (\"fitting...animal id: \", animal_id, \"  body centre: \", k)\n",
    "                    imp = IterativeImputer(max_iter=10, \n",
    "                                           #n_nearest_features = 2,\n",
    "                                           #sample_posterior = True,\n",
    "                                           estimator=estimators[self.model_type],\n",
    "                                           random_state=0)\n",
    "                    imp.fit(X_train)\n",
    "                    print (\"done\")\n",
    "\n",
    "                    # \n",
    "                    with open(fname_out, \"wb\") as f:\n",
    "                        pickle.dump(imp, f)\n",
    "\n",
    "                        \n",
    "#\n",
    "root_dir = '/media/cat/1TB/dan/'\n",
    "I = Impute(root_dir)\n",
    "I.cb = cb\n",
    "I.animal_id = 0\n",
    "I.animal_ids = [I.animal_id]\n",
    "\n",
    "\n",
    "# generate all required models; need clean data only\n",
    "I.model_type = 2\n",
    "I.body_centre = 0  # use head fixed only for now\n",
    "I.generate_imputation_models()\n",
    "I.load_models()\n",
    "\n",
    "\n",
    "# test against ground truth/cleand data \n",
    "I.fname_dropout = '/home/cat/feats_dropout.tsv'\n",
    "res, idx_drop = I.predict_imputation_ground_truth(drops=np.arange(3,6,1)) # drops = 'fixed' or None\n",
    "\n",
    "# check missing features in the data\n",
    "# I.calculate_missing_features()\n",
    "\n",
    "# \n",
    "# I.plot_imputation_results(features_array, animal_id, idx_test, res, idx_drop)\n",
    "# plt.suptitle(\"animal \"+str(animal_id)+ \" imputed vs. ground truth\",fontsize=20)\n",
    "\n",
    "# # \n",
    "# I.evaluate_imputation_error(features_array, animal_id, res, idx_train, idx_test)\n",
    "# plt.suptitle(\"animal \"+str(animal_id)+ \"  Egocentric (fixed nose) errors (pixels)\",fontsize=20)\n",
    "# #plt.suptitle(\"animal \"+str(animal_id)+ \" NON-Egocentric (fixed nose) errors (pixels)\",fontsize=20)\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# print (res_ave.shape)\n",
    "# print (all_locs.shape)\n",
    "# print (idxc.shape, idxc)\n",
    "\n",
    "\n",
    "def make_vae_data():\n",
    "    import csv\n",
    "\n",
    "    print (np.vstack(cb.features_array[0]).shape)\n",
    "\n",
    "    X = np.vstack(cb.features_array[0])\n",
    "    #X = X.reshape(X.shape[0],-1)\n",
    "\n",
    "    max_drop_outs = 3\n",
    "    n_frames = 50000\n",
    "    n_frames = X.shape[0]\n",
    "\n",
    "    # \n",
    "    with open('/home/cat/feats_ground_truth.tsv', 'w') as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "        #for record in SeqIO.parse(\"/home/fil/Desktop/420_2_03_074.fastq\", \"fastq\"):\n",
    "        #for k in trange(X.shape[0]): \n",
    "        for k in trange(n_frames): \n",
    "\n",
    "            temp = X[k]\n",
    "            if False:\n",
    "                idx = np.random.choice(np.arange(6), max_drop_outs, replace=False)\n",
    "\n",
    "                if idx.shape[0]>0:\n",
    "                    temp[idx]=np.nan\n",
    "            temp = temp.reshape(-1)\n",
    "            writer.writerow(temp)\n",
    "\n",
    "    with open('/home/cat/feats_dropout.tsv', 'w') as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "        #for record in SeqIO.parse(\"/home/fil/Desktop/420_2_03_074.fastq\", \"fastq\"):\n",
    "        #for k in trange(X.shape[0]): \n",
    "        for k in trange(n_frames): \n",
    "\n",
    "            temp = X[k]\n",
    "            if True:\n",
    "                idx = np.random.choice(np.arange(6),max_drop_outs, replace=False)\n",
    "\n",
    "                if idx.shape[0]>0:\n",
    "                    temp[idx]=np.nan\n",
    "            temp = temp.reshape(-1)\n",
    "            writer.writerow(temp)        \n",
    "        \n",
    "def plot_multiple_imputation_results(frame_ids, I):\n",
    "    all_locs = np.array(np.loadtxt('/home/cat/feats_ground_truth.tsv'))\n",
    "    all_locs = all_locs.reshape(all_locs.shape[0],6,2)\n",
    "    #all_locs = np.vstack(cb.features_array[0])\n",
    "    print (all_locs.shape)\n",
    "\n",
    "    #\n",
    "    drops = np.arange(3,6,1)\n",
    "    leftin = np.arange(0,3,1)\n",
    "\n",
    "    #frame_ids = [13149,3399,32567,31647,14473,48186,39608,32802,46362,34761]\n",
    "    fig=plt.figure()\n",
    "    for k in range(10):\n",
    "    #\n",
    "        #id_ = np.random.choice(all_locs.shape[0],1)\n",
    "        id_ = frame_ids[k]\n",
    "        #print (idx_drop[:,id_].T)\n",
    "    #     if np.array_equal(idx_drop[:,id_].T[0], drops):\n",
    "    #         break\n",
    "\n",
    "        # \n",
    "        ax=plt.subplot(2,5,k+1)\n",
    "\n",
    "        ############## PLOT GROUND TRUTH #################\n",
    "        plt.scatter(all_locs[id_,:,0]-100,\n",
    "                    all_locs[id_,:,1],\n",
    "                    s=np.arange(1,7,1)[::-1]*50,\n",
    "                    c='black'\n",
    "                   )\n",
    "\n",
    "        plt.scatter(all_locs[id_,drops,0]-100,\n",
    "                    all_locs[id_,drops,1],\n",
    "                    s=np.arange(1,7,1)[::-1][drops]*50,\n",
    "                    c='blue'\n",
    "                   )    \n",
    "\n",
    "\n",
    "        ############## PLOT IMPUTED #################\n",
    "        plt.scatter(res[id_,:,0],\n",
    "                   res[id_,:,1],\n",
    "                    s=np.arange(1,7,1)[::-1]*50,\n",
    "                    c='red'\n",
    "                   )\n",
    "\n",
    "        plt.scatter(res[id_,leftin,0],\n",
    "                    res[id_,leftin,1],\n",
    "                    s=(np.arange(7,0,-1)[leftin]*50),\n",
    "                    c='black'\n",
    "                   )\n",
    "        plt.title(str(id_))\n",
    "\n",
    "    #plt.suptitle(str(I.model_type_names[I.model_type]))\n",
    "    plt.suptitle(I.model_type_names[I.model_type])\n",
    "\n",
    "    \n",
    "def plot_vae_scatter():\n",
    "\n",
    "    #\n",
    "    imputed = np.loadtxt('/home/cat/data_imputed.tsv')\n",
    "\n",
    "    #\n",
    "    gt = np.loadtxt('/home/cat/feats_ground_truth.tsv')\n",
    "    gt_dropout = np.loadtxt('/home/cat/feats_dropout.tsv')\n",
    "\n",
    "    #\n",
    "    print (gt.shape)\n",
    "    print (imputed.shape)\n",
    "    print (gt_dropout.shape)\n",
    "\n",
    "    frame_ids = []\n",
    "    fig=plt.figure()\n",
    "    for k in range(10):\n",
    "        #\n",
    "        while True:\n",
    "\n",
    "            id_ = np.random.choice(np.arange(gt.shape[0]),1)\n",
    "\n",
    "            # PLOT GT\n",
    "\n",
    "            temp = gt_dropout[id_]\n",
    "            #temp = temp.reshape()\n",
    "           # print (temp)\n",
    "            idx = np.where(np.isnan(temp))[1]\n",
    "            #print (idx)\n",
    "            \n",
    "            # look only for the bottom 3 feature\n",
    "            if np.array_equal(idx,np.arange(6,12,1))==False:\n",
    "                continue\n",
    "\n",
    "\n",
    "            temp = gt[id_]\n",
    "            temp = temp.reshape(6,2)\n",
    "\n",
    "            ax=plt.subplot(2,5,k+1)\n",
    "            plt.scatter(temp[:,0]-100,\n",
    "                       temp[:,1],\n",
    "                       s=np.arange(1,7,1)[::-1]*50,\n",
    "                       alpha=1,\n",
    "                       c='blue')\n",
    "\n",
    "            temp = gt_dropout[id_]\n",
    "            temp = temp.reshape(6,2)\n",
    "            plt.scatter(temp[:,0]-100,\n",
    "                       temp[:,1],\n",
    "                       s=np.arange(1,7,1)[::-1]*50,\n",
    "                       alpha=1,\n",
    "                       c='black')   \n",
    "\n",
    "            # PLOT IMPUTATION\n",
    "            temp5 = imputed[int(id_*5):int(id_*5+5)]\n",
    "            print (temp5.shape)\n",
    "            for t in range(temp5.shape[0]):\n",
    "                temp = temp5[t]\n",
    "                temp = temp.reshape(6,2)\n",
    "                #print (temp.shape)\n",
    "                plt.scatter(temp[:,0],\n",
    "                       temp[:,1],\n",
    "                       s=np.arange(1,7,1)[::-1]*50,\n",
    "                       c='red',\n",
    "                       alpha=.1)\n",
    "            temp = np.mean(temp5,axis=0)\n",
    "            temp = temp.reshape(6,2)\n",
    "            plt.scatter(temp[:,0],\n",
    "                   temp[:,1],\n",
    "                   s=np.arange(1,7,1)[::-1]*50,\n",
    "                   c='red',\n",
    "                   alpha=1)        \n",
    "\n",
    "\n",
    "            temp = gt_dropout[id_]\n",
    "            temp = temp.reshape(6,2)\n",
    "            plt.scatter(temp[:,0],\n",
    "                       temp[:,1],\n",
    "                       s=np.arange(1,7,1)[::-1]*50,\n",
    "                       alpha=1,\n",
    "                       c='black')   \n",
    "            plt.title(str(id_))\n",
    "            \n",
    "            frame_ids.append(id_)\n",
    "            break\n",
    "\n",
    "    plt.suptitle(\"Variational Autoencoder with Arbitrary Conditioning (https://github.com/tigvarts/vaeac)\")\n",
    "    plt.show()\n",
    "    return frame_ids    \n",
    "\n",
    "#   \n",
    "def evaluate_imputation_vae():\n",
    "\n",
    "    # VAE ERROR\n",
    "    imputed = np.loadtxt('/home/cat/data_imputed.tsv')\n",
    "    gt = np.float32(np.loadtxt('/home/cat/feats_ground_truth.tsv'))\n",
    "    \n",
    "    gt = np.float32(gt.reshape(gt.shape[0],6,2))\n",
    "\n",
    "    \n",
    "    imputed = np.array(np.array_split(imputed, 5))\n",
    "    imputed = np.mean(imputed,axis=0)\n",
    "    \n",
    "    imputed = imputed.reshape(imputed.shape[0],6,2)\n",
    "\n",
    "    \n",
    "    diff = np.float32(np.abs(gt-imputed))\n",
    "    #print (\"diff: \", diff.shape)\n",
    "\n",
    "    errors = []\n",
    "    for k in range(diff.shape[1]):\n",
    "        errors.append([])\n",
    "\n",
    "    for k in range(diff.shape[0]):\n",
    "        for p in range(diff.shape[1]):\n",
    "            temp = diff[k,p]\n",
    "            #print (temp)\n",
    "            tdiff = np.linalg.norm(temp)\n",
    "            if tdiff>0:\n",
    "                errors[p].append(tdiff)\n",
    "\n",
    "    t =[]\n",
    "    for k in range(len(errors)):\n",
    "        temp = errors[k]\n",
    "        pad = np.zeros(350000-len(errors[k]),'float32')+np.nan\n",
    "        temp = np.concatenate((temp, pad))\n",
    "        \n",
    "        idx = np.where(temp<1E-8)[0]\n",
    "        temp[idx]=np.nan\n",
    "        t.append(temp)\n",
    "\n",
    "    errors = np.float64(t).T\n",
    "\n",
    "    columns = ['nose','spine1','spine2', 'spine3', 'spine4', 'spine5']\n",
    "    df = pd.DataFrame(errors, columns = columns)\n",
    "\n",
    "    print (df)\n",
    "\n",
    "    fig=plt.figure()\n",
    "    ax = sns.violinplot(data=df) #x=df['spine2'])\n",
    "    plt.ylim(0,50)\n",
    "    plt.title(\"VAE IMPUTATION\")\n",
    "    #plt.ylabel(\" pixel error\")\n",
    "\n",
    "    return df\n",
    "    \n",
    "def evaluate_imputation_multivariate(I):\n",
    "\n",
    "    # VAE ERROR\n",
    "    imputed = I.res #np.loadtxt('/home/cat/data_imputed.tsv')\n",
    "    gt = np.loadtxt('/home/cat/feats_ground_truth.tsv')\n",
    "    \n",
    "    gt = gt.reshape(gt.shape[0],6,2)\n",
    "\n",
    "    print (imputed.shape)\n",
    "    \n",
    "#     imputed = np.array(np.array_split(imputed, 5))\n",
    "#     print (imputed.shape)\n",
    "#     imputed = np.mean(imputed,axis=0)\n",
    "#     print (imputed.shape)\n",
    "    \n",
    "#     imputed = imputed.reshape(imputed.shape[0],6,2)\n",
    "    #gt_dropout = np.loadtxt('/home/cat/feats_dropout.tsv')\n",
    "    \n",
    "    \n",
    "#     temp = np.vstack(features_array[animal_id])\n",
    "#     temp = temp-temp[:,0][:,None]\n",
    "\n",
    "\n",
    "    diff = np.float32(np.abs(gt-imputed))\n",
    "    #print (\"diff: \", diff.shape)\n",
    "    print (\"Data type: \", type(diff[0][0][0]))\n",
    "\n",
    "    errors = []\n",
    "    for k in range(diff.shape[1]):\n",
    "        errors.append([])\n",
    "\n",
    "    for k in range(diff.shape[0]):\n",
    "        for p in range(diff.shape[1]):\n",
    "            temp = diff[k,p]\n",
    "            #print (temp)\n",
    "            tdiff = np.linalg.norm(temp)\n",
    "            if tdiff>0:\n",
    "                errors[p].append(tdiff)\n",
    "\n",
    "    t =[]\n",
    "    for k in range(len(errors)):\n",
    "        temp = errors[k]\n",
    "        pad = np.zeros(350000-len(errors[k]),'float32')+np.nan\n",
    "        temp = np.concatenate((temp, pad))\n",
    "        t.append(temp)\n",
    "\n",
    "    data = np.array(t).T\n",
    "    print (\"DAT TIPE:\", type(data[0][0]))\n",
    "    columns = ['nose','spine1','spine2', 'spine3', 'spine4', 'spine5']\n",
    "    df = pd.DataFrame(data, columns = columns)\n",
    "\n",
    "    print (df)\n",
    "\n",
    "    fig=plt.figure()\n",
    "    \n",
    "    ax = sns.violinplot(data=df) #x=df['spine2'])\n",
    "    plt.ylim(0,50)\n",
    "    plt.title(I.model_type_names[I.model_type])\n",
    "\n",
    "    return df\n",
    "#     ax=plt.subplot(2,1,2)\n",
    "#     plt.title(\"Zoom\",fontsize=20)\n",
    "#     sns.violinplot(data=df) #x=df['spine2'])\n",
    "#     plt.ylabel(\" pixel error\")\n",
    "#     plt.ylim(0,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# make_vae_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cat/anaconda3/envs/gerbil/lib/python3.7/site-packages/ipykernel_launcher.py:232: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        nose    spine1     spine2     spine3     spine4     spine5\n",
      "0        NaN  6.915198  14.171872  32.746586  13.954534  15.646737\n",
      "1        NaN  6.920685  14.155935  32.661476  13.741578  15.489763\n",
      "2        NaN  6.928537  13.574489  32.219589  13.784212  17.295628\n",
      "3        NaN  6.799797  13.708711  32.038719  12.180178  15.949262\n",
      "4        NaN  6.874421  13.581362  32.197235  12.272676  15.428333\n",
      "...      ...       ...        ...        ...        ...        ...\n",
      "349995   NaN       NaN        NaN        NaN        NaN        NaN\n",
      "349996   NaN       NaN        NaN        NaN        NaN        NaN\n",
      "349997   NaN       NaN        NaN        NaN        NaN        NaN\n",
      "349998   NaN       NaN        NaN        NaN        NaN        NaN\n",
      "349999   NaN       NaN        NaN        NaN        NaN        NaN\n",
      "\n",
      "[350000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = evaluate_imputation_vae()\n",
    "\n",
    "#df2 = evaluate_imputation_multivariate(I)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 12)\n",
      "(250000, 12)\n",
      "(50000, 12)\n",
      "(5, 12)\n",
      "(5, 12)\n",
      "(5, 12)\n",
      "(5, 12)\n",
      "(5, 12)\n",
      "(5, 12)\n",
      "(5, 12)\n",
      "(5, 12)\n",
      "(5, 12)\n",
      "(5, 12)\n",
      "frame ids:  [array([18594]), array([42767]), array([44510]), array([11645]), array([26048]), array([27577]), array([15465]), array([30459]), array([43248]), array([36295])]\n",
      "(50000, 6, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# \n",
    "frame_ids = plot_vae_scatter()\n",
    "print (\"frame ids: \", frame_ids)  \n",
    "    \n",
    "#        \n",
    "plot_multiple_imputation_results(frame_ids, I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##########################################################\n",
    "##########################################################\n",
    "from scipy.spatial import cKDTree\n",
    "import joblib\n",
    "\n",
    "def knn_triage(th, pca_wf):\n",
    "    tree = cKDTree(pca_wf)\n",
    "    dist, ind = tree.query(pca_wf, k=6)\n",
    "    dist = np.sum(dist, 1)\n",
    "    idx_keep1 = dist <= np.percentile(dist, th)\n",
    "    return idx_keep1\n",
    "\n",
    "\n",
    "\n",
    "# Fit the PCA object, but do not transform the data\n",
    "for k in range(4):\n",
    "    ax=plt.subplot(2,2,k+1)\n",
    "    \n",
    "    temp = features_array[k]\n",
    "    d = []\n",
    "    clrs = []\n",
    "    for p in range(len(temp)):\n",
    "        d.append(temp[p])\n",
    "        clrs.extend(np.zeros(temp[p].shape[0])+p)\n",
    "    \n",
    "    clrs = np.array(clrs)\n",
    "    d = np.vstack(d)\n",
    "    print (\"D: \", d.shape)\n",
    "    d = d.reshape(d.shape[0],-1)\n",
    "    continue\n",
    "    #d = sklearn.preprocessing.normalize(d)\n",
    "\n",
    "    # remove 1% of outliers\n",
    "    if True:\n",
    "        th = 95  # % of data to keep\n",
    "        idx_keep = knn_triage(th, d)\n",
    "        print (\" d before traige: \", d.shape)\n",
    "        d = d[idx_keep]\n",
    "        print (\" d after traige: \", d.shape)\n",
    "        clrs = clrs[idx_keep]\n",
    "    \n",
    "    \n",
    "    if False:\n",
    "        pca = PCA(2)\n",
    "\n",
    "        print (\"... data into pca: \", d.shape)\n",
    "\n",
    "        feats_pca = pca.fit_transform(d)\n",
    "        print (feats_pca.shape)\n",
    "\n",
    "        # \n",
    "        plt.scatter(feats_pca[::5,0],\n",
    "           feats_pca[::5,1],\n",
    "            #c=np.arange(feats_pca.shape[0])[::5],\n",
    "            c=clrs[::5],\n",
    "            alpha=.05)\n",
    "        \n",
    "    if True:\n",
    "        \n",
    "#         import gpumap\n",
    "#         #from sklearn.datasets import load_digits\n",
    "\n",
    "#         #digits = load_digits()\n",
    "#         print (\"Data into gpumap: \", d.shape)\n",
    "#         feats_pca = gpumap.GPUMAP().fit_transform(d)\n",
    "#         print (\"Data out of gpumap: \", feats_pca.shape)\n",
    "\n",
    "        import umap\n",
    "    \n",
    "        umap = umap.UMAP(n_components=2,\n",
    "                        init='random',\n",
    "                        random_state=0)\n",
    "        \n",
    "        d = d[::2]\n",
    "        clrs = clrs[::2]\n",
    "        \n",
    "        print (\"... data into umap: \", d.shape)\n",
    "        \n",
    "        if False:\n",
    "            umap_ = umap.fit(d) #[::10])\n",
    "            feats_pca = umap_.transform(d)\n",
    "        else:\n",
    "            feats_pca = umap.fit_transform(d) #[::10])\n",
    "        \n",
    "        \n",
    "            # remove 1% of outliers\n",
    "        if True:\n",
    "            th = 90  # % of data to keep\n",
    "            idx_keep = knn_triage(th, feats_pca)\n",
    "            print (\" d before traige: \", feats_pca.shape)\n",
    "            feats_pca = feats_pca[idx_keep]\n",
    "            print (\" d after traige: \", feats_pca.shape)\n",
    "            clrs = clrs[idx_keep]\n",
    "        \n",
    "        plt.scatter(feats_pca[:,0],\n",
    "               feats_pca[:,1],\n",
    "                #c=np.arange(feats_pca.shape[0])[::5],\n",
    "                c=clrs,\n",
    "                alpha=.05)\n",
    "    if False:\n",
    "        \n",
    "        #from openTSNE import TSNE\n",
    "        #print (\"... data into tsne: \", d.shape)\n",
    "        #feats_pca = TSNE().fit(d)\n",
    "        \n",
    "        \n",
    "        from fastTSNE import TSNE\n",
    "\n",
    "        tsne = TSNE(\n",
    "            n_components=2, perplexity=30, learning_rate=100, early_exaggeration=12,\n",
    "            n_jobs=4, \n",
    "            #angle=0.5, \n",
    "            initialization='random', metric='euclidean',\n",
    "            n_iter=750, early_exaggeration_iter=250, neighbors='exact',\n",
    "            negative_gradient_method='bh', min_num_intervals=10,\n",
    "            #ints_in_inverval=2, \n",
    "            #late_exaggeration_iter=100, \n",
    "            #late_exaggeration=4,\n",
    "        )\n",
    "        \n",
    "        # \n",
    "        feats_pca = tsne.fit(d)\n",
    "\n",
    "        print (\" output: \", feats_pca.shape)\n",
    "\n",
    "\n",
    "        plt.scatter(feats_pca[:,0],\n",
    "            feats_pca[:,1],\n",
    "            #c=np.arange(feats_pca.shape[0])[::5],\n",
    "            c=clrs,\n",
    "            alpha=.05)\n",
    "\n",
    "    # \n",
    "    plt.title(\"Animal:\"+str(k))\n",
    "    \n",
    "    \n",
    "plt.suptitle(\"Static vertically aligned postures\",fontsize=20)\n",
    "plt.show()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.57  4.58  6.43  8.56]\n"
     ]
    }
   ],
   "source": [
    "lens = [218641, 94647, 132861, 176982]\n",
    "\n",
    "lens = np.array(lens)\n",
    "print (np.round(lens/(23*89900)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.]\n",
      " [ 6. 12.]\n",
      " [ 3.  6.]]\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "############### IMPUTE MISSING DATA #############\n",
    "#################################################\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n",
    "X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n",
    "# the model learns that the second feature is double the first\n",
    "\n",
    "print(np.round(imp.transform(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "########## FEATURIZE BEHAVIOR CHUNKS #########\n",
    "##############################################\n",
    "from sklearn import decomposition\n",
    "import sklearn\n",
    "\n",
    "fig = plt.figure()\n",
    "X_all = []\n",
    "n_events = []\n",
    "for animal_id in animal_ids:\n",
    "    X = X4[animal_id].copy()\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    print (X.shape)\n",
    "    X_all.append(X)\n",
    "    n_events.append(X.shape[0])\n",
    "\n",
    "#     \n",
    "X_all = np.vstack(X_all)\n",
    "print (X_all.shape)\n",
    "X = sklearn.preprocessing.normalize(X_all)\n",
    "\n",
    "#\n",
    "if True:\n",
    "    pca = decomposition.PCA(n_components=3)\n",
    "\n",
    "    X_pca = pca.fit_transform(X_all)\n",
    "    print (X_pca.shape)\n",
    "    \n",
    "if False:\n",
    "    import umap\n",
    "    umap = umap.UMAP(n_components=2,\n",
    "                    init='random',\n",
    "                    random_state=0)\n",
    "\n",
    "    umap_ = umap.fit(X_all[::10])\n",
    "\n",
    "    X_pca = umap_.transform(X_all)\n",
    "        \n",
    "\n",
    "print (\"plotting: \", X_pca.shape)\n",
    "\n",
    "\n",
    "print (n_events)\n",
    "fig=plt.figure()\n",
    "for k in range(4):\n",
    "    ax = plt.subplot(2,2,k+1)\n",
    "    start = np.int32(n_events[:k]).sum()\n",
    "    end = np.int32(n_events[:k+1]).sum()\n",
    "    print (start, end)\n",
    "    plt.scatter(X_pca[start:end,0],\n",
    "                X_pca[start:end,1],\n",
    "               alpha=.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
