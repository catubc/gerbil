{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import gridspec\n",
    "from scipy import signal\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from tqdm import trange\n",
    "\n",
    "#import glob2\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "#import hdf5storage\n",
    "import csv\n",
    "from tqdm import trange\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "############ MAKE VIDEO BASES ON TRACES ####\n",
    "############################################\n",
    "\n",
    "#\n",
    "def make_video(video_name,\n",
    "               fname_chain_ids,\n",
    "               fname_traces,\n",
    "               start,\n",
    "               end):\n",
    "    \n",
    "    from tqdm import trange\n",
    "\n",
    "    # colors are fixed\n",
    "    colors_4= ['blue','red', 'yellow', 'green', 'yellow']  # Female is actually red, but brg is the color scheme\n",
    "\n",
    "    original_vid = cv2.VideoCapture(video_name)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "    # video sizes\n",
    "    size_vid = np.array([1280,1024])\n",
    "    scale = 1\n",
    "    dot_size1 = 8//scale\n",
    "    dot_size2 = 4//scale\n",
    "    shift = 10\n",
    "\n",
    "    n_features = 14\n",
    "    n_animals = 4\n",
    "    \n",
    "    # \n",
    "    original_vid.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "\n",
    "    # Initialize video out\n",
    "    fname_out = video_name[:-4]+\"_locs_added_\"+str(start)+\"_\"+str(end)+\".mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc('M','P','E','G')\n",
    "    video_out = cv2.VideoWriter(fname_out,fourcc, 25, (size_vid[0]//scale,size_vid[1]//scale), True)\n",
    "\n",
    "    for n in trange(start,end, 1):\n",
    "        ret, frame = original_vid.read()\n",
    "        cv2.putText(frame, str(n), (50, 100), font, 5, (255, 255, 0), 5)\n",
    "        frame = frame[::scale, ::scale]\n",
    "\n",
    "        # loop over the info\n",
    "        for k in range(n_animals):\n",
    "\n",
    "            ###############################################\n",
    "            ############# CNN ALG RESULTS #################\n",
    "            ###############################################\n",
    "            y = data_classification[n, k,:,0]\n",
    "            x = data_classification[n, k,:,1]\n",
    "\n",
    "            idx = np.where(np.isnan(x)==False)[0]\n",
    "            plot_flag = True\n",
    "            if idx.shape[0]>0:\n",
    "\n",
    "                xs = x[idx]\n",
    "                ys = y[idx]\n",
    "\n",
    "                for p in range(xs.shape[0]):\n",
    "                    x=int(xs[p])//scale+shift\n",
    "                    y=int(ys[p])//scale+shift\n",
    "\n",
    "                    frame[x-dot_size2:x+dot_size2,y-dot_size2:y+dot_size2]= (np.float32(\n",
    "                        matplotlib.colors.to_rgb(colors_4[k]))*255.).astype('uint8')\n",
    "\n",
    "                if plot_flag:\n",
    "\n",
    "                    cv2.putText(frame, str(chain_ids[n,k]), (y, x), font, 2, (255, 255, 0), 5)\n",
    "                    plot_flag=False\n",
    "\n",
    "\n",
    "            ###############################################\n",
    "            ######### HUNGARIAN ALG RESULTS ###############\n",
    "            ###############################################\n",
    "            if False:\n",
    "                y = data_hungarian[n, k,:,0]\n",
    "                x = data_hungarian[n, k,:,1]\n",
    "\n",
    "                idx = np.where(np.isnan(x)==False)[0]\n",
    "                plot_flag = True\n",
    "                if idx.shape[0]>0:\n",
    "\n",
    "                    xs = x[idx]\n",
    "                    ys = y[idx]\n",
    "\n",
    "                    for p in range(xs.shape[0]):\n",
    "                        x=int(xs[p])//scale\n",
    "                        y=int(ys[p])//scale\n",
    "\n",
    "                        frame[x-dot_size1:x+dot_size1,y-dot_size1:y+dot_size1]= (np.float32(\n",
    "                                    matplotlib.colors.to_rgb(colors_4[k]))*255.).astype('uint8')\n",
    "\n",
    "\n",
    "\n",
    "        #print (\"\")\n",
    "        video_out.write(frame)\n",
    "\n",
    "        #print (\"\")\n",
    "\n",
    "    video_out.release()\n",
    "    original_vid.release()\n",
    "    #cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "#  \n",
    "def get_animal_ids(frame,\n",
    "                  chain_ids,\n",
    "                  root_dir,\n",
    "                  vals,\n",
    "                  classes,\n",
    "                  n_animals,\n",
    "                  comments=False):\n",
    "\n",
    "    # \n",
    "    dirs = chain_ids[frame]\n",
    "\n",
    "    if comments:\n",
    "        print (\"DIRS: \", dirs)\n",
    "    \n",
    "    animal_ids = []\n",
    "    animal_ids2 = []\n",
    "    threshold_purity1 = 0.5\n",
    "    threshold_purity2 = 0.7\n",
    "\n",
    "    # HUNGARIAN ALGORITHM BASED SEARCH FOR MOST CORRECT IDS\n",
    "    best_ids_cts = np.zeros((5,5),'float32')\n",
    "    animals3 = []\n",
    "    for ctr_dir, dir_ in enumerate(dirs):\n",
    "        #print (\"2nd Loop: \", dir_)\n",
    "        if np.isnan(dir_)==False:\n",
    "            \n",
    "            # find the classifier values for that segment\n",
    "            idx = np.where(vals==dir_)[0]\n",
    "\n",
    "            # find predicted id;\n",
    "            # First find \n",
    "            fnames = np.sort(glob.glob(root_dir+str(int(dir_))+'/*.npz',recursive = False))\n",
    "\n",
    "            ctr_chain = 0\n",
    "            for fname in fnames:\n",
    "                if ('frame_'+str(frame).zfill(7)) in fname:\n",
    "                     break \n",
    "                ctr_chain+=1\n",
    "        \n",
    "            animal_id3 = classes[idx][ctr_chain]\n",
    "            animals3.append(animal_id3)\n",
    "            \n",
    "            idx5 = np.where(classes[idx]<1E10)[0]\n",
    "            classes_local = classes[idx[idx5]]\n",
    "            \n",
    "            classes_unique = np.unique(classes_local, return_counts=True)\n",
    "            #print (\"best id: \", best_id)\n",
    "            \n",
    "            best_ids_cts[ctr_dir, classes_unique[0]]=classes_unique[1]/idx.shape[0]\n",
    "        else:\n",
    "            animals3.append(None)\n",
    "    \n",
    "    # FIRST LOOP TO CHECK ANY GREAT MATCHES AND ZERO OUT EVERYTHING ELSE\n",
    "    for k1 in range(n_animals):\n",
    "        temp = best_ids_cts[k1]\n",
    "        max_val = np.max(temp)\n",
    "        if max_val>threshold_purity2:\n",
    "            idx8 = np.argmax(temp)\n",
    "            best_ids_cts[:,idx8]=0\n",
    "            best_ids_cts[k1,idx8]=max_val\n",
    "            \n",
    "    if comments:\n",
    "        print (best_ids_cts)\n",
    "    \n",
    "    # RENORMALIZE PROBABILITIES AFTER ZEROING OUT\n",
    "    for k1 in range(n_animals):\n",
    "        sum_ = best_ids_cts[k1].sum()\n",
    "        if sum_>0:\n",
    "            best_ids_cts[k1]*= 1./sum_\n",
    "    if comments:\n",
    "        print (best_ids_cts)    \n",
    "        \n",
    "    # FINAL LOOP TO GRAB WHATEVER IS LEFT\n",
    "    for k1 in range(n_animals):\n",
    "        max_val = np.max(best_ids_cts[k1])\n",
    "        if max_val==0: # or max_val<threshold_purity1:\n",
    "            animal_ids.append(None)\n",
    "            animal_ids2.append(animals3[k1])\n",
    "            continue\n",
    "        else:\n",
    "            idx9 = np.argmax(best_ids_cts[k1])\n",
    "            #print (\"idx9: \", idx9, classes_unique[0])\n",
    "            animal_ids.append(idx9)\n",
    "            animal_ids2.append(animals3[k1])\n",
    "            \n",
    "            best_ids_cts[:,idx9]= 0\n",
    "            # must zero out all the others\n",
    "            \n",
    "    if comments:\n",
    "        print (best_ids_cts)   \n",
    "        \n",
    "    return animal_ids, animal_ids2, dirs\n",
    "\n",
    "\n",
    "\n",
    "def get_animal_ids_function2(fname_classification,\n",
    "                             fname_traces_inferences,\n",
    "                             chain_ids,\n",
    "                             root_dir,\n",
    "                             comments,\n",
    "                             n_animals,\n",
    "                             n_features):\n",
    "    \n",
    "    fname_out = os.path.split(fname_classification)[0]+\"/animal_arrays.npy\"\n",
    "    if os.path.exists(fname_out)==False:\n",
    "\n",
    "        data = np.load(fname_classification, allow_pickle=True)\n",
    "        vals = data['vals']\n",
    "        frames = np.hstack(data['frames_array'])\n",
    "        print (\"# of lableed frames: \", vals.shape)\n",
    "        classes = data['classes']\n",
    "\n",
    "        reassembled = np.load(fname_traces_inferences)\n",
    "\n",
    "        #reassembled = np.load('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_57_12_418305_compressed/pickle/2020-3-16_12_57_12_418305_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full_traces_inferences.npz')\n",
    "        tracesx_re = reassembled[:,:,0]\n",
    "        tracesy_re = reassembled[:,:,1]\n",
    "        print (\"Lenght of tracessx: \", tracesx_re.shape)\n",
    "\n",
    "        start = 1\n",
    "        end = tracesx_re.shape[0]\n",
    "\n",
    "        # INITIALIZE ARRAYS TO BE SAVED\n",
    "        animal_ids_hungarian = np.zeros((tracesx_re.shape[0], n_networks), 'float32')\n",
    "        animal_arrays = np.zeros((n_networks, tracesx_re.shape[0], 2), 'float32')\n",
    "        \n",
    "        # loop over all video frames\n",
    "        for n in trange(start,end, 1):\n",
    "            # animal_ids is the hungarian output;\n",
    "            # animal_ids is the CNN output\n",
    "            animal_ids, animal_ids2, dirs = get_animal_ids(n,\n",
    "                                                        chain_ids,\n",
    "                                                        root_dir,\n",
    "                                                        vals,\n",
    "                                                        classes,\n",
    "                                                        n_animals,\n",
    "                                                        comments)\n",
    "\n",
    "            # \n",
    "            for k in range(0, n_features*n_animals,n_features):\n",
    "                y = tracesx_re[n,k:k+n_features]\n",
    "                x = tracesy_re[n,k:k+n_features]\n",
    "\n",
    "                idx = np.where(x!=0)[0]\n",
    "                x = x[idx]\n",
    "                y = y[idx]\n",
    "                if x.shape[0]==0:\n",
    "                    x_ave = None\n",
    "                    y_ave = None\n",
    "                else:\n",
    "                    x_ave = np.nanmean(x)\n",
    "                    y_ave = np.nanmean(y)\n",
    "\n",
    "                if animal_ids[k//n_features] is None:\n",
    "                    continue\n",
    "                animal_arrays[animal_ids[k//n_features], n] = x_ave, y_ave\n",
    "\n",
    "            animal_ids_hungarian[n]=animal_ids \n",
    "\n",
    "        np.save(fname_out, animal_arrays) # these are the mean location of the data\n",
    "        np.save(fname_out[:-4]+\"_animal_ids_hungarian.npy\", animal_ids_hungarian)\n",
    "    \n",
    "    else:\n",
    "        animal_arrays = np.load(fname_out)\n",
    "        animal_ids_hungarian = np.load(fname_out[:-4]+\"_animal_ids_hungarian.npy\")\n",
    "\n",
    "    print (animal_arrays.shape)\n",
    "    n_frames = animal_arrays.shape[1]\n",
    "\n",
    "    # \n",
    "    traces_inferences_classification_hungarian = np.zeros((n_frames, \n",
    "                                                           n_animals, \n",
    "                                                           n_features, \n",
    "                                                           2), 'float32')+np.nan\n",
    "    #\n",
    "    print (\"Combining hungarian and sleap output\")\n",
    "    \n",
    "    # combine hungarian output and slp output\n",
    "    traces_inferences = np.load(fname_traces_inferences)\n",
    "    tracesx_re = traces_inferences[:,:,0]\n",
    "    tracesy_re = traces_inferences[:,:,1]\n",
    "    for frame in trange(1,n_frames,1):\n",
    "        animals_selected = animal_ids_hungarian[frame]\n",
    "        #print (\"animals_selected: \", animals_selected)\n",
    "\n",
    "        for k in range(n_networks):\n",
    "            if np.isnan(animals_selected[k]):\n",
    "                continue\n",
    "            \n",
    "            animal_selected = int(animals_selected[k])\n",
    "                \n",
    "            featuresx = tracesx_re[frame,k*n_features:(k+1)*n_features]\n",
    "            featuresy = tracesy_re[frame,k*n_features:(k+1)*n_features]\n",
    "\n",
    "            # \n",
    "            traces_inferences_classification_hungarian[frame,\n",
    "                          animal_selected,:,0]=featuresx\n",
    "\n",
    "            traces_inferences_classification_hungarian[frame,\n",
    "                          animal_selected,:,1]=featuresy\n",
    "\n",
    "    np.save(fname_traces_inferences[:-4]+\"_hungarian.npy\", \n",
    "            traces_inferences_classification_hungarian)\n",
    "            \n",
    "    print (\"DONE making animal_arrays: \", animal_arrays.shape)\n",
    "    \n",
    "    return animal_arrays, animal_ids_hungarian, traces_inferences_classification_hungarian\n",
    "\n",
    "\n",
    "def get_single_feature_traces(fname_classification,\n",
    "                              fname_traces_inferences,\n",
    "                              animal_ids_hungarian,\n",
    "                              n_networks,\n",
    "                              features):\n",
    "    \n",
    "    fname_out = os.path.split(fname_classification)[0] + \"/locs_array.npy\"\n",
    "    if os.path.exists(fname_out)==False:\n",
    "        \n",
    "        reassembled = np.load(fname_traces_inferences)\n",
    "        tracesx_re = reassembled[:,:,0]\n",
    "        tracesy_re = reassembled[:,:,1]\n",
    "        print (\"Lenght of tracessx: \", tracesx_re.shape)\n",
    "        \n",
    "        locs_array = np.zeros((n_networks, animal_ids_hungarian.shape[0],2),'float32')+np.nan\n",
    "        \n",
    "        #for p in ids:\n",
    "        from tqdm import trange\n",
    "        for p in trange(n_networks):\n",
    "            # Plot spine1 location for each animal\n",
    "            for k in range(1, animal_ids_hungarian.shape[0]):\n",
    "                idxb = np.where(animal_ids_hungarian[k]==p)[0]\n",
    "                if idxb.shape[0]>0:\n",
    "                    tempx = tracesx_re[k,features+idxb*14]\n",
    "                    #print (\"tempx: \", tempx.shape)\n",
    "                    tempy = tracesy_re[k, features+idxb*14]\n",
    "                    locs_array[p,k]= tempx, tempy\n",
    "\n",
    "        np.save(fname_out, locs_array)\n",
    "    \n",
    "    else:\n",
    "        locs_array = np.load(fname_out)\n",
    "        \n",
    "    return locs_array\n",
    "\n",
    "# \n",
    "def cleanup_traces(locs_array, \n",
    "                  animal_id,\n",
    "                  velocity_max,\n",
    "                  velocity_min,\n",
    "                  velocity_rel_max,\n",
    "                  min_seg_len):\n",
    "    \n",
    "    data = locs_array[animal_id].copy()\n",
    "    \n",
    "    # REMOVE GARBAGE / FAILED PREDICTION\n",
    "    idx = np.where(data[:,0]==0.)[0]\n",
    "    #print (\" ZERO REMOVAL: \", idx.shape)\n",
    "    data[idx,0]=np.nan\n",
    "    data[idx,1]=np.nan\n",
    "\n",
    "    # DEL SHORT SEGS \n",
    "    idx = np.where(np.isnan(data[:,0])==True)[0]\n",
    "    diffs = idx[1:]-idx[:-1]\n",
    "    idx2 = np.where(np.logical_and(diffs>1, diffs<min_seg_len))[0]\n",
    "    #print (\"short segs deletion: \", idx2.shape)\n",
    "\n",
    "    # loop over short segs and remove them\n",
    "    for id_ in idx2:\n",
    "        data[idx[id_]:idx[id_+1]]=np.nan\n",
    "\n",
    "    # VELOCITIES\n",
    "    if True:\n",
    "\n",
    "        if False:\n",
    "            plt.scatter(np.arange(data.shape[0])/25, \n",
    "                    data.sum(1), color='black')\n",
    "            \n",
    "        #temp = temp1[1:]-temp1[:-1]\n",
    "\n",
    "        locs_temp = np.sqrt(data[:,0]**2+data[:,1]**2)\n",
    "        velocity = locs_temp[1:]-locs_temp[:-1]\n",
    "\n",
    "        # absolute velocity delete\n",
    "        idx = np.where(np.abs(velocity)>velocity_max)[0]\n",
    "        data[idx]=np.nan\n",
    "\n",
    "        # relative velocity delete\n",
    "        idx = np.where(np.logical_and(np.abs(velocity[1:])>np.abs(velocity[:-1])*velocity_rel_max,\n",
    "                                     np.abs(velocity[1:])>velocity_min))[0]\n",
    "\n",
    "        #print (\"relative vellcity deletions \", idx.shape)\n",
    "        data[idx]=np.nan   \n",
    "        \n",
    "        idx = np.where(np.logical_and(np.abs(velocity[1:])*velocity_rel_max<np.abs(velocity[:-1]),\n",
    "                                     np.abs(velocity[1:])>velocity_min))[0]\n",
    "\n",
    "        #print (\"relative vellcity deletions \", idx.shape)\n",
    "        data[idx]=np.nan   \n",
    "\n",
    "        \n",
    "\n",
    "    # DEL SHORT SEGS \n",
    "    idx = np.where(np.isnan(data[:,0])==True)[0]\n",
    "    diffs = idx[1:]-idx[:-1]\n",
    "    idx2 = np.where(np.logical_and(diffs>1, diffs<min_seg_len))[0]\n",
    "    #print (\"short segs deletion: \", idx2.shape)\n",
    "\n",
    "    # loop over short segs and remove them\n",
    "    for id_ in idx2:\n",
    "        data[idx[id_]:idx[id_+1]]=np.nan\n",
    "        \n",
    "    locs_array[animal_id] = data\n",
    "\n",
    "    return locs_array\n",
    "\n",
    "def connect_CSUA(locs_array,\n",
    "                       animal_id,\n",
    "                       max_jump, \n",
    "                       max_jump_intra_animal,     \n",
    "                       min_nearby_animal_dist):\n",
    "    \n",
    "    animal_ids = np.arange(4)\n",
    "    \n",
    "    data = locs_array[animal_id].copy()\n",
    "    #print (\"Data: \", data.shape)\n",
    "    \n",
    "    \n",
    "    idx = np.where(np.isnan(data[:,0])==False)[0]\n",
    "   # print (\"interpo: \", idx.shape)\n",
    "   # print (idx[:10])\n",
    "    diffs= idx[1:]-idx[:-1]\n",
    "    idx2 = np.where(diffs>1)[0]\n",
    "    #print (idx[idx2[:10]])\n",
    "    \n",
    "    idxnan = np.where(np.isnan(data[:,0])==True)[0]\n",
    "    diffsnan = idxnan[1:]-idxnan[:-1]\n",
    "    idx2nan = np.where(diffsnan>1)[0]\n",
    "    \n",
    "    for k in range(1,idx2.shape[0]-1,1):\n",
    "        start = idx[idx2[k]]\n",
    "        duration = diffs[idx2[k]]\n",
    "        end = idxnan[idx2nan[k+1]]+1\n",
    "        \n",
    "        dist = np.sqrt((data[start,0]-data[end,0])**2+\n",
    "                       (data[start,1]-data[end,1])**2)\n",
    "        \n",
    "        # CHECK MIN DIST; SET THIS TO CONSERVATIVE VALUE\n",
    "        if dist <= max_jump:\n",
    "                \n",
    "            #print (data[start,0], data[end,0], duration)\n",
    "            data[start:end,0]= np.linspace(data[start,0], data[end,0], duration)\n",
    "            data[start:end,1]= np.linspace(data[start,1], data[end,1], duration)\n",
    "            \n",
    "        # CHECK MORE BROADLY ENSURE NO OTHER ANIMAL NEARBY\n",
    "        elif dist <= max_jump_intra_animal:\n",
    "            for a in animal_ids:\n",
    "                if a == animal_id:\n",
    "                    continue\n",
    "                    #dists_start.append(1E10)\n",
    "                    #dists_end.append(1E10)\n",
    "                    \n",
    "                near_anim = np.sqrt((locs_array[a][start,0]-locs_array[animal_id][start,0])**2+\n",
    "                                      (locs_array[a][start,1]-locs_array[animal_id][start,1])**2)\n",
    "                \n",
    "                if np.min(near_anim)<min_nearby_animal_dist:\n",
    "                    continue\n",
    "                    \n",
    "                near_anim = np.sqrt((locs_array[a][end,0]-locs_array[animal_id][end,0])**2+\n",
    "                                      (locs_array[a][end,1]-locs_array[animal_id][end,1])**2)\n",
    "                \n",
    "                if np.min(near_anim)<min_nearby_animal_dist:\n",
    "                    continue\n",
    "\n",
    "            #print (data[start,0], data[end,0], duration)\n",
    "            data[start:end,0]= np.linspace(data[start,0], data[end,0], duration)\n",
    "            data[start:end,1]= np.linspace(data[start,1], data[end,1], duration)\n",
    "            \n",
    "    locs_array[animal_id] = data\n",
    "    \n",
    "    return locs_array\n",
    "\n",
    "\n",
    "\n",
    "def swap_traces(locs_array, \n",
    "                animal_id,\n",
    "                animal_ids):\n",
    "    '''  Function that tries to correct obvious swaps\n",
    "    '''\n",
    "    \n",
    "    # GRAB CSUAs FOR ANIMAL AND SEE IF BETTER MATCH WITH OTHER STARTS/ENDS\n",
    "    # GRAB CSUAs from main animal\n",
    "    idx = np.where(np.isnan(locs_array[animal_id])==False)[0]\n",
    "    diffs = idx[1:]-idx[:-1]\n",
    "    idx_end = np.where(diffs>1)[0]\n",
    "    \n",
    "    idxnan = np.where(np.isnan(locs_array[animal_id])==True)[0]\n",
    "    diffs = idxnan[1:]-idxnan[:-1]\n",
    "    idx_start = np.where(diffs>1)[0]\n",
    "    \n",
    "    CSUA = np.array([idxnan[idx_start],\n",
    "                    idx[idx_end]]).T\n",
    "    print (\"animal id: \", animal_id, \n",
    "           \"CSUA start: \", CSUA)\n",
    "    \n",
    "    # FIND WHICH CSUAS HAVE LARGE JUMPS FROM NEIGHBOURS\n",
    "    \n",
    "    \n",
    "#     # FIND CSUAs for other animals\n",
    "#     for a in animal_ids:\n",
    "#         idx = np.where(np.isnan(locs_array[a])==False)[0]\n",
    "#         diffs = idx[1:]-idx[:-1]\n",
    "#         idx2 = np.where(diffs>1)[0]    \n",
    "    \n",
    "    \n",
    "    return locs_array\n",
    "\n",
    "\n",
    "\n",
    "def cleanup(fname_classification, \n",
    "            animal_arrays,\n",
    "            animal_ids,\n",
    "            locs_array,\n",
    "            velocity_max = 200,\n",
    "            velocity_min = 10.,\n",
    "            velocity_rel_max = 7.,\n",
    "            min_seg_len = 5,\n",
    "            max_jump = 100, # SAFE DISTANCE TO MERGE SPLIT \n",
    "            max_jump_intra_animal = 500, # DISTANCE TO MERGE IF NO OTHER ANIMALS \"NEARBY\"\n",
    "            min_nearby_animal_dist = 30, # DISTANCE DEFINING \"NEARBY\"\n",
    "            separation_dist = 100, # Distance under which labels can no longer change location\n",
    "           ):\n",
    "\n",
    "    for animal_id in animal_ids:\n",
    "        #print (locs_array[animal_id].shape)\n",
    "\n",
    "        ###################################################\n",
    "        ################## CLEANUP TRACES #################\n",
    "        ###################################################\n",
    "        if True:\n",
    "            locs_array = cleanup_traces(locs_array,\n",
    "                              animal_id,\n",
    "                              velocity_min,\n",
    "                              velocity_min,\n",
    "                              velocity_rel_max,\n",
    "                              min_seg_len)\n",
    "\n",
    "        ###################################################\n",
    "        ############# SWAP TRACES - NOT IMPLEMENTED #######\n",
    "        ###################################################\n",
    "    #     locs_array = swap_traces(locs_array,\n",
    "    #                              animal_id,\n",
    "    #                              animal_ids)\n",
    "\n",
    "        ###################################################\n",
    "        ############## CONNECTS CSUAS  ####################\n",
    "        ###################################################\n",
    "        if True:\n",
    "            # REPEAT INTERPOLATION X TIMES\n",
    "            for k in range(5):\n",
    "                locs_array = connect_CSUA(locs_array,\n",
    "                                                animal_id,\n",
    "                                                max_jump,\n",
    "                                                max_jump_intra_animal,     \n",
    "                                                min_nearby_animal_dist)\n",
    "        ###################################################\n",
    "        ################# INTERPOLATE TRACES ##############\n",
    "        ###################################################\n",
    "        if True:\n",
    "            # Fix x,y positions for missing data to last value in time\n",
    "            # TODO: Try to recover missing data by looking at \n",
    "            #  raw heatmaps or peaks outputed by DLC\n",
    "            locs_array = interpolate_traces(locs_array,\n",
    "                                            animal_id)\n",
    "        \n",
    "        \n",
    "        ###################################################\n",
    "        ######## STOPS TRACES FROM JUMPING/JITTER #########\n",
    "        ###################################################\n",
    "        locs_array = stop_traces_jumps(locs_array, separation_dist)\n",
    "\n",
    "            \n",
    "\n",
    "    return locs_array \n",
    "\n",
    "def interpolate_traces(locs_array,\n",
    "                       animal_id):\n",
    "    \n",
    "    ''' Replace all np.nans by the last good value\n",
    "    '''\n",
    "        \n",
    "    data = locs_array[animal_id].copy()\n",
    "    #print (\"Data: \", data.shape)\n",
    "    \n",
    "    idx = np.where(np.isnan(data)==True)[0]\n",
    "    for id_ in idx:\n",
    "        data[id_]=data[id_-1]\n",
    "    \n",
    "    # go backwoards in time to ensure beginning of time also has non np.nan vals\n",
    "    data = data[::-1]\n",
    "    idx = np.where(np.isnan(data)==True)[0]\n",
    "    for id_ in idx:\n",
    "        data[id_]=data[id_-1]\n",
    "\n",
    "    data = data[::-1]\n",
    "    locs_array[animal_id] = data\n",
    "    \n",
    "    return locs_array\n",
    "\n",
    "def stop_traces_jumps(locs_array, separation_dist):\n",
    "    ''' Function that freezes traces once an animal gets near another\n",
    "    '''\n",
    "    \n",
    "    # COMPUTE INTER-ANIMAL DISTANCES AT EVERY POINT:\n",
    "    \n",
    "    for a in range(4):\n",
    "        \n",
    "        locs_a = locs_array[a]\n",
    "        \n",
    "        for aa in range(4):\n",
    "            if a==aa:\n",
    "                continue\n",
    "            locs_aa = locs_array[aa]\n",
    "            \n",
    "            dists = np.sqrt((locs_a[:,0]-locs_aa[:,0])**2+\n",
    "                            (locs_a[:,1]-locs_aa[:,1])**2)\n",
    "            \n",
    "            idx = np.where(dists<separation_dist)[0]\n",
    "            # search forward in time and fix to last value if too close \n",
    "            for id_ in idx:\n",
    "                locs_a[id_]=locs_a[id_-1]\n",
    "\n",
    "    \n",
    "    return locs_array\n",
    "\n",
    "                       \n",
    "def csv_to_npy(fname_csv):\n",
    "\n",
    "    ''' Convert csv to a numpy file with only floats\n",
    "    '''\n",
    "\n",
    "    import csv\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "\n",
    "    # LOAD .CSV DATA FIRST\n",
    "    \n",
    "    fname_out = fname_csv[:-4]+'.npy'\n",
    "    \n",
    "    if os.path.exists(fname_out)==False:\n",
    "        print (\"converting sleap .csv to .npy standard\")\n",
    "        data_tracks = []\n",
    "        with open(fname_csv) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            ctr=0\n",
    "            for row in tqdm(csv_reader):\n",
    "                #print (ctr)\n",
    "                data_tracks.append([])\n",
    "                for item in row:\n",
    "                    if item=='':\n",
    "                        item = 1E10\n",
    "                    elif 'track' in item:\n",
    "                        item = item.replace('track_','')\n",
    "                    data_tracks[ctr].append(item)\n",
    "                ctr+=1\n",
    "        data_tracks = np.array(data_tracks[1:])\n",
    "        #print (data_tracks[0])\n",
    "\n",
    "        # CONVERT DATA TO FLOAT32 and replace missing data with Nan\n",
    "        #print (data_tracks.shape)\n",
    "        data_tracks = np.float32(data_tracks)\n",
    "        #print (data_tracks)\n",
    "\n",
    "        idx = np.where(data_tracks==1E10)\n",
    "        data_tracks[idx]=np.nan\n",
    "\n",
    "        print (data_tracks.shape)\n",
    "\n",
    "        np.save(fname_out,\n",
    "               data_tracks)\n",
    "\n",
    "    else:\n",
    "        data_tracks = np.load(fname_out)\n",
    "    \n",
    "    return data_tracks\n",
    "\n",
    "\n",
    "def classifications_to_standard_output(n_features, \n",
    "                                       n_frames,\n",
    "                                       n_animals,\n",
    "                                       fname_csv,\n",
    "                                       fname_classification,\n",
    "                                       fname_chain_ids,\n",
    "                                       fname_traces_inferences):\n",
    "    \n",
    "    ''' Combine Sleap and CNN classification output to standard format for hungarian algorithm downstream\n",
    "        input:  CNN output is: classes [n_labels] usually 3-4 x n_frames\n",
    "                Sleap output converted to track_ids .npy file: [track_id, frame_id, feature1_x, feature1_y,...]\n",
    "                \n",
    "                # Note: Sleap output is already converted to:\n",
    "                #               - chain_ids [n_frames, n_animals]\n",
    "                #               - traces_inferences [n_frames, n_animals*n_features, 2]\n",
    "        \n",
    "        output: [n_frames, n_animals, n_features, 2]\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # convert the sleap .csv file to an npy containing only floats\n",
    "    data_tracks = csv_to_npy(fname_csv)\n",
    "    print (\"sleap data tracks;\", data_tracks.shape)\n",
    "    \n",
    "    # \n",
    "    classification = np.load(fname_classification)\n",
    "    classes = classification['classes']        # classification output\n",
    "    confidence = classification['confidence']  # likelihood of classifier\n",
    "    frames_array = classification['frames_array']  # this is the movie frame ids\n",
    "    vals = classification['vals']  # this is the track ids\n",
    "    \n",
    "    tracks_unique = np.int32(np.unique(vals))\n",
    "    print (\" unique tracks: \", tracks_unique)\n",
    "    \n",
    "    # start reassigning data;\n",
    "    traces_inferences_classification = np.zeros((n_frames, n_animals, n_features, 2), 'float32')+np.nan\n",
    "    print (\"traces_inferences_classification: \", traces_inferences_classification.shape)\n",
    "    \n",
    "    # loop over all unique tracks and assign them to frames based on classification\n",
    "    for track_unique in tqdm(tracks_unique):\n",
    "        idx = np.where(vals==track_unique)[0]\n",
    "        classes_selected = classes[idx]\n",
    "        \n",
    "        # \n",
    "        idx2 = np.where(data_tracks[:,0]==track_unique)[0]\n",
    "        \n",
    "        for ctr, idx2_selected in enumerate(idx2):\n",
    "            data_selected = data_tracks[idx2_selected]\n",
    "            frame_selected = int(data_selected[1])\n",
    "                \n",
    "            animal_selected = int(classes_selected[ctr])\n",
    "            \n",
    "            features_selected_x = data_selected[2:][::2]\n",
    "            features_selected_y = data_selected[2:][1::2]\n",
    "\n",
    "            if frame_selected==12276:\n",
    "                print (track_unique, ctr, idx2_selected)\n",
    "                print (\"frame_selected :\", frame_selected)\n",
    "                print (\"animal_selected :\", animal_selected)\n",
    "                print (\"features_selected_x: \", features_selected_x)\n",
    "                \n",
    "            traces_inferences_classification[frame_selected, \n",
    "                                             animal_selected,:,0]=features_selected_x\n",
    "            traces_inferences_classification[frame_selected, \n",
    "                                             animal_selected,:,1]=features_selected_y\n",
    "    \n",
    "    fname_out = fname_traces_inferences[:-4]+\"_classification.npy\"\n",
    "    print (\"Saved fname: \", fname_out)\n",
    "    np.save(fname_out,\n",
    "           traces_inferences_classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain ids:  (89987, 4)\n",
      "traces:  (89987, 56, 2)\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "######## SET FILENAMES #############\n",
    "####################################\n",
    "\n",
    "# Classifier output for CSUA; using 100 x 100 pixel images;\n",
    "fname_classification = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/classification_output.npz'\n",
    "                        #/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images/classification_output.npz\n",
    "    \n",
    "# Notebook 11 - CC based image extraction generates chain_id file \n",
    "fname_chain_ids = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis_chain_ids.npy'\n",
    "chain_ids = np.load(fname_chain_ids)\n",
    "print (\"chain ids: \", chain_ids.shape)\n",
    "\n",
    "# \n",
    "fname_csv = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis.csv'\n",
    "\n",
    "# CSUA image locations; required for hungarian; may wish to simplify;\n",
    "root_dir = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/animal_images/'\n",
    "\n",
    "# Traces post-CC step\n",
    "fname_traces_inferences = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis_traces_reassembled.npy'\n",
    "                          #'/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/2020-3-16_12_54_07_193951_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full_traces_inferences_0_89988.npz'\n",
    "traces = np.load(fname_traces_inferences)\n",
    "print (\"traces: \", traces.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################################################################\n",
    "# ###### COMBINE CNN OUTPUT AND SLEAP OUTPUT + HUNGARIAN ALGORITHM ######\n",
    "# #######################################################################\n",
    "# def get_animal_ids_function3(fname_classification,\n",
    "#                            fname_traces_inferences,\n",
    "#                            chain_ids,\n",
    "#                            root_dir,\n",
    "#                            comments,\n",
    "#                            n_networks):\n",
    "    \n",
    "    \n",
    "#     #####################################################\n",
    "#     ############## GET HUNGARYAN ASSIGNMENT #############\n",
    "#     #####################################################\n",
    "#     fname_out = os.path.split(fname_classification)[0]+\"/animal_arrays.npy\"\n",
    "#     if os.path.exists(fname_out)==False:\n",
    "\n",
    "#         data = np.load(fname_classification, allow_pickle=True)\n",
    "#         vals = data['vals']\n",
    "#         frames = np.hstack(data['frames_array'])\n",
    "#         print (\"# of lableed frames: \", vals.shape)\n",
    "#         classes = data['classes']\n",
    "\n",
    "#         reassembled = np.load(fname_traces_inferences)\n",
    "\n",
    "#         #reassembled = np.load('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_57_12_418305_compressed/pickle/2020-3-16_12_57_12_418305_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full_traces_inferences.npz')\n",
    "#         tracesx_re = reassembled[:,:,0]\n",
    "#         tracesy_re = reassembled[:,:,1]\n",
    "#         print (\"Lenght of tracessx: \", tracesx_re.shape)\n",
    "\n",
    "#         start = 1\n",
    "#         end = tracesx_re.shape[0]\n",
    "\n",
    "#         # INITIALIZE ARRAYS TO BE SAVED\n",
    "#         animal_ids_hungarian = np.zeros((tracesx_re.shape[0], n_networks), 'float32')\n",
    "#         animal_arrays = np.zeros((n_networks, tracesx_re.shape[0], 2), 'float32')\n",
    "#         for n in trange(start,end, 1):\n",
    "#             #ret, frame = original_vid.read()\n",
    "\n",
    "#             # animal_ids is the hungarian output;\n",
    "#             # animal_ids is the CNN output\n",
    "#             animal_ids, animal_ids2, dirs = get_animal_ids(n,\n",
    "#                                                         chain_ids,\n",
    "#                                                         root_dir,\n",
    "#                                                         vals,\n",
    "#                                                         classes,\n",
    "#                                                         n_networks,\n",
    "#                                                         comments)\n",
    "\n",
    "#             #print (\"animal_ids: \", animal_ids)\n",
    "#             for k in range(0, 14*n_networks,14):\n",
    "#                 y = tracesx_re[n,k:k+14]\n",
    "#                 x = tracesy_re[n,k:k+14]\n",
    "\n",
    "#                 idx = np.where(x!=0)[0]\n",
    "#                 x = x[idx]\n",
    "#                 y = y[idx]\n",
    "#                 if x.shape[0]==0:\n",
    "#                     x_ave = None\n",
    "#                     y_ave = None\n",
    "#                 else:\n",
    "#                     x_ave = np.nanmean(x)\n",
    "#                     y_ave = np.nanmean(y)\n",
    "\n",
    "#                 if animal_ids[k//14] is None:\n",
    "#                     continue\n",
    "#                 animal_arrays[animal_ids[k//14], n] = x_ave, y_ave\n",
    "\n",
    "#             animal_ids_hungarian[n]=animal_ids \n",
    "\n",
    "#         np.save(fname_out, animal_arrays) # these are the mean location of the data\n",
    "#         np.save(fname_out[:-4]+\"_animal_ids_hungarian.npy\", animal_ids_hungarian)\n",
    "    \n",
    "#     else:\n",
    "#         animal_arrays = np.load(fname_out)\n",
    "#         animal_ids_hungarian = np.load(fname_out[:-4]+\"_animal_ids_hungarian.npy\")\n",
    "\n",
    "\n",
    "#     #####################################################\n",
    "#     ####### USE HUNGARIAN OUTPUT TO REMAKE TRACES #######\n",
    "#     ##################################################### \n",
    "#     traces_inferences_classification_hungarian = np.zeros((n_frames, \n",
    "#                                                            n_animals, \n",
    "#                                                            n_features, \n",
    "#                                                            2), 'float32')+np.nan\n",
    "#     #\n",
    "#     print (\"Combining hungarian and sleap output\")\n",
    "    \n",
    "#     # combine hungarian output and slp output\n",
    "#     traces_inferences = np.load(fname_traces_inferences)\n",
    "#     tracesx_re = traces_inferences[:,:,0]\n",
    "#     tracesy_re = traces_inferences[:,:,1]\n",
    "#     for frame in trange(1,n_frames,1):\n",
    "#         animals_selected = animal_ids_hungarian[frame]\n",
    "#         #print (\"animals_selected: \", animals_selected)\n",
    "\n",
    "#         for k in range(n_networks):\n",
    "#             if np.isnan(animals_selected[k]):\n",
    "#                 continue\n",
    "            \n",
    "#             animal_selected = int(animals_selected[k])\n",
    "                \n",
    "#             featuresx = tracesx_re[frame,k*n_features:(k+1)*n_features]\n",
    "#             featuresy = tracesy_re[frame,k*n_features:(k+1)*n_features]\n",
    "\n",
    "#             # \n",
    "#             traces_inferences_classification_hungarian[frame,\n",
    "#                           animal_selected,:,0]=featuresx\n",
    "\n",
    "#             traces_inferences_classification_hungarian[frame,\n",
    "#                           animal_selected,:,1]=featuresy\n",
    "\n",
    "#     np.save(fname_traces_inferences[:-4]+\"_hungarian.npy\", traces_inferences_classification_hungarian)\n",
    "            \n",
    "#     print (\"DONE making animal_arrays: \", animal_arrays.shape)\n",
    "    \n",
    "#     return animal_arrays, animal_ids_hungarian, traces_inferences_classification_hungarian\n",
    "\n",
    "\n",
    "\n",
    "# comments=False\n",
    "# n_animals = 4\n",
    "# n_features = 14\n",
    "\n",
    "# (animal_ids, animal_ids_hungarian, traces_inferences_classification_hungarian) = get_animal_ids_function2(\n",
    "#                                                                                            fname_classification,\n",
    "#                                                                                            fname_traces_inferences,\n",
    "#                                                                                            chain_ids,\n",
    "#                                                                                            root_dir,\n",
    "#                                                                                            comments,\n",
    "#                                                                                            n_animals,\n",
    "#                                                                                            n_features)\n",
    "\n",
    "# print (animal_ids.shape, animal_ids[:,10],\n",
    "#       animal_ids_hungarian.shape)\n",
    "# print (\"DONE computing hungarian ID assignments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:184: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:184: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-205-f6033abfe2e2>:184: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if track_id is 'all':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleap output:  (307652, 30) [  0.        0.      310.35138 572.4292        nan       nan 328.01074\n",
      " 592.1255        nan       nan 332.36166 620.2585  322.48322 642.2069\n",
      " 344.07208 678.09973 352.18756 696.1015  374.36697 704.43665 432.44778\n",
      " 724.13495 458.30075 742.2128        nan       nan       nan       nan\n",
      "       nan       nan]\n",
      "(307652,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-205-f6033abfe2e2>:128: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax2 = plt.subplot(gs[1, :])\n",
      "<ipython-input-205-f6033abfe2e2>:128: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax2 = plt.subplot(gs[1, :])\n",
      "<ipython-input-205-f6033abfe2e2>:128: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax2 = plt.subplot(gs[1, :])\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "########## UPDATED UNGARIAN ALGORITHM ###########\n",
    "#################################################\n",
    "\n",
    "def make_movie(fname_video,\n",
    "               traces\n",
    "              ):\n",
    "    \n",
    "    #colors_4= ['orange','green', 'blue', 'red', 'cyan']\n",
    "    colors_4= ['blue','red', 'cyan', 'green', 'yellow']\n",
    "\n",
    "    video_name = fname_video#'/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi'\n",
    "    #video_name = root_dir + '/' + os.path.split(root_dir)[1]+'.avi'\n",
    "    \n",
    "    print (\"video name: \", video_name)\n",
    "    original_vid = cv2.VideoCapture(video_name)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "    # video sizes\n",
    "    size_vid = np.array([1280,1024])\n",
    "    scale = 1\n",
    "    dot_size = 8//scale\n",
    "\n",
    "    n_features =14\n",
    "\n",
    "    #out_dir = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/2020-3-9_08_18_49_128168/'\n",
    "\n",
    "    start = 8*60*25\n",
    "    end = start+1000\n",
    "    original_vid.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "\n",
    "    # Initialize video out\n",
    "    fname_out = video_name[:-4]+\"_locs_added_\"+str(start)+\"_\"+str(end)+\".mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc('M','P','E','G')\n",
    "    video_out = cv2.VideoWriter(fname_out,fourcc, 25, (size_vid[0]//scale,size_vid[1]//scale), True)\n",
    "\n",
    "    for n in trange(start,end, 1):\n",
    "        ret, frame = original_vid.read()\n",
    "        cv2.putText(frame, str(n), (50, 100), font, 5, (255, 255, 0), 5)\n",
    "        frame = frame[::scale, ::scale]\n",
    "\n",
    "        # loop over the info\n",
    "\n",
    "\n",
    "        #for k in range(locs.shape[0]):\n",
    "        for k in range(n_features*n_networks):\n",
    "\n",
    "            #traces_inferences[frame][k*14:(k+1)*14])\n",
    "            y = traces_inferences[n,k,0]\n",
    "            x = traces_inferences[n,k,1]\n",
    "\n",
    "            #y = locs[k,n,0]\n",
    "            #x = locs[k,n,1]\n",
    "\n",
    "            if np.isnan(x) or np.isnan(y) or np.isnan(animal_ids_hungarian[n][k//14]):\n",
    "                continue\n",
    "            else:\n",
    "                x=int(x)//scale\n",
    "                y=int(y)//scale\n",
    "\n",
    "                # select color index\n",
    "                p = int(animal_ids_hungarian[n][k//14])\n",
    "\n",
    "                frame[x-dot_size:x+dot_size,y-dot_size:y+dot_size]= (np.float32(\n",
    "                    matplotlib.colors.to_rgb(colors_4[p]))*255.).astype('uint8')\n",
    "                #print (colors_4[k])\n",
    "                #frame[y-dot_size:y+dot_size,x-dot_size:x+dot_size]= (np.float32(\n",
    "                #    matplotlib.colors.to_rgb(colors_4[z//14]))*255.).astype('uint8')\n",
    "\n",
    "        #print (\"\")\n",
    "        video_out.write(frame)\n",
    "\n",
    "        #print (\"\")\n",
    "\n",
    "    video_out.release()\n",
    "    original_vid.release()\n",
    "    #cv2.destroyAllWindows()\n",
    "\n",
    "        \n",
    "def get_classes(track_ids,\n",
    "                 track_id, \n",
    "                 classes,\n",
    "                 confidence,\n",
    "                 frames_array,\n",
    "                 plotting):\n",
    "    \n",
    "    ''' Visualize the distribution of confidence in a track\n",
    "    \n",
    "        Input:\n",
    "        - track_ids\n",
    "        - track_id  <- selected track id\n",
    "        - classes\n",
    "        - confidence\n",
    "    \n",
    "    '''\n",
    "    clrs = ['red','blue','cyan','green']\n",
    "    n_animals = 4\n",
    "    fps= 25\n",
    "    \n",
    "    idx = np.where(track_ids==track_id)[0]\n",
    "\n",
    "    classes_pred = classes[idx]\n",
    "    confidence_pred = confidence[idx,classes_pred]\n",
    "    frames_pred = frames_array[idx]\n",
    "    length_frames = int(frames_pred[-1]-frames_pred[0])+1\n",
    "        \n",
    "    if plotting:\n",
    "        gs = gridspec.GridSpec(2,4)\n",
    "        plt.suptitle(\"Distribution of confidence across all IDs for track \"+str(track_id)+\n",
    "                    \" with # frames: \" +str(idx.shape[0])+\n",
    "                    \" (\"+str(frames_pred[0])+ \" to \"+str(frames_pred[-1])+\")\")\n",
    "    \n",
    "    #times = np.zeros((idx.shape[0],n_animals),'int32')\n",
    "    times = np.zeros((length_frames,n_animals),'int32')\n",
    "    \n",
    "    for p in range(n_animals):\n",
    "\n",
    "        idx1 = np.where(classes_pred==p)[0]\n",
    "        times[idx1,p]+=1\n",
    "        temp = confidence_pred[idx1]\n",
    "        y = np.histogram(temp, np.arange(0,1.1,0.1))\n",
    "\n",
    "        if plotting:\n",
    "            ax = plt.subplot(gs[p//4, p%4])\n",
    "            ax.set_title(\"ID: \"+str(p))\n",
    "            ax.plot(y[1][:-1],y[0], c=clrs[p])\n",
    "\n",
    "            ax2 = plt.subplot(gs[1, :])\n",
    "            ax2.scatter(frames_pred[idx1]/fps, idx1*0+0.01*p, c=clrs[p])\n",
    "\n",
    "    \n",
    "    # show rolling curves for cumulative ids;\n",
    "    window = fps\n",
    "    t=np.arange(frames_pred[0], frames_pred[-1]+1,1)/window\n",
    "    for p in range(n_animals):\n",
    "        temp = np.convolve(times[:,p], np.ones(window)/window, mode='same')\n",
    "        \n",
    "        if plotting:\n",
    "            ax2.plot(t, temp, clrs[p])\n",
    "            ax2.set_title(\"Predicted animal vs. time w. rolling window average (1sec)\",fontsize=20)\n",
    "            ax2.set_xlabel(\"Real Time (sec)\", fontsize=20)\n",
    "            ax2.set_xlim(t[0],t[-1])\n",
    "            \n",
    "    if plotting:\n",
    "        plt.show()\n",
    "\n",
    "    return classes_pred, confidence_pred         \n",
    "        \n",
    "        \n",
    "def post_CNN_hungarian(root_dir_cnn,\n",
    "                       fname_sleap_output,\n",
    "                       fname_classification,\n",
    "                       fname_chain_ids,\n",
    "                       fname_traces_inferences,\n",
    "                       n_animals,\n",
    "                       n_features,\n",
    "                       track_id):\n",
    "    \n",
    "    '''  Function which implements a time-based hungarian constraint on the CNN outputs\n",
    "    \n",
    "        Input: classification data\n",
    "        - classes [n_images]\n",
    "        - confidence [n_images, n_animals]\n",
    "        \n",
    "        Output: hungarian corrected classification data\n",
    "        - classes [n_images]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # load sleap output\n",
    "    sleap = np.load(fname_sleap_output)\n",
    "    print (\"sleap output: \", sleap.shape, sleap[0])\n",
    "    \n",
    "    # load CNN classification output; \n",
    "    data = np.load(fname_classification)\n",
    "    classes = data['classes']              #predicted class for each image\n",
    "    confidence = data['confidence']\n",
    "    frames_array = data['frames_array']\n",
    "    track_ids = data['vals'].astype('int32')    # DO NOT change order, \n",
    "    print (track_ids.shape)\n",
    "\n",
    "    # loop over all tracks and get a sense of purity\n",
    "    \n",
    "    if track_id is 'all':\n",
    "        ids = np.arange(track_ids.shape[0])\n",
    "    else:\n",
    "        ids = [track_id]\n",
    "    for k in ids:\n",
    "        plotting = True\n",
    "        classes_pred, confidence_pred = get_classes(track_ids,\n",
    "                                                    k,\n",
    "                                                    classes,\n",
    "                                                    confidence,\n",
    "                                                    frames_array,\n",
    "                                                    plotting)\n",
    "        \n",
    "   \n",
    "#\n",
    "n_animals = 4\n",
    "n_features = 14\n",
    "\n",
    "# \n",
    "root_dir_cnn = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/animal_images/'\n",
    "\n",
    "# \n",
    "fname_sleap =             '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis.npy'\n",
    "fname_sleap_traces =      '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis_traces_reassembled.npy'\n",
    "fname_classification =    '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/classification_output.npz'\n",
    "\n",
    "# need to make this file which contains the sleap tracks -> traces shape -> with IDs corrected by CNN \n",
    "# this is required for making movies/analysis downstream\n",
    "fname_chain_ids =         '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis_chain_ids.npy'\n",
    "  \n",
    "\n",
    "track_id = 120\n",
    "#\n",
    "post_CNN_hungarian(root_dir_cnn,\n",
    "                   fname_sleap_output,\n",
    "                   fname_classification,\n",
    "                   fname_chain_ids,\n",
    "                   fname_traces_inferences,\n",
    "                   n_animals,\n",
    "                   n_features,\n",
    "                   track_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 50/2187 [00:00<00:04, 491.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleap traces:  (89987, 4, 14, 2)\n",
      "sleap sleap_traces_classification:  (89987, 4, 14, 2)\n",
      "chain ids:  (89987, 4)\n",
      "# of images classified by sleap:  (307652,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2187/2187 [00:04<00:00, 464.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "###### COMBINE SLEAP WITH CNN INTO TRACES FILE ########\n",
    "#######################################################\n",
    "\n",
    "# \n",
    "def make_sleap_traces_classification(fname_chain_ids,\n",
    "                                     fname_sleap_traces,\n",
    "                                     fname_classification):\n",
    "        \n",
    "    '''  Function to make traces file which contains sleap output with CNN corrected ID in a simple format\n",
    "    \n",
    "        Input:\n",
    "        - sleap_traces file converted to .npy format : [n_images, track_id+frame_id+features*2]; \n",
    "        - chain_ids [n_frames, n_animals] \n",
    "        - classification data: classes [n-images], confidence [n_images, n_animals], track_ids [n_tracks]\n",
    "        \n",
    "        Output:\n",
    "        - sleap_traces_classification file : [n_frames, n_animals, n_features, 2]\n",
    "    \n",
    "    '''\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    n_animals = 4\n",
    "    n_features = 14\n",
    "    \n",
    "    # load sleap traces\n",
    "    sleap_traces = np.load(fname_sleap_traces)\n",
    "    sleap_traces = sleap_traces.reshape(sleap_traces.shape[0],\n",
    "                                       n_animals,\n",
    "                                       n_features,\n",
    "                                       2)\n",
    "    # \n",
    "    print (\"sleap traces: \", sleap_traces.shape)\n",
    "    \n",
    "    # make final traces \n",
    "    sleap_traces_classification = np.zeros(sleap_traces.shape, 'float32')+np.nan\n",
    "    print (\"sleap sleap_traces_classification: \", sleap_traces_classification.shape)\n",
    "\n",
    "    # load chain ids which show conversion between sleap output and traces\n",
    "    chain_ids = np.load(fname_chain_ids)\n",
    "    print (\"chain ids: \", chain_ids.shape)\n",
    "    \n",
    "    # \n",
    "    chain_ids_corrected = np.zeros(chain_ids.shape, 'float32')+np.nan\n",
    "    \n",
    "    # load CNN classification output\n",
    "    data = np.load(fname_classification)\n",
    "    classes = data['classes']              #predicted class for each image\n",
    "    confidence = data['confidence']\n",
    "    frames_array = data['frames_array']\n",
    "    track_ids = data['vals'].astype('int32')    # DO NOT change order, \n",
    "    print (\"# of images classified by sleap: \", track_ids.shape)\n",
    "    \n",
    "    # \n",
    "    idx_tracks = np.int32(np.unique(track_ids))\n",
    "    for id_ in tqdm(idx_tracks):\n",
    "    #for id_ in [0,1732]:\n",
    "        \n",
    "        #print (\" track id: \", id_)\n",
    "        # get index location to the x,y data in the \n",
    "        frames_, animals_original_ = np.where(chain_ids==id_)\n",
    "        #print (\"frames_ \", frames_ )\n",
    "        #print (\"original animals \", animals_original_ )\n",
    "        \n",
    "        # find location in sleap file\n",
    "        idx2 = np.where(track_ids==id_)[0]\n",
    "        #print (\"track \", id_, \" locations: \", idx2)\n",
    "        \n",
    "        # find class ID in sleap file\n",
    "        animals_predicted_ = classes[idx2]\n",
    "        #print (\"predicted animals_: \", animals_predicted_)\n",
    "        \n",
    "        # find frames THIS IS REDUNDANT\n",
    "        # frames_ = np.int32(frames_array[idx2])\n",
    "        # print (\"frames: \", frames_)\n",
    "        \n",
    "        # assign data into the\n",
    "        for f in range(frames_.shape[0]):\n",
    "            #for a in range(animals_predicted_.shape[0]):\n",
    "            sleap_traces_classification[frames_[f], \n",
    "                        animals_predicted_[f]]= sleap_traces[frames_[f],\n",
    "                                                        animals_original_[f]]\n",
    "        \n",
    "            chain_ids_corrected[frames_[f],animals_predicted_[f]] = id_\n",
    "    \n",
    "    # save corrected traces\n",
    "    fname_sleap_traces_classification = fname_sleap_traces[:-4]+\"_classification.npy\"\n",
    "    np.save(fname_sleap_traces_classification, sleap_traces_classification)\n",
    "\n",
    "    # save corrected chain_ids file\n",
    "    fname_chain_ids_corrected = fname_chain_ids[:-4]+\"_classification.npy\"\n",
    "    np.save(fname_chain_ids_corrected, chain_ids_corrected)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return sleap_traces_classification\n",
    "\n",
    "\n",
    "# \n",
    "# fname_sleap =             '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis.npy'\n",
    "fname_chain_ids =         '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis_chain_ids.npy'\n",
    "fname_sleap_traces =      '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis_traces_reassembled.npy'\n",
    "fname_classification =    '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/classification_output.npz'\n",
    "\n",
    "# need to make this file which contains the sleap tracks -> traces shape -> with IDs corrected by CNN \n",
    "# this is required for making movies/analysis downstream\n",
    "\n",
    "# \n",
    "sleap_traces_classification = make_sleap_traces_classification(fname_chain_ids,\n",
    "                                                             fname_sleap_traces,\n",
    "                                                             fname_classification)\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:06<00:00, 42.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# show videos\n",
    "    \n",
    "# use the original post-classification\n",
    "fname_traces = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis_traces_reassembled_classification.npy'\n",
    "fname_chain_ids = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis_chain_ids_classification.npy'\n",
    "video_name = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi'\n",
    "\n",
    "start = 2598\n",
    "end = 2874\n",
    "\n",
    "#\n",
    "make_video(video_name,\n",
    "           fname_chain_ids,\n",
    "           fname_traces,\n",
    "           start,\n",
    "           end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307652, 30)\n",
      "[[0.000e+00 1.000e+00       nan       nan]\n",
      " [0.000e+00 1.000e+00       nan       nan]\n",
      " [0.000e+00 1.000e+00       nan       nan]\n",
      " ...\n",
      " [2.181e+03 2.184e+03 2.187e+03 2.188e+03]\n",
      " [2.181e+03 2.184e+03 2.187e+03 2.188e+03]\n",
      " [2.181e+03 2.184e+03 2.187e+03 2.188e+03]] (89987, 4)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis.npy')\n",
    "print (data.shape)\n",
    "\n",
    "chain_ids = np.load('/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis_chain_ids.npy')\n",
    "\n",
    "print (chain_ids, chain_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3]), array([ 272, 1804, 1021,  265]))\n",
      "[[2.2059169e-06 5.6868776e-05 4.8914370e-01 5.1079720e-01]\n",
      " [8.6293346e-04 9.9825829e-01 6.8832225e-07 8.7806600e-04]\n",
      " [7.1768379e-03 9.8313981e-01 9.0713743e-03 6.1205594e-04]\n",
      " ...\n",
      " [4.4628687e-05 9.9926239e-01 3.3892933e-04 3.5404877e-04]\n",
      " [8.1040021e-03 7.4503267e-01 2.4213201e-01 4.7313678e-03]\n",
      " [7.5283046e-06 9.9440986e-01 3.4747529e-04 5.2351169e-03]]\n",
      "[ 9887.  9888.  9889. ... 13265. 13268. 13269.]\n"
     ]
    }
   ],
   "source": [
    "# CHECK TRACK IDS\n",
    "track_id = 285\n",
    "\n",
    "fname = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/animal_images/'+str(track_id)+'/predictions.npz'\n",
    "data = np.load(fname)\n",
    "predictions = data['predictions']\n",
    "print (np.unique(predictions, return_counts=True))\n",
    "confidence = data['confidence']\n",
    "frame_ids = data['frame_ids']\n",
    "print (confidence)\n",
    "print (frame_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89987, 4, 14, 2)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 1.000e+00       nan       nan]\n",
      " [0.000e+00 1.000e+00       nan       nan]\n",
      " [0.000e+00 1.000e+00       nan       nan]\n",
      " ...\n",
      " [2.181e+03 2.184e+03 2.187e+03 2.188e+03]\n",
      " [2.181e+03 2.184e+03 2.187e+03 2.188e+03]\n",
      " [2.181e+03 2.184e+03 2.187e+03 2.188e+03]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames_array:  (307652,)\n",
      "classes:  (307652,) [0 0 0 0 2 0 0 0 0 2 2 0 0 0 2 0 3 3 0 3 0 0 2 2 0 0 0 2 0 0 3 3 0 2 2 2 1\n",
      " 0 2 0 0 0 0 0 0 3 0 0 2 2 0 0 0 0 3 0 1 0 0 0 0 1 1 0 0 0 2 0 0 2 0 0 0 0\n",
      " 0 2 0 0 0 0 0 0 0 0 2 0 3 0 0 2 3 0 0 0 0 0 0 2 0 1]\n",
      "vals  [831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831.]\n",
      "chain ids:  (89987, 4) [ 0.  1. nan nan]\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "######## MERGE CLASSIFICAITON AND TRACES_INFERENCES OUTPUT ###########\n",
    "######################################################################\n",
    "def get_animal_ids_confidence(frame,\n",
    "                              chain_ids,\n",
    "                              root_dir,\n",
    "                              vals,\n",
    "                              classes,\n",
    "                              n_animals,\n",
    "                              comments=False):\n",
    "\n",
    "    # \n",
    "    dirs = chain_ids[frame]\n",
    "\n",
    "    if comments:\n",
    "        print (\"DIRS: \", dirs)\n",
    "    \n",
    "    animal_ids = []\n",
    "    animal_ids2 = []\n",
    "    threshold_purity1 = 0.5\n",
    "    threshold_purity2 = 0.7\n",
    "\n",
    "    # HUNGARIAN ALGORITHM BASED SEARCH FOR MOST CORRECT IDS\n",
    "    best_ids_cts = np.zeros((5,5),'float32')\n",
    "    animals3 = []\n",
    "    for ctr_dir, dir_ in enumerate(dirs):\n",
    "        #print (\"2nd Loop: \", dir_)\n",
    "        if np.isnan(dir_)==False:\n",
    "            \n",
    "            # find the classifier values for that segment\n",
    "            idx = np.where(vals==dir_)[0]\n",
    "\n",
    "            # find predicted id;\n",
    "            # First find \n",
    "            #fnames = np.sort(glob.glob(root_dir+str(int(dir_))+'/*.npy',recursive = False))\n",
    "            fnames = np.sort(glob.glob(root_dir+str(int(dir_))+'/*.npz',recursive = False))\n",
    "\n",
    "            ctr_chain = 0\n",
    "            for fname in fnames:\n",
    "                if ('frame_'+str(frame).zfill(7)) in fname:\n",
    "                     break \n",
    "                ctr_chain+=1\n",
    "        \n",
    "            animal_id3 = classes[idx][ctr_chain]\n",
    "            animals3.append(animal_id3)\n",
    "            \n",
    "            idx5 = np.where(classes[idx]<1E10)[0]\n",
    "            classes_local = classes[idx[idx5]]\n",
    "            \n",
    "            classes_unique = np.unique(classes_local, return_counts=True)\n",
    "            #print (\"best id: \", best_id)\n",
    "            \n",
    "            best_ids_cts[ctr_dir, classes_unique[0]]=classes_unique[1]/idx.shape[0]\n",
    "        else:\n",
    "            animals3.append(None)\n",
    "    \n",
    "    # FIRST LOOP TO CHECK ANY GREAT MATCHES AND ZERO OUT EVERYTHING ELSE\n",
    "    for k1 in range(n_animals):\n",
    "        temp = best_ids_cts[k1]\n",
    "        max_val = np.max(temp)\n",
    "        if max_val>threshold_purity2:\n",
    "            idx8 = np.argmax(temp)\n",
    "            best_ids_cts[:,idx8]=0\n",
    "            best_ids_cts[k1,idx8]=max_val\n",
    "            \n",
    "    if comments:\n",
    "        print (best_ids_cts)\n",
    "    \n",
    "    # RENORMALIZE PROBABILITIES AFTER ZEROING OUT\n",
    "    for k1 in range(n_animals):\n",
    "        sum_ = best_ids_cts[k1].sum()\n",
    "        if sum_>0:\n",
    "            best_ids_cts[k1]*= 1./sum_\n",
    "    if comments:\n",
    "        print (best_ids_cts)    \n",
    "        \n",
    "    # FINAL LOOP TO GRAB WHATEVER IS LEFT\n",
    "    for k1 in range(n_animals):\n",
    "        max_val = np.max(best_ids_cts[k1])\n",
    "        if max_val==0: # or max_val<threshold_purity1:\n",
    "            animal_ids.append(None)\n",
    "            animal_ids2.append(animals3[k1])\n",
    "            continue\n",
    "        else:\n",
    "            idx9 = np.argmax(best_ids_cts[k1])\n",
    "            #print (\"idx9: \", idx9, classes_unique[0])\n",
    "            animal_ids.append(idx9)\n",
    "            animal_ids2.append(animals3[k1])\n",
    "            \n",
    "            best_ids_cts[:,idx9]= 0\n",
    "            # must zero out all the others\n",
    "            \n",
    "    if comments:\n",
    "        print (best_ids_cts)   \n",
    "        \n",
    "    return animal_ids, animal_ids2, dirs\n",
    "\n",
    "\n",
    "traces_inferences = np.load(fname_traces_inferences)\n",
    "classification = np.load(fname_classification)\n",
    "classes = classification['classes']\n",
    "confidence = classification['confidence']\n",
    "frames_array = classification['frames_array']\n",
    "print ('frames_array: ', frames_array.shape)\n",
    "print (\"classes: \", classes.shape, classes[:100])\n",
    "vals = classification['vals']\n",
    "print (\"vals \", vals[:100])\n",
    "\n",
    "#\n",
    "n_animals=4\n",
    "\n",
    "#\n",
    "frame = 0\n",
    "\n",
    "#\n",
    "print (\"chain ids: \", chain_ids.shape, \n",
    "       chain_ids[frame])\n",
    "\n",
    "#\n",
    "animal_ids, animal_ids2, dirs = get_animal_ids_confidence(frame,\n",
    "                                                          chain_ids,\n",
    "                                                          root_dir,\n",
    "                                                          vals,\n",
    "                                                          classes,\n",
    "                                                          n_animals,\n",
    "                                                          comments=False)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames_array:  (307652,)\n",
      "classes:  (307652,) [0 0 0 0 2 0 0 0 0 2 2 0 0 0 2 0 3 3 0 3 0 0 2 2 0 0 0 2 0 0 3 3 0 2 2 2 1\n",
      " 0 2 0 0 0 0 0 0 3 0 0 2 2 0 0 0 0 3 0 1 0 0 0 0 1 1 0 0 0 2 0 0 2 0 0 0 0\n",
      " 0 2 0 0 0 0 0 0 0 0 2 0 3 0 0 2 3 0 0 0 0 0 0 2 0 1]\n",
      "vals  [831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831. 831.\n",
      " 831. 831.]\n",
      "chain ids:  (89987, 4) [ 0.  1. nan nan]\n",
      "traces:  (89987, 56, 2)\n",
      "classification animal ids:  (4, 89987, 2) [669.44037 373.67206]\n",
      "classification animal_ids_hungarian:  (89987, 4) [ 0.  1. nan nan]\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "###################################\n",
    "###################################\n",
    "traces_inferences = np.load(fname_traces_inferences)\n",
    "classification = np.load(fname_classification)\n",
    "classes = classification['classes']\n",
    "confidence = classification['confidence']\n",
    "frames_array = classification['frames_array']\n",
    "print ('frames_array: ', frames_array.shape)\n",
    "print (\"classes: \", classes.shape, classes[:100])\n",
    "vals = classification['vals']\n",
    "print (\"vals \", vals[:100])\n",
    "frame = 2555\n",
    "\n",
    "#\n",
    "# print (\"classes: \", classes.shape, classes[frame])\n",
    "# print ('vals: ', vals.shape, vals[frame])\n",
    "# for k in range(4):\n",
    "#     print (traces_inferences[frame, k*14:(k+1)*14][:3])\n",
    "#     print ('')\n",
    "\n",
    "print ('chain ids: ', chain_ids.shape, chain_ids[1])\n",
    "print (\"traces: \", traces_inferences.shape)\n",
    "print (\"classification animal ids: \", animal_ids.shape, animal_ids[0,1])\n",
    "print (\"classification animal_ids_hungarian: \", animal_ids_hungarian.shape, animal_ids_hungarian[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1006.34607  618.2427 ]\n",
      " [1004.34564  658.5099 ]\n",
      " [1040.4918   644.3095 ]\n",
      " [1006.505    692.188  ]\n",
      " [1056.4115   674.3973 ]\n",
      " [1034.6678   694.457  ]\n",
      " [1024.6968   724.305  ]\n",
      " [1008.36365  744.2123 ]\n",
      " [ 984.416    750.198  ]\n",
      " [ 958.69275  746.26917]\n",
      " [ 934.603    748.47144]\n",
      " [ 906.5242   754.2024 ]\n",
      " [ 878.5866   748.6434 ]\n",
      " [       nan        nan]]\n",
      "[988.05005 707.56964]\n",
      "animal ids hungarian:  [1. 0. 2. 3.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frame = 22256\n",
    "for k in range(1):\n",
    "    print (traces_inferences[frame][k*14:(k+1)*14])\n",
    "    #print (animal_ids[k,frame][::-1])\n",
    "    #print (animal_ids[int(animal_ids_hungarian[frame][k]),frame][::-1])\n",
    "    print (\"animal ids hungarian: \", animal_ids_hungarian[frame])\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video name:  /media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:24<00:00, 40.83it/s]\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "############ MAKE VIDEO BASES ON TRACES ####\n",
    "############################################\n",
    "#          pup1     pup2    female  male\n",
    "# go through first videos\n",
    "from tqdm import trange\n",
    "\n",
    "colors_4= ['orange','green', 'blue', 'red', 'cyan']\n",
    "colors_4= ['blue','red', 'cyan', 'green', 'yellow']\n",
    "\n",
    "video_name = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi'\n",
    "#video_name = root_dir + '/' + os.path.split(root_dir)[1]+'.avi'\n",
    "print (\"video name: \", video_name)\n",
    "original_vid = cv2.VideoCapture(video_name)\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "# video sizes\n",
    "size_vid = np.array([1280,1024])\n",
    "scale = 1\n",
    "dot_size = 8//scale\n",
    "\n",
    "n_features =14\n",
    "\n",
    "#out_dir = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/2020-3-9_08_18_49_128168/'\n",
    "\n",
    "start = 8*60*25\n",
    "end = start+1000\n",
    "original_vid.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "\n",
    "# Initialize video out\n",
    "fname_out = video_name[:-4]+\"_locs_added_\"+str(start)+\"_\"+str(end)+\".mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc('M','P','E','G')\n",
    "video_out = cv2.VideoWriter(fname_out,fourcc, 25, (size_vid[0]//scale,size_vid[1]//scale), True)\n",
    "\n",
    "for n in trange(start,end, 1):\n",
    "    ret, frame = original_vid.read()\n",
    "    cv2.putText(frame, str(n), (50, 100), font, 5, (255, 255, 0), 5)\n",
    "    frame = frame[::scale, ::scale]\n",
    "    \n",
    "    # loop over the info\n",
    "    \n",
    "    \n",
    "    #for k in range(locs.shape[0]):\n",
    "    for k in range(n_features*n_networks):\n",
    "    \n",
    "        #traces_inferences[frame][k*14:(k+1)*14])\n",
    "        y = traces_inferences[n,k,0]\n",
    "        x = traces_inferences[n,k,1]\n",
    "        \n",
    "        #y = locs[k,n,0]\n",
    "        #x = locs[k,n,1]\n",
    "        \n",
    "        if np.isnan(x) or np.isnan(y) or np.isnan(animal_ids_hungarian[n][k//14]):\n",
    "            continue\n",
    "        else:\n",
    "            x=int(x)//scale\n",
    "            y=int(y)//scale\n",
    "            \n",
    "            # select color index\n",
    "            p = int(animal_ids_hungarian[n][k//14])\n",
    "\n",
    "            frame[x-dot_size:x+dot_size,y-dot_size:y+dot_size]= (np.float32(\n",
    "                matplotlib.colors.to_rgb(colors_4[p]))*255.).astype('uint8')\n",
    "            #print (colors_4[k])\n",
    "            #frame[y-dot_size:y+dot_size,x-dot_size:x+dot_size]= (np.float32(\n",
    "            #    matplotlib.colors.to_rgb(colors_4[z//14]))*255.).astype('uint8')\n",
    "                \n",
    "    #print (\"\")\n",
    "    video_out.write(frame)\n",
    "\n",
    "    #print (\"\")\n",
    "\n",
    "video_out.release()\n",
    "original_vid.release()\n",
    "#cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89987, 4)\n",
      "[[ 0.  1. nan nan]\n",
      " [ 0.  1. nan nan]\n",
      " [ 0.  1. nan nan]\n",
      " [ 0.  1. nan nan]\n",
      " [ 0.  1.  2. nan]\n",
      " [ 0.  1. nan nan]\n",
      " [ 0.  1. nan nan]\n",
      " [ 0.  1.  2. nan]\n",
      " [ 0.  1. nan nan]\n",
      " [ 0.  1.  2. nan]]\n"
     ]
    }
   ],
   "source": [
    "# data = np.load('/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/2020_3_15_11_53_51_617746_compressed.avi.predictions.analysis_chain_ids.npy')\n",
    "# print (data.shape)\n",
    "# print (data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE with single feature post huganrian (4, 89987, 2)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "####### COMPUTE SINGLE FEATURE TRACES POST HUNGARIAN #########\n",
    "##############################################################\n",
    "features = np.array(6, 'int32') # spine_1\n",
    "#ids = np.arange(5)\n",
    "#ids=[0]\n",
    "\n",
    "locs_array = get_single_feature_traces(fname_classification,\n",
    "                                      fname_traces_inferences,\n",
    "                                      animal_ids_hungarian,\n",
    "                                      n_networks,\n",
    "                                      features)\n",
    "\n",
    "print (\"DONE with single feature post huganrian\", locs_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE Post-processing traces - GO TO Notebook 15_behaviour_analysis\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "###### CLEANUP: Del short segs; connect safe segs ###############\n",
    "#################################################################\n",
    "clrs = ['blue','red','yellow','green','yellow']\n",
    "\n",
    "animal_arrays = np.load(os.path.split(fname_classification)[0]+'/animal_arrays.npy')\n",
    "animal_arrays = locs_array.copy()\n",
    "\n",
    "animal_ids = np.arange(n_networks)\n",
    "locs_array_clean = cleanup(fname_classification, \n",
    "                            animal_arrays,\n",
    "                            animal_ids,\n",
    "                            locs_array)\n",
    "            \n",
    "np.save(os.path.split(fname_classification)[0]+'/locs_array_clean.npy', \n",
    "                       locs_array_clean)\n",
    "\n",
    "print (\"DONE Post-processing traces - GO TO Notebook 15_behaviour_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307652,)\n",
      "[0.7935309  0.9942002  0.86842144 ... 0.80841905 0.72285736 0.9090116 ]\n",
      "[0 0 0 ... 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "fname_classification = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_16/2020_3_15_11_53_51_617746_compressed/classification_output.npz'\n",
    "data = np.load(fname_classification)\n",
    "confidence = data['confidence']\n",
    "print (confidence.shape)\n",
    "print (confidence)\n",
    "\n",
    "classes = data['classes']\n",
    "print (classes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
