{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f4a4ff-d4f7-4fdc-a470-2a46653bd9b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import gridspec\n",
    "from scipy import signal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from tqdm import trange\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import h5py\n",
    "import csv\n",
    "import sleap\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/cat/code/gerbil/')\n",
    "\n",
    "from simba_tools.track_simba import track as Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd067979-5c41-4f3a-b7a9-9affc7758f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "###################################################\n",
    "###################################################\n",
    "# \n",
    "def save_two_animal_slp(animal_ids, fname_slp, fname_out):\n",
    "    \n",
    "    #\n",
    "    fname_out = fname_out[:-4] + \"_\"+str(animal_ids[0])+\"_\"+str(animal_ids[1])+\".slp\"\n",
    "    \n",
    "    #\n",
    "    if os.path.exists(fname_out):\n",
    "        return\n",
    "    \n",
    "    # select first to animals\n",
    "    labels = sleap.load_file(fname_slp)\n",
    "    \n",
    "    # Change this to specify which pair of tracks to save:\n",
    "    if len(labels.tracks)==0:\n",
    "        print ('sleap did not find tracks: ', fname_slp)\n",
    "        return \n",
    "    #\n",
    "    keep_tracks = [labels.tracks[animal_ids[0]], labels.tracks[animal_ids[1]]]\n",
    "\n",
    "    # Remove instances that aren't in keep_tracks\n",
    "    for lf in labels:\n",
    "        lf.instances = [inst for inst in lf.instances if inst.track in keep_tracks]\n",
    "\n",
    "    # Keep only those tracks in the labels\n",
    "    labels.tracks = keep_tracks\n",
    "\n",
    "    # Remove frames that are now empty due to the track filtering\n",
    "    labels.remove_empty_frames()\n",
    "\n",
    "    # Save the result with the specified tracks in the filename\n",
    "    labels.save(fname_out)\n",
    "\n",
    "#\n",
    "def process_simba_files_parallel(row, \n",
    "                                dir_path, \n",
    "                                animal_ids):\n",
    "\n",
    "    \n",
    "    #\n",
    "    video_scaling_factor = 1.3\n",
    "    \n",
    "    #    \n",
    "    fname_slp = os.path.join(dir_path, 'features', row['Slp filename'])    \n",
    "    fname_out = os.path.join(dir_path, 'simba', 'slp_files', row['Slp filename'])\n",
    "    \n",
    "    for animal_id in animal_ids:\n",
    "        \n",
    "        #try:\n",
    "        if True:\n",
    "            \n",
    "            no_track_files = []\n",
    "        \n",
    "            result = save_two_animal_slp(animal_id, fname_slp, fname_out)\n",
    "            \n",
    "            if result:\n",
    "                no_track_files.append(result)\n",
    "\n",
    "            #########################################################\n",
    "            ################ GENERATE CLEANED TRACKS ################\n",
    "            #########################################################\n",
    "\n",
    "            # print (\"STARTING CLEANING...\")\n",
    "\n",
    "            # \n",
    "            fname_slp_in = fname_out[:-4] + \"_\"+str(animal_id[0])+\"_\"+str(animal_id[1])+\".slp\"\n",
    "            if os.path.exists(fname_slp_in)==False:\n",
    "                print (\"Missing: \", fname_slp_in)\n",
    "                continue\n",
    "            fname_clean = os.path.join(dir_path, 'simba', 'cleaned', row['Slp filename'])\n",
    "            fname_slp_clean = fname_slp_in.replace('.slp','_cleaned.npy')\n",
    "            \n",
    "            if os.path.exists(fname_slp_clean):\n",
    "                continue\n",
    "\n",
    "            #\n",
    "            track = Track.Track(fname_slp_in)\n",
    "            track.track_type = \"features\"\n",
    "            track.exclude_huddles = True\n",
    "            track.use_dynamic_centroid = True\n",
    "\n",
    "\n",
    "            #\n",
    "            track.animal_ids = [0,1]\n",
    "            track.tracks_names = ['female','male','pup1','pup2','pup3','pup4']\n",
    "            track.recompute_spine_centres=True\n",
    "            track.verbose = True                         # gives additional printouts\n",
    "            track.n_animals = len(track.animal_ids)      # number of animals\n",
    "            track.filter_width = 10                      # this is the median filter width in frames; e.g. 10 ~=0.4 seconds\n",
    "                                                         # higher values provide more stability, but less temporally precise locations\n",
    "            # \n",
    "            track.recompute_h5 = True\n",
    "            track.load_tracks()\n",
    "\n",
    "            ####################################################\n",
    "            ### OPTIONAL - MEDIAN FILTER ALL TRACKS ############\n",
    "            ####################################################\n",
    "            if True:\n",
    "                track.filter_tracks()\n",
    "\n",
    "            ####################################################\n",
    "            ### OPTIONAL - ALGORITHM TO REASSIGN CHUNKING ######\n",
    "            ####################################################\n",
    "            if True:\n",
    "\n",
    "                # makes scores based on .slp output? (to check)\n",
    "                track.get_scores()\n",
    "\n",
    "                # uses track_spines to break up all the data into continuous chunks\n",
    "                track.max_jump_single_frame = 30  # max distance in pixels (?) that an animal can move in a single frame\n",
    "                track.make_tracks_chunks()        \n",
    "\n",
    "                # deletig very short chunks of track that are orphaned..\n",
    "                min_chunk_len = 5\n",
    "                track.del_short_chunks(min_chunk_len)\n",
    "\n",
    "                ############## FIX TRACKS PARAMS #############\n",
    "                track.time_threshold = 25       # window to search for nearest chunks, about 1sec seems fair...\n",
    "                track.safe_chunk_length = 15    # chunks this long will not change id\n",
    "                track.min_chunk_len = 4         # min length of chukn to be used for anchoring/correcting\n",
    "                track.max_distance_merge = 75   # max pix diff allowed for merging when using model; not just for neighbouring frames\n",
    "                # track.memory_length = 25      # how many frames back is it ok to remember a prev animal\n",
    "                track.verbose = False\n",
    "                track.update_tracks = True\n",
    "\n",
    "                # parameters for fixing track chunking\n",
    "                track.max_time_automerge = 3      # time to automerget chunks from same animal ???\n",
    "                track.max_dist_automerge = 25     # distance to auto merge chunks from same animal separated by single time skip\n",
    "\n",
    "\n",
    "                track.fix_tracks()\n",
    "\n",
    "            else:\n",
    "                # recompute spine centres from scratch\n",
    "                track.get_track_spine_centers()\n",
    "\n",
    "                #\n",
    "                #print (\"tracks loaded: \", track.tracks_spine.shape)\n",
    "\n",
    "            ####################################################\n",
    "            ### OPTIONAL - MEDIAN FILTER SPINE CENTRES #########\n",
    "            ####################################################\n",
    "            if False:\n",
    "                track.filter_tracks_spines() \n",
    "\n",
    "            \n",
    "            # scale the data if in cohorts3 or 4\n",
    "            if 'cohort3' in fname_slp_clean or 'cohort4' in fname_slp_clean:\n",
    "                track.tracks_spine = track.tracks_spine/video_scaling_factor\n",
    "            \n",
    "            #\n",
    "            np.save(fname_slp_clean, track.tracks_spine)\n",
    "        #\n",
    "        else:\n",
    "        #except:\n",
    "            print (\"could not compute : \", animal_id, \" for \", fname_slp,)\n",
    "            #print(f'{fname_slp_clean} could not be computed')\n",
    "        \n",
    "    return no_track_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d7265-3ec7-4560-82f3-daf9b0d511fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "import parmap\n",
    "\n",
    "#\n",
    "data_path = \"/mnt/b3a68699-495d-4ebb-9ab1-ac74f11c68c5/gerbil/cohort2/database.xlsx\"\n",
    "df = pd.read_excel(data_path)\n",
    "\n",
    "#\n",
    "dir_path = \"/mnt/b3a68699-495d-4ebb-9ab1-ac74f11c68c5/gerbil/cohort2\"\n",
    "\n",
    "#\n",
    "animal_ids = [[0,1], [1,0], [0,2], [2,0], [0,3], [3,0], [0,4], [4,0], [0,5], [5,0],\n",
    "              [1,2], [2,1], [1,3], [3,1], [1,4], [4,1], [1,5], [5,1],\n",
    "              [2,3], [3,2], [2,4], [4,2], [2,5], [5,2],\n",
    "              [3,4], [4,3], [3,5], [5,3],\n",
    "              [4,5], [5,4]]\n",
    "\n",
    "# load the rows from the dataframe spreadsheet\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    rows.append(row)\n",
    "\n",
    "  # use parallel option if working\n",
    "if True:\n",
    "    results = parmap.map(process_simba_files_parallel,\n",
    "               rows, \n",
    "               dir_path, \n",
    "               animal_ids,\n",
    "               pm_processes=16,\n",
    "               pm_pbar=True)\n",
    "    \n",
    "# use single core option to start/debug\n",
    "else:\n",
    "    for row in tqdm(rows):\n",
    "        process_simba_files_parallel(row,\n",
    "                                     dir_path,\n",
    "                                     animal_ids)\n",
    "        \n",
    "no_tracks = [item for sublist in results if sublist is not None for item in sublist]\n",
    "np.save('/home/cat/Downloads/no_tracks.npy', np.array(no_tracks))\n",
    "\n",
    "#\n",
    "print (\"DONE...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "359a08fe-131c-4785-874d-420f52d301ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.load('/home/cat/Downloads/no_tracks.npy')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95cc266-03b7-4229-b87e-300b98200b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "directory = \"/mnt/b3a68699-495d-4ebb-9ab1-ac74f11c68c5/gerbil/cohort3/simba/slp_files\"\n",
    "\n",
    "files = glob.glob(os.path.join(directory, '*_cleaned.npy'))\n",
    "\n",
    "for file in tqdm(files, desc=\"Processing files\", unit=\"file\"):\n",
    "    \n",
    "    try:\n",
    "        #\n",
    "        video = np.load(file)\n",
    "\n",
    "        # change the order of the animals\n",
    "        video_reordered = video[:, [1, 0], :]\n",
    "\n",
    "        # extract the animal numbers from the file name\n",
    "        match = re.search(r'(.*_Day_)(\\d+)_(\\d+)(_cleaned.npy)', os.path.basename(file))\n",
    "        if match:\n",
    "            prefix, animal1, animal2, suffix = match.groups()\n",
    "\n",
    "            # form the new filename\n",
    "            new_file = f'{prefix}{animal2}_{animal1}{suffix}'\n",
    "            new_file_path = os.path.join(directory, new_file)\n",
    "\n",
    "            # check if the new file already exists\n",
    "            if os.path.exists(new_file_path):\n",
    "                # print(f\"File {new_file} already exists. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # save the reordered array to a new file\n",
    "            np.save(new_file_path, video_reordered)\n",
    "        else:\n",
    "            print (\"couldn't find match: \", file)\n",
    "            \n",
    "    except:\n",
    "        print(file)\n",
    "\n",
    "print('DONE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b018a66-4128-47ea-b4b4-639492e2c0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86407, 1, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fname = '/mnt/b3a68699-495d-4ebb-9ab1-ac74f11c68c5/gerbil/cohort4/features/2020_09_23_09_06_31_558986_compressed_Day.slp'\n",
    "\n",
    "d = np.load(fname[:-4]+'.npy')\n",
    "\n",
    "print (d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43cc4d59-fdae-4de4-a04b-099872f4c21e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 2 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20661/3096256998.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     plt.plot(d[:,k,0],\n\u001b[0;32m----> 4\u001b[0;31m              d[:,k,1])\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 2 with size 1"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "for k in range(1):\n",
    "    plt.plot(d[:,k,0],\n",
    "             d[:,k,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a442a06-b5d4-4d65-ad6f-47599f71d710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2c5b0-a7a4-49d4-8b20-d21ec00b14da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88973e-97af-485e-89ad-191143bcbdd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import parmap\n",
    "\n",
    "#\n",
    "data_path = \"/mnt/b3a68699-495d-4ebb-9ab1-ac74f11c68c5/gerbil/cohort3/database.xlsx\"\n",
    "df = pd.read_excel(data_path)\n",
    "\n",
    "#\n",
    "dir_path = \"/mnt/b3a68699-495d-4ebb-9ab1-ac74f11c68c5/gerbil/cohort3\"\n",
    "\n",
    "#\n",
    "animal_ids = [[0,1], [0,2], [0,3], [0,4], [0,5],\n",
    "              [1,2], [1,3], [1,4], [1,5],\n",
    "              [2,3], [2,4], [2,5],\n",
    "              [3,4], [3,5],\n",
    "              [4,5]]\n",
    "\n",
    "#\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    rows.append(row)\n",
    "\n",
    "#\n",
    "print (rows[0])\n",
    "\n",
    "#\n",
    "print (\"len(rows): \", len(rows))\n",
    "\n",
    "#\n",
    "for row in rows:\n",
    "    print (row['Slp filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205add85-fe61-4254-b027-afa2e598c4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4f32e-ab08-44ca-94b6-b126e1e12e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0141f8-4a3d-44d1-ada6-0b0587fded58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleap_new",
   "language": "python",
   "name": "sleap_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
