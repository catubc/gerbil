{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import trange\n",
    "\n",
    "from pathlib import Path\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"DLClight\"] = \"True\"\n",
    "import deeplabcut\n",
    "import tensorflow.contrib.slim as slim\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "vers = (tf.__version__).split(\".\")\n",
    "if int(vers[0]) == 1 and int(vers[1]) > 12:\n",
    "    TF = tf.compat.v1\n",
    "else:\n",
    "    TF = tf\n",
    "from tensorflow.contrib.slim.nets import resnet_v1\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "#from deeplabcut.pose_estimation_tensorflow.train import get_optimizer, LearningRate\n",
    "from deeplabcut.pose_estimation_tensorflow.train import  LearningRate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ------------------- User inputs --------\n",
    "# define paths\n",
    "\n",
    "def get_optimizer(loss_op, cfg, train_resnet=False):\n",
    "    # train_resnet=True trains resnet!\n",
    "    learning_rate = TF.placeholder(tf.float32, shape=[])\n",
    "\n",
    "    if cfg.optimizer == \"sgd\":\n",
    "        optimizer = TF.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    elif cfg.optimizer == \"adam\":\n",
    "        optimizer = TF.train.AdamOptimizer(learning_rate)\n",
    "    else:\n",
    "        raise ValueError('unknown optimizer {}'.format(cfg.optimizer))\n",
    "\n",
    "    #%%\n",
    "    all_variables_to_train = tf.trainable_variables()\n",
    "    if train_resnet:\n",
    "        variables_to_train = all_variables_to_train\n",
    "    else:\n",
    "        variables_to_train = list(filter(lambda k: 'mclassid' in k.name, all_variables_to_train))\n",
    "\n",
    "    train_op = slim.learning.create_train_op(loss_op, optimizer, variables_to_train=variables_to_train)\n",
    "\n",
    "    return learning_rate, train_op\n",
    "\n",
    "#\n",
    "def get_train_config(cfg, shuffle=1):\n",
    "    from deeplabcut.utils import auxiliaryfunctions\n",
    "    from deeplabcut.pose_estimation_tensorflow.config import load_config\n",
    "    project_path = cfg['project_path']\n",
    "    iteration = cfg['iteration']\n",
    "    TrainingFraction = cfg['TrainingFraction'][iteration]\n",
    "    modelfolder = os.path.join(\n",
    "        project_path,\n",
    "        str(auxiliaryfunctions.GetModelFolder(TrainingFraction, shuffle, cfg)))\n",
    "\n",
    "    path_test_config = Path(modelfolder) / 'train' / 'pose_cfg.yaml'\n",
    "    print(path_test_config)\n",
    "    try:\n",
    "        dlc_cfg = load_config(str(path_test_config))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            \"It seems the model for shuffle %s and trainFraction %s does not exist.\"\n",
    "            % (shuffle, TrainingFraction))\n",
    "    return dlc_cfg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config_path = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/dlc/config.yaml'\n",
    "videofile_path = '/media/cat/4TBSSD/DLC_full_directory/videos_to_label/2020-3-16_12_54_07_193951_compressed_label.mp4'\n",
    "shuffle = 1 #dlc experiment shuffle\n",
    "\n",
    "#\n",
    "# train w imagenet: data/255\n",
    "#BATCH_SIZE = 60\n",
    "num_classes = 4\n",
    "is_training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2032, 100, 100)\n",
      "(2137, 100, 100)\n",
      "(1932, 100, 100)\n",
      "(2328, 100, 100)\n",
      "x train:  (8429, 100, 100, 3) y train:  (8429,) x test:  (2110, 100, 100, 3) y test:  (2110,)\n"
     ]
    }
   ],
   "source": [
    "# LOAD GERBIL DATA SPECIFICALLY FORMATED\n",
    "#%%\n",
    "\n",
    "def load_gerbil_training_data_34_x_34(animal_dir, \n",
    "                              n_networks):\n",
    "    \n",
    "    from skimage.measure import block_reduce\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    for k in range(n_networks):\n",
    "        data = np.load(animal_dir+str(k)+'.npy')\n",
    "        \n",
    "        # scale data down:\n",
    "        #image = np.arange(3*3*4).reshape(3, 3, 4)\n",
    "        data_downsampled = []\n",
    "        for p in range(data.shape[0]):\n",
    "            temp = data[p]\n",
    "           # print (\"temp start: \", temp.shape)\n",
    "            temp = block_reduce(data[p], \n",
    "                                block_size=(6,6), \n",
    "                                func=np.mean)\n",
    "            #print (\"temp finish: \", temp.shape)\n",
    "            data_downsampled.append(temp)\n",
    "        data = np.array(data_downsampled)\n",
    "        \n",
    "        idx_rand = np.random.choice(np.arange(data.shape[0]), int(data.shape[0]*0.8), replace=False)\n",
    "        #print (\"idx rand: \", idx_rand.shape)\n",
    "        \n",
    "        x_train.append(data[idx_rand])\n",
    "        print (data[idx_rand].shape)\n",
    "        y_train.append(np.zeros(idx_rand.shape[0])+k)\n",
    "\n",
    "        # make test set\n",
    "        idx_test = np.delete(np.arange(data.shape[0]), \n",
    "                             idx_rand)\n",
    "        #print (\"idx test: \", idx_test.shape)\n",
    "        temp = data[idx_test]\n",
    "        #print (temp.shape)\n",
    "        x_test.append(temp)\n",
    "        y_test.append(np.zeros(temp.shape[0])+k)\n",
    "\n",
    "        \n",
    "\n",
    "    x_train = np.vstack(x_train)\n",
    "    x_train = np.int32((x_train,x_train,x_train)).transpose(1,2,3,0)\n",
    "    #np.save('/home/cat/x_train.npy', x_train)\n",
    "    y_train = np.hstack(y_train)#[:,None]\n",
    "    \n",
    "    x_test = np.vstack(x_test)\n",
    "    x_test = np.int32((x_test,x_test,x_test)).transpose(1,2,3,0)\n",
    "    y_test = np.hstack(y_test) \n",
    "        \n",
    "        \n",
    "    return x_train, y_train, x_test, y_test \n",
    "\n",
    "def load_gerbil_training_data_100_x_100(animal_dir, \n",
    "                              n_networks):\n",
    "    \n",
    "    from skimage.measure import block_reduce\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    for k in range(n_networks):\n",
    "        data = np.load(animal_dir+str(k)+'.npy')\n",
    "        \n",
    "        # scale data down:\n",
    "        #image = np.arange(3*3*4).reshape(3, 3, 4)\n",
    "        data_downsampled = []\n",
    "        for p in range(data.shape[0]):\n",
    "            temp = data[p]\n",
    "           # print (\"temp start: \", temp.shape)\n",
    "            temp = block_reduce(data[p], \n",
    "                                block_size=(2,2), \n",
    "                                func=np.mean)\n",
    "            #print (\"temp finish: \", temp.shape)\n",
    "            data_downsampled.append(temp)\n",
    "        data = np.array(data_downsampled)\n",
    "        \n",
    "        idx_rand = np.random.choice(np.arange(data.shape[0]), int(data.shape[0]*0.8), replace=False)\n",
    "        #print (\"idx rand: \", idx_rand.shape)\n",
    "        \n",
    "        x_train.append(data[idx_rand])\n",
    "        print (data[idx_rand].shape)\n",
    "        y_train.append(np.zeros(idx_rand.shape[0])+k)\n",
    "\n",
    "        # make test set\n",
    "        idx_test = np.delete(np.arange(data.shape[0]), \n",
    "                             idx_rand)\n",
    "        #print (\"idx test: \", idx_test.shape)\n",
    "        temp = data[idx_test]\n",
    "        #print (temp.shape)\n",
    "        x_test.append(temp)\n",
    "        y_test.append(np.zeros(temp.shape[0])+k)\n",
    "\n",
    "        \n",
    "\n",
    "    x_train = np.vstack(x_train)\n",
    "    x_train = np.int32((x_train,x_train,x_train)).transpose(1,2,3,0)\n",
    "    #np.save('/home/cat/x_train.npy', x_train)\n",
    "    y_train = np.hstack(y_train)#[:,None]\n",
    "    \n",
    "    x_test = np.vstack(x_test)\n",
    "    x_test = np.int32((x_test,x_test,x_test)).transpose(1,2,3,0)\n",
    "    y_test = np.hstack(y_test) \n",
    "        \n",
    "        \n",
    "    return x_train, y_train, x_test, y_test \n",
    "\n",
    "\n",
    "animal_dir ='/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images_cc/segments_training/'\n",
    "#animal_dir = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images/animals/'\n",
    "animal_dir ='/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/2020-3-9_12_14_22_815059_compressed/cnn_training/animals/'\n",
    "\n",
    "\n",
    "def make_training_arrays(animal_dir):\n",
    "    \n",
    "    import os\n",
    "    dirs = os.listdir(animal_dir)\n",
    "    \n",
    "    if len(dirs)>4:\n",
    "        return\n",
    "    \n",
    "    print(dirs)\n",
    "    for ctr,dir_ in enumerate(dirs):\n",
    "        fnames = os.listdir(animal_dir+'/'+dir_)\n",
    "        \n",
    "        imgs = []\n",
    "        for fname in fnames:\n",
    "            \n",
    "            imgs.append(np.load(\n",
    "                            os.path.join(animal_dir,\n",
    "                                         dir_, fname                           \n",
    "                                            )))\n",
    "        imgs = np.array(imgs)\n",
    "        np.save(os.path.join(animal_dir,\n",
    "                            str(ctr)),\n",
    "               imgs)\n",
    "        print (imgs.shape)\n",
    "    \n",
    "make_training_arrays(animal_dir)\n",
    "\n",
    "n_networks = 4\n",
    "\n",
    "(x_train, y_train, x_test, y_test) = load_gerbil_training_data_100_x_100(animal_dir,\n",
    "                                                              n_networks)\n",
    "\n",
    "nx_in, ny_in, nc_in = x_train.shape[1:]\n",
    "assert nc_in == 3\n",
    "\n",
    "print (\"x train: \", x_train.shape, \n",
    "       \"y train: \", y_train.shape,  \n",
    "       \"x test: \", x_test.shape, \n",
    "       \"y test: \", y_test.shape)\n",
    "\n",
    "# 4-fold training; \n",
    "# required dimensions: [#_frames x 4, 100, 100, 3] ;  output [# frames x 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config...\n"
     ]
    }
   ],
   "source": [
    "# ------------------- READ DLC info --------\n",
    "# read config\n",
    "print(\"Reading config...\")\n",
    "cfg = deeplabcut.auxiliaryfunctions.read_config(str(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/dlc/dlc-models/iteration-0/madeline_july2Jul2-trainset95shuffle1/train/pose_cfg.yaml\n"
     ]
    }
   ],
   "source": [
    "# ------------------- READ DLC snapshot --------\n",
    "\n",
    "#overwrite_snapshot = None # load  resnet weights trained until last dlc snapshot\n",
    "# set to 'snapshot-0' to load resnet weights not trained for tracking w dlc\n",
    "#\n",
    "def get_train_config(cfg, shuffle=1):\n",
    "    from deeplabcut.utils import auxiliaryfunctions\n",
    "    from deeplabcut.pose_estimation_tensorflow.config import load_config\n",
    "    project_path = cfg['project_path']\n",
    "    iteration = cfg['iteration']\n",
    "    TrainingFraction = cfg['TrainingFraction'][iteration]\n",
    "    modelfolder = os.path.join(\n",
    "        project_path,\n",
    "        str(auxiliaryfunctions.GetModelFolder(TrainingFraction, shuffle, cfg)))\n",
    "\n",
    "    path_test_config = Path(modelfolder) / 'train' / 'pose_cfg.yaml'\n",
    "    print(path_test_config)\n",
    "    try:\n",
    "        dlc_cfg = load_config(str(path_test_config))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            \"It seems the model for shuffle %s and trainFraction %s does not exist.\"\n",
    "            % (shuffle, TrainingFraction))\n",
    "    return dlc_cfg\n",
    "\n",
    "dlc_cfg = get_train_config(cfg,shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ------------- BEGIN GRAPH -------------\n",
    "TF.reset_default_graph()\n",
    "# tf.data API\n",
    "batch_size = tf.placeholder(tf.int64, shape=[])\n",
    "x, y = tf.placeholder(tf.float32, shape=[None, nx_in, ny_in, nc_in]), tf.placeholder(tf.int32, shape=[None,])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat().shuffle(5000)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).shuffle(5000)\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "image_batch, targets = iterator.get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init iterators\n",
    "train_init_op = iterator.make_initializer(train_dataset)\n",
    "test_init_op = iterator.make_initializer(test_dataset)\n",
    "# upsample cifar images which are 32 x 32\n",
    "if nx_in <= 200:\n",
    "    inputs = tf.image.resize_images(image_batch, [224, 224])\n",
    "targets = tf.reshape(targets, [batch_size, ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make model\n",
    "net_funcs = {'resnet_50': resnet_v1.resnet_v1_50,\n",
    "             'resnet_101': resnet_v1.resnet_v1_101,\n",
    "             'resnet_152': resnet_v1.resnet_v1_152}\n",
    "\n",
    "net_fun = net_funcs[dlc_cfg.net_type]\n",
    "# preprocess the same way as dlc -- here note that they don't normalize\n",
    "mean = tf.constant(dlc_cfg.mean_pixel, dtype=tf.float32, shape=[1, 1, 1, 3],\n",
    "                   name='img_mean')\n",
    "im_centered = inputs - mean\n",
    "# add strides st we can load dlc weights\n",
    "if 'output_stride' not in dlc_cfg.keys():\n",
    "    dlc_cfg.output_stride = 16\n",
    "if 'deconvolutionstride' not in dlc_cfg.keys():\n",
    "    dlc_cfg.deconvolutionstride = 2\n",
    "with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "    net, endpoints = net_fun(im_centered, num_classes=None,#num_classes,\n",
    "                             is_training=is_training,\n",
    "                             #global_pool=True,\n",
    "                             output_stride=dlc_cfg.output_stride)\n",
    "\n",
    "net = tf.squeeze(net, axis=[1, 2])\n",
    "logits = slim.fully_connected(net, num_outputs=num_classes,\n",
    "                              activation_fn=None,\n",
    "                              scope='mclassid')\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, \n",
    "                                                   labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extrainfo\n",
    "logits2 = tf.nn.softmax(logits)\n",
    "classes = tf.cast(tf.argmax(logits2, axis=1, ), tf.int32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(classes, targets), dtype=tf.float32))\n",
    "eval_dict = {'logits2': logits2,\n",
    "             'classes': classes,\n",
    "             'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# restore variables for resnet\n",
    "variables_to_restore = slim.get_variables_to_restore(include=[\"resnet_v1\"])\n",
    "restorer = TF.train.Saver(variables_to_restore)\n",
    "saver = TF.train.Saver(\n",
    "        max_to_keep=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# %% Init session\n",
    "print (\"1\")\n",
    "config_TF = TF.ConfigProto()\n",
    "config_TF.gpu_options.allow_growth = True\n",
    "sess = TF.Session(config=config_TF)\n",
    "TF.summary.FileWriter('logs/', sess.graph)\n",
    "print (\"2\")\n",
    "\n",
    "learning_rate, train_op = get_optimizer(loss, dlc_cfg)\n",
    "lr = tf.get_variable('learning_rate', initializer=0.01, trainable=False)\n",
    "print (\"3\")\n",
    "#%%\n",
    "sess.run(TF.global_variables_initializer())\n",
    "print (\"4\")\n",
    "sess.run(TF.local_variables_initializer())\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored variables from\n",
      "/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/dlc/resnet_v1_50_2016_08_28/resnet_v1_50.ckpt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Restore the resnet weights from dlc\n",
    "dlc_cfg.init_weights = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/dlc/resnet_v1_50_2016_08_28/resnet_v1_50.ckpt'\n",
    "\n",
    "#model_name = \n",
    "#dlc_cfg.init_weights = '/home/cat/code/dlc_data/snapshots/mclass_epoch49-iter96--96'\n",
    "\n",
    "restorer.restore(sess, dlc_cfg.init_weights)\n",
    "print('Restored variables from\\n{}\\n'.format(dlc_cfg.init_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "lr_gen = LearningRate(dlc_cfg)\n",
    "#%%\n",
    "EPOCHS = 50 # 10000\n",
    "current_lr = 0.0001\n",
    "BATCH_SIZE = 300\n",
    "n_batches = max(1, int(x_train.shape[0] / BATCH_SIZE))\n",
    "if True:\n",
    "    print('Training...')\n",
    "    for epoch in range(EPOCHS):\n",
    "        sess.run(train_init_op, feed_dict={x: x_train, y: y_train, batch_size: BATCH_SIZE,\n",
    "                                           learning_rate: current_lr})\n",
    "\n",
    "        tot_loss = 0\n",
    "        for iter in range(n_batches):\n",
    "            _, loss_value, accuracy_value = sess.run([train_op, loss, accuracy],\n",
    "                                                     feed_dict={learning_rate:current_lr})\n",
    "            if iter%25==0:\n",
    "                print(\"Epoch: {} \\t Iter: {}/{}, Loss: {:.4f} Accuracy:{:.4f}\".format(epoch, iter, n_batches, loss_value, accuracy_value))\n",
    "            tot_loss += loss_value\n",
    "\n",
    "            if iter % 2500 == 0 or (iter+1 == n_batches):\n",
    "                model_name = 'snapshots/mclass_epoch{}-iter{}-'.format(epoch,iter)\n",
    "                saver.save(sess, model_name, global_step=iter)\n",
    "                if iter +1  == EPOCHS:\n",
    "                    model_name = 'snapshots/mclass_epoch{}-iter{}-final-'.format(epoch,iter)\n",
    "                    saver.save(sess, model_name, global_step=0)\n",
    "\n",
    "        print(\"Epoch: {}, Loss: {:.4f}\".format(epoch, tot_loss / n_batches))\n",
    "        # initialise iterator with test data\n",
    "        sess.run(test_init_op,\n",
    "                 feed_dict={x: x_test, y: y_test, batch_size: BATCH_SIZE, learning_rate:current_lr})\n",
    "        print('Epoch: {}, '.format(epoch) + 'Test Loss: {:.4f} Test Accuracy {:.4f}'.format(*sess.run([loss,accuracy])))\n",
    "\n",
    "else:\n",
    "\n",
    "    model_name = '/home/cat/code/dlc_data/snapshots/mclass_epoch49-iter96--96'\n",
    "    dlc_cfg.init_weights = model_name\n",
    "\n",
    "    restorer.restore(sess, dlc_cfg.init_weights)\n",
    "    print('Restored variables from\\n{}\\n'.format(dlc_cfg.init_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%% TEST ON THE TRAINING DATA\n",
    "# batch_np = 1\n",
    "# print (x_test.shape)\n",
    "# #for ii ,(image_np, target_np) in enumerate(zip(x_test, y_test)):\n",
    "# ids = np.random.choice(np.arange(x_test.shape[0]), 500, replace=False)\n",
    "# ctr=0\n",
    "# for id_ in ids:\n",
    "#     image_np = x_test[id_][None, :]\n",
    "#     target_np = y_test[id_]\n",
    "#     feed_dict ={image_batch: image_np,\n",
    "#                 #targets: target_np,\n",
    "#                 batch_size: batch_np}\n",
    "\n",
    "#     classes_np, logits2_np = sess.run([classes, logits2], feed_dict=feed_dict)\n",
    "#     if id_%100==0:\n",
    "#         print (\"image: \", id_, \"  predicted: \", classes_np[0], \" true val: \", target_np, \"  confidence: \", logits2_np.max())\n",
    "#         print ('...')\n",
    "#     if classes_np[0]==target_np:\n",
    "#         ctr+=1\n",
    "#     #p#rint('True class {} Predicted class {} Confidence {}'.format(target_np, classes_np[0], logits2_np.max()))\n",
    "# #%%\n",
    "# print (\"% Correct: \", ctr/ids.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_cnn2(x_test, \n",
    "                y_test):\n",
    "    \n",
    "    batch_np = 10\n",
    "    ids = np.arange(0,\n",
    "                    x_test.shape[0],\n",
    "                    batch_np)\n",
    "\n",
    "    ctr=0\n",
    "    class_array = []\n",
    "    logit_array = []\n",
    "    for id_ in tqdm(ids):\n",
    "\n",
    "        #print (\"frame: \", id_)\n",
    "        image_np = x_test[id_:id_+batch_np] #[None, :]\n",
    "        target_np = y_test[id_:id_+batch_np]\n",
    "        feed_dict ={image_batch: image_np,\n",
    "                    #targets: target_np,\n",
    "                    batch_size: batch_np}\n",
    "\n",
    "        classes_np, logits2_np = sess.run([classes, logits2], feed_dict=feed_dict)\n",
    "        #if classes_np[0]==target_np:\n",
    "        #    ctr+=1\n",
    "        class_array.append(classes_np)\n",
    "        logit_array.append(np.max(logits2))\n",
    "        \n",
    "    return ctr, ids.shape[0], class_array, logit_array\n",
    "\n",
    "\n",
    "def load_gerbil_single_directory_100_x_100(animal_dir,\n",
    "                                 animal_id):\n",
    "    \n",
    "    from skimage.measure import block_reduce\n",
    "    import glob\n",
    "    \n",
    "    data = []\n",
    "    #print(animal_dir+'/**/*.npy')\n",
    "    fnames = glob.glob(animal_dir+'*.npy',recursive = True)\n",
    "    #print (\"Fnames; \", len(fnames))\n",
    "    \n",
    "    ctr_k = 0\n",
    "    frame_ids = []\n",
    "    for ctr, fname in enumerate(fnames):\n",
    "        if '.npy' in fname:\n",
    "            temp = np.load(fname)\n",
    "            idx1 = fname.index('frame_')\n",
    "            idx2 = fname.index('_id_')\n",
    "            frame_id = int(fname[idx1+6:idx2])\n",
    "            #print (frame_id)\n",
    "            frame_ids.append(frame_id)\n",
    "            if temp.shape[0]!= 200 or temp.shape[1]!=200:\n",
    "                continue\n",
    "            #print (ctr, temp.shape)\n",
    "            data.append(temp)\n",
    "            ctr_k+=1\n",
    "    #print (ctr_k)\n",
    "            \n",
    "    data = np.array(data)\n",
    "    #print (\"data: \", \n",
    "    #       data.shape)\n",
    "\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    #data = np.load(animal_dir+\"id_\"+str(k)+'.npy')\n",
    "        \n",
    "    # scale data down:\n",
    "    #image = np.arange(3*3*4).reshape(3, 3, 4)\n",
    "    data_downsampled = []\n",
    "    for p in range(data.shape[0]):\n",
    "        temp = data[p]\n",
    "        #print (\"temp start: \", temp.shape)\n",
    "        temp = block_reduce(temp, \n",
    "                            block_size=(2,2), \n",
    "                            func=np.mean)\n",
    "        #print (\"temp finish: \", temp.shape)\n",
    "        data_downsampled.append(temp)\n",
    "    x_test = np.array(data_downsampled)\n",
    "    #print (x_test.shape)\n",
    "    y_test = np.zeros(x_test.shape[0])+ animal_id\n",
    "    \n",
    "    #x_test = np.vstack(x_test)\n",
    "    x_test = np.int32((x_test,x_test,x_test)).transpose(1,2,3,0)\n",
    "    y_test = np.hstack(y_test)[:,None]\n",
    "        \n",
    "    #print (\"x test: \", x_test.shape,\n",
    "    #       \"y test: \", y_test.shape)\n",
    "    \n",
    "    return x_test, y_test, frame_ids\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12467/12467 [05:58<00:00, 34.74it/s] \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 292353 is out of bounds for axis 1 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-71701dbaf1ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#print (vals)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#x_test = np.vstack(x_array)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 292353 is out of bounds for axis 1 with size 100"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################################################\n",
    "############## ALL DATA FORMATTING FOR CLASSIFICATION ################\n",
    "######################################################################\n",
    "\n",
    "# MAKE TRAINING DATA FOR A SINGLE NETWORK\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "root_dirs = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images_cc/segments_all/'\n",
    "root_dirs = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images/'\n",
    "dirs = glob.glob(root_dirs + '/**')\n",
    "\n",
    "x_array = []\n",
    "y_array = []\n",
    "frames_array = []\n",
    "vals = []\n",
    "x_array = np.zeros((300000, 100, 100, 3),'uint8')\n",
    "from tqdm import tqdm\n",
    "start = 0\n",
    "for ctr, dir_ in enumerate(tqdm(dirs)):\n",
    "    # print (dir_)\n",
    "    (x_test, y_test, frame_ids) = load_gerbil_single_directory_100_x_100(dir_+'/', ctr)\n",
    "    #print (x_test.shape, y_test.shape)\n",
    "    frames_array.append(frame_ids)\n",
    "    \n",
    "    #x_array.append(x_test)\n",
    "    length = x_test.shape[0]\n",
    "    x_array[start:start+length]= x_test\n",
    "    #print (start, length)\n",
    "    start = start+ length    \n",
    "    y_array.append(y_test)\n",
    "    vals.append(np.zeros(x_test.shape[0])+np.int32(os.path.split(dir_)[1]))\n",
    "    \n",
    "vals = np.hstack(vals)\n",
    "#print (vals)\n",
    "#x_test = np.vstack(x_array)\n",
    "x_test = x_array[:start]\n",
    "print (x_test.shape)\n",
    "y_test = np.vstack(y_array)\n",
    "print (y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 100, 100, 3)\n",
      "(292353, 100, 100, 3)\n",
      "(292353, 1)\n"
     ]
    }
   ],
   "source": [
    "print (x_array.shape)\n",
    "x_test = x_array[:start]\n",
    "print (x_test.shape)\n",
    "y_test = np.vstack(y_array)\n",
    "print (y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images_cc/segments_all/frames_array.npy',\n",
    "#        frames_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29236/29236 [46:40<00:00, 10.44it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  [4 4 4 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "############## RUN CLASSIFIER ON ALL DATA ############################\n",
    "######################################################################\n",
    "(ctr, ids, classes, logits) = predict_cnn2(x_test, y_test)\n",
    "  \n",
    "classes = np.hstack(classes)\n",
    "print (\"classes: \", classes)\n",
    "\n",
    "np.savez(root_dirs+'/classification_output.npz',\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "        vals=vals,\n",
    "        classes=classes,\n",
    "        frames_array=frames_array)\n",
    "#print (np.unique(classes, return_counts=True))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"CAN not do any more visualization, backend is off : GO TO NOTEBOOK 12: MOVIE MAKING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################################################\n",
    "# ############## SHOW EXAMPLES OF CLASSIFIED DATA ######################\n",
    "# ######################################################################\n",
    "# start = 0\n",
    "# end = 24\n",
    "# vals_temp = vals[start:end]\n",
    "\n",
    "# data = np.load('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images_cc/segments_all/classification_output.npz')\n",
    "\n",
    "# print (np.unique(vals_temp).shape)\n",
    "# for ctr, dir_ in enumerate(np.unique(vals_temp)):\n",
    "#     idx = np.where(vals_temp==dir_)\n",
    "    \n",
    "#     temp_classes = classes[idx]\n",
    "    \n",
    "#     ax=plt.subplot(4,6,ctr+1)\n",
    "#     print (dir_, temp_classes)\n",
    "# plt.show\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# ############## SHOW DISTRIBUTION OF CLASSSES ######################\n",
    "# ######################################################################\n",
    "\n",
    "# classes = np.hstack(classes)\n",
    "# print (classes)\n",
    "\n",
    "# y = np.histogram(classes, np.arange(5))\n",
    "# plt.bar(y[1][:-1], y[0], 0.9)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "\n",
    "# def load_gerbil_all_data2(fname_test_data,\n",
    "#                         animal_id):\n",
    "    \n",
    "#     ''' This loads previously saved data arrays from 4 animals\n",
    "    \n",
    "#     '''\n",
    "#     from skimage.measure import block_reduce\n",
    "    \n",
    "#     data = np.load(fname_test_data)\n",
    "\n",
    "#     x_test = []\n",
    "#     y_test = []\n",
    "#     #data = np.load(animal_dir+\"id_\"+str(k)+'.npy')\n",
    "        \n",
    "#     # scale data down:\n",
    "#     #image = np.arange(3*3*4).reshape(3, 3, 4)\n",
    "#     data_downsampled = []\n",
    "#     for p in range(data.shape[0]):\n",
    "#         temp = data[p]\n",
    "#        # print (\"temp start: \", temp.shape)\n",
    "#         temp = block_reduce(data[p], \n",
    "#                             block_size=(6,6), \n",
    "#                             func=np.mean)\n",
    "#         #print (\"temp finish: \", temp.shape)\n",
    "#         data_downsampled.append(temp)\n",
    "#     x_test = np.array(data_downsampled)\n",
    "#     #print (x_test.shape)\n",
    "#     y_test = np.zeros(x_test.shape[0])+ animal_id\n",
    "    \n",
    "#     #x_test = np.vstack(x_test)\n",
    "#     x_test = np.int32((x_test,x_test,x_test)).transpose(1,2,3,0)\n",
    "#     y_test = np.hstack(y_test)[:,None]\n",
    "        \n",
    "#     print (\"x test: \", x_test.shape,\n",
    "#            \"y test: \", y_test.shape)\n",
    "#     return x_test, y_test\n",
    "\n",
    "\n",
    "# def load_gerbil_all_data(animal_dir,\n",
    "#                         animal_id):\n",
    "    \n",
    "#     from skimage.measure import block_reduce\n",
    "    \n",
    "#     names = ['female','male','pup1','pup2']\n",
    "\n",
    "#     import glob\n",
    "\n",
    "    \n",
    "#     if True:\n",
    "#         data = []\n",
    "#         fnames = glob.glob(animal_dir+'/**/*',recursive = True)\n",
    "#         #print (\"Fnames; \", fnames)\n",
    "#         for fname in fnames:\n",
    "#             if '.npy' in fname:\n",
    "#                 temp = np.load(fname)\n",
    "#                 print (temp.shape)\n",
    "#                 data.append(temp)\n",
    "                \n",
    "#         data = np.array(data)\n",
    "#         print (\"data: \", \n",
    "#                data.shape)\n",
    "\n",
    "#     x_test = []\n",
    "#     y_test = []\n",
    "#     #data = np.load(animal_dir+\"id_\"+str(k)+'.npy')\n",
    "        \n",
    "#     # scale data down:\n",
    "#     #image = np.arange(3*3*4).reshape(3, 3, 4)\n",
    "#     data_downsampled = []\n",
    "#     for p in range(data.shape[0]):\n",
    "#         temp = data[p]\n",
    "#         print (\"temp start: \", temp.shape)\n",
    "#         temp = block_reduce(temp, \n",
    "#                             block_size=(6,6), \n",
    "#                             func=np.mean)\n",
    "#         #print (\"temp finish: \", temp.shape)\n",
    "#         data_downsampled.append(temp)\n",
    "#     x_test = np.array(data_downsampled)\n",
    "#     print (x_test.shape)\n",
    "#     y_test = np.zeros(x_test.shape[0])+ animal_id\n",
    "    \n",
    "#     #x_test = np.vstack(x_test)\n",
    "#     x_test = np.int32((x_test,x_test,x_test)).transpose(1,2,3,0)\n",
    "#     y_test = np.hstack(y_test)[:,None]\n",
    "        \n",
    "#     print (\"x test: \", x_test.shape,\n",
    "#            \"y test: \", y_test.shape)\n",
    "#     return x_test, y_test\n",
    "\n",
    "\n",
    "# animals = ['female','male','pup1','pup2']\n",
    "\n",
    "# if False:\n",
    "#     root_dir = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images/animals/'\n",
    "\n",
    "#     for animal_id, animal in enumerate(animals):\n",
    "#         animal_dir = root_dir + animal\n",
    "\n",
    "#         (x_test, y_test) = load_gerbil_all_data(animal_dir,\n",
    "#                             animal_id)\n",
    "\n",
    "        \n",
    "#         (ctr, n_images, classes_array, logit_array) = predict_cnn(x_test, \n",
    "#                                                                   y_test)\n",
    "\n",
    "# if False:\n",
    "#     root_dir = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images/animals/id_'\n",
    "#     for animal_id in range(4):\n",
    "        \n",
    "#         fname_test_data = root_dir + str(animal_id) + '_test.npy'\n",
    "        \n",
    "#         #if animal_id ==1:\n",
    "#         #    continue\n",
    "            \n",
    "#         (x_test, y_test) =  load_gerbil_all_data2(fname_test_data,\n",
    "#                         animal_id)\n",
    "        \n",
    "#         (ctr, n_images, classes_array, logit_array) = predict_cnn(x_test, \n",
    "#                                                                   y_test)\n",
    "        \n",
    "#         print (animals[animal_id], animal_id,\n",
    "#                \" # images: \", n_images, \n",
    "#                \" correct classification: \", ctr/n_images, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%% NEW: load one frame\n",
    "# import tqdm\n",
    "\n",
    "# def predict_cnn(x_test, \n",
    "#                 y_test):\n",
    "    \n",
    "#     batch_np = 10\n",
    "#     ids = np.arange(0,\n",
    "#                     x_test.shape[0],\n",
    "#                     batch_np)\n",
    "\n",
    "#     ctr=0\n",
    "#     class_array = []\n",
    "#     logit_array = []\n",
    "#     for id_ in ids:\n",
    "#         #if id_%100==0:\n",
    "#         print (\"fraem: \", id_)\n",
    "#         image_np = x_test[id_:id_+batch_np] #[None, :]\n",
    "#         target_np = y_test[id_:id_+batch_np]\n",
    "#         feed_dict ={image_batch: image_np,\n",
    "#                     #targets: target_np,\n",
    "#                     batch_size: batch_np}\n",
    "\n",
    "#         classes_np, logits2_np = sess.run([classes, logits2], feed_dict=feed_dict)\n",
    "#         #if classes_np[0]==target_np:\n",
    "#         #    ctr+=1\n",
    "#         class_array.append(classes_np)\n",
    "#         logit_array.append(np.max(logits2))\n",
    "        \n",
    "#     return ctr, ids.shape[0], class_array, logit_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# def load_gerbil_single_directory(animal_dir,\n",
    "#                                  animal_id):\n",
    "    \n",
    "#     from skimage.measure import block_reduce\n",
    "    \n",
    "#     import glob\n",
    "\n",
    "    \n",
    "#     data = []\n",
    "#     #print(animal_dir+'/**/*.npy')\n",
    "#     fnames = glob.glob(animal_dir+'/**/*.npy',recursive = True)\n",
    "#     print (\"Fnames; \", len(fnames))\n",
    "    \n",
    "#     ctr_k = 0\n",
    "#     for ctr, fname in enumerate(fnames):\n",
    "#         if '.npy' in fname:\n",
    "#             temp = np.load(fname)\n",
    "#             if temp.shape[0]!= 200 or temp.shape[1]!=200:\n",
    "#                 continue\n",
    "#             #print (ctr, temp.shape)\n",
    "#             data.append(temp)\n",
    "#             ctr_k+=1\n",
    "#     print (ctr_k)\n",
    "            \n",
    "#     data = np.array(data)\n",
    "#     print (\"data: \", \n",
    "#            data.shape)\n",
    "\n",
    "#     x_test = []\n",
    "#     y_test = []\n",
    "#     #data = np.load(animal_dir+\"id_\"+str(k)+'.npy')\n",
    "        \n",
    "#     # scale data down:\n",
    "#     #image = np.arange(3*3*4).reshape(3, 3, 4)\n",
    "#     data_downsampled = []\n",
    "#     for p in range(data.shape[0]):\n",
    "#         temp = data[p]\n",
    "#         #print (\"temp start: \", temp.shape)\n",
    "#         temp = block_reduce(temp, \n",
    "#                             block_size=(6,6), \n",
    "#                             func=np.mean)\n",
    "#         #print (\"temp finish: \", temp.shape)\n",
    "#         data_downsampled.append(temp)\n",
    "#     x_test = np.array(data_downsampled)\n",
    "#     print (x_test.shape)\n",
    "#     y_test = np.zeros(x_test.shape[0])+ animal_id\n",
    "    \n",
    "#     #x_test = np.vstack(x_test)\n",
    "#     x_test = np.int32((x_test,x_test,x_test)).transpose(1,2,3,0)\n",
    "#     y_test = np.hstack(y_test)[:,None]\n",
    "        \n",
    "#     print (\"x test: \", x_test.shape,\n",
    "#            \"y test: \", y_test.shape)\n",
    "#     return x_test, y_test\n",
    "\n",
    "# # MAKE TRAINING DATA FOR A SINGLE NETWORK\n",
    "\n",
    "# # root_dir = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images/10000_frames_all_networks/5/'\n",
    "# # animal_id = 0\n",
    "\n",
    "# # (x_test, y_test) = load_gerbil_single_directory(root_dir,\n",
    "# #                                                 animal_id)\n",
    "# # #print (x_test.shape)\n",
    "\n",
    "# # print (\"DONE Loading data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def predict_cnn2(x_test, \n",
    "#                 y_test):\n",
    "    \n",
    "#     batch_np = 10\n",
    "#     ids = np.arange(0,\n",
    "#                     x_test.shape[0],\n",
    "#                     batch_np)\n",
    "\n",
    "#     ctr=0\n",
    "#     class_array = []\n",
    "#     logit_array = []\n",
    "#     for id_ in ids:\n",
    "#         #if id_%100==0:\n",
    "#         print (\"fraem: \", id_)\n",
    "#         image_np = x_test[id_:id_+batch_np] #[None, :]\n",
    "#         target_np = y_test[id_:id_+batch_np]\n",
    "#         feed_dict ={image_batch: image_np,\n",
    "#                     #targets: target_np,\n",
    "#                     batch_size: batch_np}\n",
    "\n",
    "#         classes_np, logits2_np = sess.run([classes, logits2], feed_dict=feed_dict)\n",
    "#         #if classes_np[0]==target_np:\n",
    "#         #    ctr+=1\n",
    "#         class_array.append(classes_np)\n",
    "#         logit_array.append(np.max(logits2))\n",
    "        \n",
    "#     return ctr, ids.shape[0], class_array, logit_array\n",
    "\n",
    "\n",
    "# root_dir = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images/10000_frames_all_networks/'\n",
    "\n",
    "\n",
    "# current_dir = root_dir\n",
    "# all_sub_dir_paths = glob(str(current_dir) + '/*/') # returns list of sub directory paths\n",
    "\n",
    "# all_sub_dir_names = [Path(sub_dir).name for sub_dir in all_sub_dir_paths] \n",
    "\n",
    "# print (len(all_sub_dir_names))\n",
    "\n",
    "# for dir_ in all_sub_dir_names[10:11]:\n",
    "#     (x_test, y_test) = load_gerbil_single_directory(root_dir+dir_,\n",
    "#                                                     0)\n",
    "#     #print (type(x_test))\n",
    "    \n",
    "#     (ctr, ids, classes, logits) = predict_cnn2(x_test,\n",
    "#                                               y_test)\n",
    "    \n",
    "#     # \n",
    "#     print (\"classes: \", classes)\n",
    "#     print (np.unique(classes, return_counts=True))\n",
    "#     #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The name '/home/cat/code/dlc_toolbox/snapshots/mclass_epoch1-iter32--32.data-00000-of-00001' looks like an (invalid) Operation name, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-aae1ddf1e4cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m##model = graph.get_tensor_by_name('resnet_model/final_dense:0')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#model.predict()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'/home/cat/code/dlc_toolbox/snapshots/mclass_epoch1-iter32--32.data-00000-of-00001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#res = sess.run(model, {inputs:img})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3652\u001b[0m       raise TypeError(\"Tensor names are strings (or similar), not %s.\" %\n\u001b[1;32m   3653\u001b[0m                       type(name).__name__)\n\u001b[0;32m-> 3654\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_tensor_by_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3480\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3550\u001b[0m         err_msg += (\" Tensor names must be of the form \"\n\u001b[1;32m   3551\u001b[0m                     \"\\\"<op_name>:<output_index>\\\".\")\n\u001b[0;32m-> 3552\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3554\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The name '/home/cat/code/dlc_toolbox/snapshots/mclass_epoch1-iter32--32.data-00000-of-00001' looks like an (invalid) Operation name, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\"."
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "#image_batch = x_test[:10]\n",
    "#inputs = tf.image.resize_images(image_batch, [224, 224])\n",
    "#print (inputs.shape)\n",
    "graph = tf.get_default_graph()\n",
    "    \n",
    "feed_dict={x: x_test[:10]}\n",
    "\n",
    "##model = graph.get_tensor_by_name('resnet_model/final_dense:0')\n",
    "#model.predict()\n",
    "model = graph.get_tensor_by_name('/home/cat/code/dlc_toolbox/snapshots/mclass_epoch1-iter32--32.data-00000-of-00001')\n",
    "\n",
    "#res = sess.run(model, {inputs:img})\n",
    "    \n",
    "#res = sess.predict(test_init_op,  \n",
    "#               feed_dict = feed_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
