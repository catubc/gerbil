{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import gridspec\n",
    "from scipy import signal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from tqdm import trange\n",
    "\n",
    "#import glob2\n",
    "\n",
    "from numba import jit\n",
    "import tables\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import h5py\n",
    "#import hdf5storage\n",
    "import csv\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default functions\n",
    "        \n",
    "def load_csv(fname):\n",
    "    with open(fname, newline='') as csvfile:\n",
    "        data = list(csv.reader(csvfile))\n",
    "\n",
    "    labels = data[1]\n",
    "    #print (\"data labels: \", labels)\n",
    "    #print (\"column vals: \", data[2])\n",
    "\n",
    "    # load values\n",
    "    data_array = np.array(data[3:])\n",
    "    #print (\"# of datapoints (x,y,likelihood): \", data_array.shape)\n",
    "\n",
    "    # \n",
    "    #labels = ['fnose','f_leye','f_reye','f_lear','f_rear','f_',\n",
    "    #         'male_nose','male_left_ear','male_right_ear','male_base_of_tail',\n",
    "    #          'pup_shaved_nose','pup_shaved_left_ear','pup_shaved_right_ear','pup_shaved_base_of_tail',\n",
    "    #          'pup_noshave_nose','pup_noshave_left_ear','pup_noshave_right_ear','pup_noshave_base_of_tail'             \n",
    "    #         ]\n",
    "    \n",
    "    labels = labels[1:]\n",
    "    \n",
    "    traces = []\n",
    "    traces_nan = []\n",
    "    # zero out low quality DLC values\n",
    "    for idx in range(1,len(labels)-1,3):\n",
    "        #print (\"idx: \", idx)\n",
    "        #print (data_array[1:,idx:idx+3])\n",
    "        #print (data_array[1:,idx:idx+3].shape)\n",
    "\n",
    "        temp = data_array[1:,idx:idx+3]\n",
    "        idx1 = np.where(temp=='')[0]\n",
    "        temp[idx1]=0\n",
    "        temp = temp.astype(np.float)# np.array(temp)\n",
    "        #print (idx, \"TEMP: \", temp.shape)\n",
    "        #temp.replace(\"''\",'0')\n",
    "        \n",
    "\n",
    "        # replace low likelihoods with median\n",
    "        likelihoods = temp[:,2]\n",
    "        idx2 = np.where(likelihoods<0.8)[0]\n",
    "        temp[idx2,0]=np.median(temp[:,0])\n",
    "        temp[idx2,1]=np.median(temp[:,1])\n",
    "        traces.append(temp.copy())\n",
    "        \n",
    "        temp[idx2,0]=np.nan\n",
    "        temp[idx2,1]=np.nan\n",
    "        traces_nan.append(temp.copy())\n",
    "\n",
    "    return traces, labels, traces_nan\n",
    "\n",
    "# function that does search forward steps: \n",
    "def search_forward(data_assembled_fixed,\n",
    "                  traces_inf,\n",
    "                  threshold_p,\n",
    "                  dist_threshold,\n",
    "                  data_assembled_all_features,\n",
    "                  selected_feature,\n",
    "                  comments=False):\n",
    "\n",
    "    # count # of merges\n",
    "    n_merges = 0\n",
    "    \n",
    "    # load traces\n",
    "    tracex = data_assembled_fixed[:,0]\n",
    "    tracey = data_assembled_fixed[:,1]\n",
    "\n",
    "    # find assembled locations with high prob. \n",
    "    probs = data_assembled_fixed[:,2]\n",
    "    idx = np.where(probs>threshold_p)[0]\n",
    "    \n",
    "    # if no values over threshold skip chunk;\n",
    "    if idx.shape[0]==0:\n",
    "        return data_assembled_fixed, data_assembled_all_features\n",
    "    \n",
    "    \n",
    "    if comments:\n",
    "        print (\"data assembled fixed: \", data_assembled_fixed.shape)\n",
    "        print (\"traces inf: \", traces_inf.shape)\n",
    "        print (\"probs: \", probs[:10])\n",
    "        print (\"IDX: \", idx.shape)\n",
    "\n",
    "    # find ends of continous labeled segments\n",
    "    # old method\n",
    "    #diffs = idx[1:]-idx[:-1]\n",
    "    #seg_ends = np.where(diffs>1)[0]\n",
    "    seg_ends=[]\n",
    "    for k in range(idx.shape[0]-1):\n",
    "        if (idx[k+1]-idx[k])>1:\n",
    "            seg_ends.append(k)\n",
    "    # append the last value if it's not at the end of the data\n",
    "    if idx.shape[0]<data_assembled_fixed.shape[0]:\n",
    "        if idx.shape[0]>0:\n",
    "            seg_ends.append(idx.shape[0]-1)\n",
    "    #print (\"idx shape:\" , idx.shape[0], \" tracex: \", tracesx.shape[0])\n",
    "    #print (\"Actual last idx values: \", idx[-1])\n",
    "\n",
    "    seg_ends = np.array(seg_ends)\n",
    "    #print (seg_ends.shape, idx[-1].shape, idx[-1])\n",
    "    # add last segment also\n",
    "    #if idx[-1]!=(tracex.shape[0]-1):\n",
    "    #    seg_ends.append()\n",
    "#     if idx.shape[0]>0:\n",
    "#         seg_ends=np.concatenate((seg_ends, [idx[-1]]),axis=0)\n",
    "    \n",
    "    if comments:\n",
    "        #print (\"diffs[ends]: \", diffs[seg_ends])\n",
    "        if seg_ends.shape[0]>0:\n",
    "            print (seg_ends)\n",
    "            print (\"seg ends: \", idx[seg_ends])\n",
    "\n",
    "    # propagate forward\n",
    "    # loop over all ends of continous segments and search forward for min distances\n",
    "    for l in range(seg_ends.shape[0]):\n",
    "        idx_start = idx[seg_ends[l]]\n",
    "\n",
    "        # initalized with the first starting location;\n",
    "        loc0 = np.array([tracex[idx_start], tracey[idx_start]])\n",
    "\n",
    "        # search forward\n",
    "        idx_next = idx_start+1\n",
    "        if idx_next>=1000:\n",
    "            continue\n",
    "        while True:\n",
    "            if comments:\n",
    "                print (\"idx_current: \", idx_next-1 , \"/\", seg_ends.shape[0],\n",
    "                       \"loc0: \", loc0, \" prob: \", probs[idx_next-1])\n",
    "                print (\"idx_next: \", idx_next , \n",
    "                       \"loc0: \", np.array([tracex[idx_next], tracey[idx_next]]), \" prob: \", probs[idx_next])\n",
    "\n",
    "            # if there is no inference, delete data\n",
    "            if len(traces_inf[idx_next])==0:\n",
    "                break\n",
    "                \n",
    "            loc_candidates = np.vstack(traces_inf[idx_next]).T\n",
    "            if comments:\n",
    "                print (\"loc_candidates: \", loc_candidates)\n",
    "  \n",
    "            # compute distance between previous assembled (true) location and canadidate\n",
    "            dist = (loc_candidates.T - loc0)**2\n",
    "            dist = np.sum(dist, axis=1)\n",
    "            dist = np.sqrt(dist)\n",
    "\n",
    "            # minimum distance is less than threshold, add to assembled animal data;\n",
    "            min_dist = np.min(dist)\n",
    "            arg_min = np.argmin(dist)\n",
    "            if comments:\n",
    "                print (\"Dist: \", min_dist, \" argmin: \", arg_min, \" values; \", loc_candidates.T[arg_min])\n",
    "\n",
    "            if min_dist < dist_threshold:\n",
    "                \n",
    "                # first check if the value has alerady been assigned to an assembled animal;\n",
    "                if comments:\n",
    "                    print (\" CROSS VALUE SEARCH *****************************************************\")\n",
    "                    print (data_assembled_all_features.shape)\n",
    "                    print (\"data assembled all features; \", data_assembled_all_features[:,idx_next,0])\n",
    "                    print (\"best inference candidate: \", loc_candidates[0][arg_min])\n",
    "                    print (np.min(np.abs(data_assembled_all_features[:,idx_next, 0]-loc_candidates[0][arg_min])))\n",
    "\n",
    "                # Chekc if x values are identical between a previously assembled feature and the best inference match\n",
    "                # if so, check if the probability of the assembled feature is above thrshold (ie. >0 as it's already been set to 0)\n",
    "                # and skip it; \n",
    "                # otherwise, do not inherit the label and exit the segment\n",
    "                if np.min(np.abs(data_assembled_all_features[:,idx_next,0]-loc_candidates[0][arg_min]))<1E-5:\n",
    "                    if np.min(np.abs(data_assembled_all_features[:,idx_next,1]-loc_candidates[1][arg_min]))<1E-5:\n",
    "                        argmin_temp = np.argmin(np.abs(data_assembled_all_features[:,idx_next,0]-\n",
    "                                                    loc_candidates[0][arg_min]))\n",
    "                        if data_assembled_all_features[argmin_temp,idx_next,2]>0.0:\n",
    "                            if comments:\n",
    "                                print (\"Infered value belongs to already assbmeld feature\", argmin_temp,\n",
    "                                      \"  with prob: \", data_assembled_all_features[argmin_temp,idx_next,2])\n",
    "                                print (\"data_assembled_all_features[idx_next]: \", data_assembled_all_features.shape)\n",
    "                                print (np.argmin(np.abs(data_assembled_all_features[:,0]-loc_candidates[0][arg_min])))\n",
    "\n",
    "                            break\n",
    "\n",
    "                if comments:\n",
    "                    print (\"replace assembled val at time step: \", idx_next, \" at loc: \", loc0 )\n",
    "                    \n",
    "                loc0 = np.array([loc_candidates[0][arg_min],\n",
    "                                loc_candidates[1][arg_min]])\n",
    "                if comments:\n",
    "\n",
    "                    print (\"      with new location from inference\", loc0)\n",
    "\n",
    "                    print ('data_assembled_fixed[feature][pre]: ', \n",
    "                           data_assembled_fixed[idx_next])\n",
    "\n",
    "                \n",
    "                # fix the data in progress\n",
    "                data_assembled_fixed[idx_next]=np.array([loc0[0],  # set x\n",
    "                                                         loc0[1],  # set y\n",
    "                                                         1.0])     # set probability\n",
    "                # fix master list as well\n",
    "                data_assembled_all_features[selected_feature,idx_next]=np.array([loc0[0],  # set x\n",
    "                                                         loc0[1],  # set y\n",
    "                                                         1.0])     # set probability\n",
    "                \n",
    "                # metadata printing\n",
    "                if comments:\n",
    "                    print ('data_assembled_fixed[feature][post]: ', \n",
    "                       data_assembled_fixed[idx_next])\n",
    "\n",
    "                    print ('')\n",
    "                    \n",
    "                    \n",
    "                idx_next+=1\n",
    "                n_merges+=1\n",
    "                # exit if at end of data\n",
    "                if idx_next>=1000:\n",
    "                    break\n",
    "\n",
    "                # exit if reached a chunk that is labled above accepted probability:\n",
    "                if data_assembled_fixed[idx_next][2]>threshold_p:\n",
    "                    if comments:\n",
    "                        print (\"***** Point has prob > threshold (moving to next discontious segment) *****\")\n",
    "                        print (\"\")\n",
    "                        print (\"\")\n",
    "                        print (\"\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            else: #move to the next continous segment if distance to nearest time point is too far\n",
    "                if comments:\n",
    "                    print (\"***** JUMPING OT NEXT SEG *****\")\n",
    "                    print (\"\")\n",
    "                    print (\"\")\n",
    "                    print (\"\")\n",
    "                break\n",
    "\n",
    "            #return\n",
    "                \n",
    "    print ('********** # OF MERGES: ', n_merges)\n",
    "\n",
    "    # return fixed data\n",
    "    return data_assembled_fixed, data_assembled_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_01_54_23_358257_compressed/2020-3-16_01_54_23_358257_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_02_54_39_170978_compressed/2020-3-16_02_54_39_170978_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_03_57_56_902379_compressed/2020-3-16_03_57_56_902379_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_03 54 54 231226_compressed/2020-3-16_03 54 54 231226_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_04_55_09_841582_compressed/2020-3-16_04_55_09_841582_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_04_58_11_998956_compressed/2020-3-16_04_58_11_998956_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_05 55 25 305681_compressed/2020-3-16_05 55 25 305681_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_05 58 27 193818_compressed/2020-3-16_05 58 27 193818_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_06 55 40 714236_compressed/2020-3-16_06 55 40 714236_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_06 58 43 678014_compressed/2020-3-16_06 58 43 678014_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_07_55_55_775234_compressed/2020-3-16_07_55_55_775234_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_07_59_00_362242_compressed/2020-3-16_07_59_00_362242_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_08_56_11_096689_compressed/2020-3-16_08_56_11_096689_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_08_59_17_534732_compressed/2020-3-16_08_59_17_534732_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_09_59_34_731308_compressed/2020-3-16_09_59_34_731308_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_09 56 26 362091_compressed/2020-3-16_09 56 26 362091_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_10_59_50_448686_compressed/2020-3-16_10_59_50_448686_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_10 56 41 406701_compressed/2020-3-16_10 56 41 406701_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_11_56_56_704655_compressed/2020-3-16_11_56_56_704655_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_12_54_07_193951_compressed/2020-3-16_12_54_07_193951_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_12 57 12 418305_compressed/2020-3-16_12 57 12 418305_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_02 57 41 995158_compressed/2020-3-16_02 57 41 995158_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle',\n",
    "'/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/madeline/march_2/march_16/2020-3-16_01_57_27_327194_compressed/2020-3-16_01_57_27_327194_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = ['/media/cat/4TBSSD/dan/cohort2/2020_07_19_16_32_55_857727_compressedDLC_resnet50_cohort2Oct21shuffle1_200000_full.pickle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing : /media/cat/4TBSSD/dan/cohort2/2020_07_19_16_32_55_857727_compressedDLC_resnet50_cohort2Oct21shuffle1_200000_full.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "100%|██████████| 28800/28800 [00:04<00:00, 6256.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28800, 11)\n",
      " traces_inferences [n_frames, #_non_unique_featres]:  (28800, 11)\n"
     ]
    }
   ],
   "source": [
    "# LOAD INFERENCE DATA from full_pickle files and convert to  .npy file\n",
    "end = None\n",
    "#end = 1000\n",
    "for full_pickle in fnames:\n",
    "    print (\"processing :\", full_pickle)\n",
    "    # convert full pickel file to simpler data structure\n",
    "    fname_out = full_pickle[:-7]+\"_traces_inferences.npy\"\n",
    "    if os.path.exists(fname_out)==False:\n",
    "        from deeplabcut.pose_estimation_tensorflow.lib.inferenceutils import (\n",
    "                convertdetectiondict2listoflist)\n",
    "        import pickle, re\n",
    "        # load pickle and \n",
    "        with open(full_pickle, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        header = data.pop(\"metadata\")\n",
    "        all_jointnames = header[\"all_joints_names\"]\n",
    "\n",
    "        #if displayedbodyparts == \"all\":\n",
    "        if True:\n",
    "            numjoints = len(all_jointnames)\n",
    "            bpts = range(numjoints)\n",
    "\n",
    "        frame_names = list(data)\n",
    "        frames = [int(re.findall(r\"\\d+\", name)[0]) for name in frame_names]\n",
    "\n",
    "        # Convert inference locations to an easier array to parse\n",
    "\n",
    "        traces_inferences = []\n",
    "        ctr=0\n",
    "        start = 0\n",
    "        if end is None:\n",
    "            end = len(frame_names)\n",
    "\n",
    "        for n in trange(start, end, 1):\n",
    "            #ind = n\n",
    "            #print (n, frame_names[n])\n",
    "            #print (data)\n",
    "            # load inference locations\n",
    "            traces_inferences.append([])\n",
    "            dets = convertdetectiondict2listoflist(data[frame_names[n]], bpts)\n",
    "            for i, det in enumerate(dets):\n",
    "                traces_inferences[ctr].append([])\n",
    "                for x, y, p, _ in det:\n",
    "                    traces_inferences[ctr][i].append([x,y])\n",
    "\n",
    "            ctr+=1\n",
    "        traces_inferences = np.array(traces_inferences)\n",
    "        print (traces_inferences.shape)\n",
    "\n",
    "        np.save(fname_out, traces_inferences)\n",
    "\n",
    "    else:\n",
    "        traces_inferences = np.load(fname_out, allow_pickle=True)\n",
    "    print (\" traces_inferences [n_frames, #_non_unique_featres]: \", traces_inferences.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " traces_inferences [n_frames, #_non_unique_featres]:  (28800, 11)\n"
     ]
    }
   ],
   "source": [
    "fname_out = '/media/cat/4TBSSD/dan/cohort2/2020_07_19_16_32_55_857727_compressedDLC_resnet50_cohort2Oct21shuffle1_200000_full_traces_inferences.npy'\n",
    "traces_inferences = np.load(fname_out, allow_pickle=True)\n",
    "print (\" traces_inferences [n_frames, #_non_unique_featres]: \", traces_inferences.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# #########################################################\n",
    "# ############## CONNECTED COMPONENTS SOLUTION ############\n",
    "# #########################################################\n",
    "# #########################################################\n",
    "\n",
    "#for fname in fnames:\n",
    "def connected_components_function(fname_in,\n",
    "                                 ctr_comments,\n",
    "                                 network_history,\n",
    "                                 min_network,\n",
    "                                 n_networks,\n",
    "                                 start,\n",
    "                                 end):\n",
    "    \n",
    "    fname_in = fname_in[:-7]+\"_traces_inferences.npy\"\n",
    "    traces_inferences = np.load(fname_in, allow_pickle=True)\n",
    "    print (\" traces_inferences [n_frames, #_non_unique_featres]: \", traces_inferences.shape)\n",
    "    print (fname_in)\n",
    "    \n",
    "    if end is None:\n",
    "        end = traces_inferences.shape[0]\n",
    "        \n",
    "    # SELECT OUTPUT FILE\n",
    "    fname_out = fname_in[:-4]+'_'+str(start)+'_'+str(end)+'.npz'\n",
    "\n",
    "    #[start:end,feature]\n",
    "    labels = []\n",
    "\n",
    "    # ##########################################################\n",
    "    # ################### PARAMETER LISTS ######################\n",
    "    # ##########################################################\n",
    "    #min_network = 5  # min length of segs to connect\n",
    "    #n_networks = 4 # essentially Number of animals that are matched to a graph\n",
    "    #network_history = 30\n",
    "\n",
    "    max_dist_between_features = 125 # connected componennts max distances allowed between sequential features\n",
    "    min_dist_chunks = 200 # min distance between matching history\n",
    "    \n",
    "    # THIS IS TRICKY: if block labeled, tough to jump \n",
    "    max_network_jump = 400 # maximum amount an animal id (centre) can move in a single frame \n",
    "    \n",
    "    \n",
    "    max_dist_merge_networks_cc = 75 # post-cc step to fix oversplits; searches cnetres of two networks\n",
    "                                     # with non-overlapping features and merges them\n",
    "\n",
    "    # otehr params\n",
    "    min_n_matches_history_override = 4 # this parameter indicates if x feature-wise\n",
    "                                       # very close matches in previous history\n",
    "                                       # will override the centres-based history tracker\n",
    "\n",
    "    # ##########################################################\n",
    "    # ################## FRAME LOOP START#######################\n",
    "    # ##########################################################\n",
    "    comments = False\n",
    "\n",
    "    labels_array = []\n",
    "    features_array = []\n",
    "    ctr=0\n",
    "    historical_centres = np.zeros((network_history,n_networks,2))\n",
    "    # loop over frames\n",
    "    #ctr_comments =[]\n",
    "    #for k in tqdm(range(start, end)):\n",
    "    for k in tqdm(range(start, end)):\n",
    "\n",
    "        if ctr in ctr_comments:\n",
    "            comments=True\n",
    "        else:\n",
    "            comments=False\n",
    "\n",
    "        if False:\n",
    "            if k%1000==0:\n",
    "                print (\"FRAME: \", k)\n",
    "\n",
    "        # make label array\n",
    "        labels_array.append([])\n",
    "        features_array.append([])\n",
    "\n",
    "        # LOAD DATA FROM INFERENCE STEP\n",
    "        flat_list = []\n",
    "        list_idx = []\n",
    "        for n, sublist in enumerate(traces_inferences[k]):\n",
    "            list_idx.extend(np.zeros(len(sublist),'int32')+n)\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "\n",
    "        #print (flat_list)\n",
    "        locs = np.vstack(flat_list).copy()\n",
    "        list_idx = np.array(list_idx)\n",
    "\n",
    "\n",
    "        # ####################################################################################\n",
    "        # ####################################################################################\n",
    "        # ########################### CONNECTED COMPONENTS ###################################\n",
    "        # ####################################################################################\n",
    "        # ####################################################################################\n",
    "        # loop over features and build graphs:\n",
    "        # find nearest next feature to the selected feature and connect them:\n",
    "        cc = []\n",
    "        cc2 = np.zeros((list_idx.shape[0],list_idx.shape[0]),'int32')\n",
    "        # loop over each group of features in the dataset \n",
    "        for feature in range(np.unique(list_idx).shape[0]-1):\n",
    "            idx = np.where(list_idx==feature)[0]\n",
    "            locs_feat_current = locs[idx]\n",
    "\n",
    "            # grab all features in the next step of hierarchy and mathc to previous hierarchy features;\n",
    "            for qq in range(feature+1,feature+2):\n",
    "                idx_next = np.where(list_idx==qq)[0]\n",
    "                locs_next = locs[idx_next]\n",
    "\n",
    "                if locs_next.shape[0]==0:\n",
    "                    continue\n",
    "\n",
    "                # Compute shortest distance between the current features and the next step features\n",
    "                dists = []\n",
    "                argmins = []\n",
    "                for p in range(locs_feat_current.shape[0]):\n",
    "                    loc_ = locs_feat_current[p]\n",
    "\n",
    "                    vect = locs_next-loc_\n",
    "                    min_dist = np.min(scipy.spatial.distance.cdist(vect*0, vect))\n",
    "                    argmin = np.argmin(scipy.spatial.distance.cdist(vect*0, vect))\n",
    "                    dists.append(min_dist)\n",
    "                    argmins.append(argmin)\n",
    "\n",
    "                # ranked by distance\n",
    "                dists = np.array(dists)\n",
    "                argmins = np.array(argmins)\n",
    "                idx_sort = np.argsort(dists)\n",
    "\n",
    "                for p in range(idx_sort.shape[0]):\n",
    "                    # load sorted data\n",
    "                    min_dist = dists[idx_sort[p]]\n",
    "                    argmin = argmins[idx_sort[p]]\n",
    "\n",
    "                    if min_dist<max_dist_between_features:\n",
    "                        # check to ensure the same type of feature doesn't already exist in the dataset:\n",
    "                        # actually need to check if any of the other parts are connected, not just the previous most recent part?! \n",
    "                        # idx here is all the locations of current hierarchy features\n",
    "                        if np.any(cc2[idx,idx_next[argmin]]):\n",
    "                            if comments: \n",
    "                                print(\"feature already connected to other parts; skipping\")\n",
    "                            continue\n",
    "                        #cc.append([idx[p], idx_next[argmin]])\n",
    "                        cc2[idx[idx_sort[p]], idx_next[argmin]]=1\n",
    "\n",
    "\n",
    "        graph=np.array(cc2)\n",
    "        graph = csr_matrix(graph)\n",
    "\n",
    "\n",
    "        n_components, labels_all = connected_components(csgraph=graph, directed=False, return_labels=True)\n",
    "        #print (n_components)\n",
    "\n",
    "        # find all unique network ids; labels_unique[0] - ids;  labels_unique[1] is counts\n",
    "        labels_unique = np.unique(labels_all, return_counts=True)\n",
    "\n",
    "        # keep only networks over min_network size;\n",
    "        idx_nets = np.where(labels_unique[1]>=min_network)[0]\n",
    "        labels_kept = labels_unique[0][idx_nets]\n",
    "\n",
    "        # order the networks by size and keep only to n_networks\n",
    "        idx_ordered = np.argsort(labels_unique[1][idx_nets])[::-1]\n",
    "        labels_kept = labels_kept[idx_ordered][:n_networks]\n",
    "        if comments:\n",
    "            print (\"idx_networks kept:\", idx_nets)\n",
    "            print (\"labels kept: \", labels_kept)\n",
    "            print (\" size of networks kept: \", labels_unique[1][idx_nets][idx_ordered][:n_networks])\n",
    "\n",
    "            for p in np.unique(labels_kept):\n",
    "                idx = np.where(labels_all==p)[0]\n",
    "                locs_network = locs[idx]\n",
    "                print (\"network :\", p, locs_network)\n",
    "                print ('size of network ', locs_network.shape)\n",
    "                print (\" feature types; \", list_idx[idx])\n",
    "            print (\"\")\n",
    "\n",
    "        # final cc check for oversplit clusters:\n",
    "        # note: This should be done in the CC matrix step, but can't seem to get it to work in full\n",
    "        labels_kept_temp = labels_kept.copy()\n",
    "        for p in np.unique(labels_kept):\n",
    "            idx1 = np.where(labels_all==p)[0]\n",
    "            features1 = list_idx[idx1]\n",
    "            locs_network1 = locs[idx1]\n",
    "\n",
    "            if idx1.shape[0]==0:\n",
    "                continue\n",
    "\n",
    "            for pp in np.unique(labels_kept):\n",
    "                if pp==p:\n",
    "                    continue\n",
    "                idx2 = np.where(labels_all==pp)[0]\n",
    "                if idx2.shape[0]==0:\n",
    "                    continue\n",
    "\n",
    "                features2 = list_idx[idx2]\n",
    "                locs_network2 = locs[idx2]\n",
    "\n",
    "                # check if nonoveralpping features\n",
    "                if np.any(np.isin(features1,features2))==False:\n",
    "\n",
    "                    # chekc if very close in distance\n",
    "    #                 vect = labels_array[ctr-1][z]-locs_feature\n",
    "    #                 min_dist = np.min(scipy.spatial.distance.cdist(vect*0, vect))\n",
    "                    dists = scipy.spatial.distance.cdist(locs_network1, locs_network2)\n",
    "                    if comments:\n",
    "                        print (\"dists between nonvoeralpping networks: \", dists)\n",
    "\n",
    "                    # merge bits\n",
    "                    if np.min(dists)<max_dist_merge_networks_cc:\n",
    "\n",
    "                        # change labels_all and labels_kept\n",
    "                        labels_all[idx2] = p\n",
    "                        idx_del = np.where(labels_kept_temp==pp)[0]\n",
    "                        labels_kept_temp = np.delete(labels_kept_temp, idx_del)\n",
    "\n",
    "                        if comments:\n",
    "                            print (\"merged networks: \", p, \" and \", pp, \" at idx: \", idx_del)\n",
    "\n",
    "\n",
    "        labels_kept = labels_kept_temp\n",
    "        #print (\"LABELS post cc: \", labels)\n",
    "\n",
    "        # ####################################################################################\n",
    "        # ####################################################################################\n",
    "        # ########################### FRAME LOOP ANALYSIS ####################################\n",
    "        # ####################################################################################\n",
    "        # ####################################################################################\n",
    "        # save previous centres to match them and overwrite below\n",
    "        # loop over all network components\n",
    "        if ctr>0:\n",
    "            # make large list to hold ftuure labels\n",
    "    #        features_list = [\n",
    "            for p in range(n_networks):\n",
    "                labels_array[ctr].append([])\n",
    "                features_array[ctr].append([])\n",
    "\n",
    "            # keep track of the matches with previous chunks\n",
    "            previous_centres = historical_centres.copy() #[0,p]=np.mean(locs_network,0))\n",
    "\n",
    "            if comments:\n",
    "                print (\"previous centres: \", previous_centres.shape)\n",
    "            #break\n",
    "\n",
    "            idx_matches = []\n",
    "            idx_matches_p = []\n",
    "\n",
    "\n",
    "            # ####################################################################################\n",
    "            # ########################### COMPUTE DISTANCES TO PREVIOUS FRAME ####################\n",
    "            # ####################################################################################\n",
    "\n",
    "            # search for current network chunk for nearest previous chunk\n",
    "            # count the number of matches between current network centre and history:\n",
    "            n_matches_history = np.zeros((labels_kept.shape[0], previous_centres.shape[1]),'int32')\n",
    "            total_dist = np.zeros((labels_kept.shape[0],previous_centres.shape[1]),'float32') \n",
    "\n",
    "            # new method checks averages\n",
    "            # and also whether min_network closest matches to the previous immediate vals;\n",
    "            # If option 2 is very close it overrides all other matches. \n",
    "            for ctr_p, p in enumerate(np.unique(labels_kept)):\n",
    "                idx = np.where(labels_all==p)[0]\n",
    "                locs_network = locs[idx]\n",
    "\n",
    "                # compute cntre:\n",
    "                centre_network = np.median(locs_network,0)\n",
    "                if comments:\n",
    "                    print (\"centre network\", p, centre_network)\n",
    "                    print (\" doing feature-wise heuristic search at prev step: \")\n",
    "                    #print (\"      Previous time step data\", labels_array[ctr-1])\n",
    "                    #print (\"      current network: \", locs_network)\n",
    "                # also quick check if the previous steps had values very close to current \n",
    "                # loop over current network values and find shortest 5 distances:\n",
    "                min_dist_count = np.zeros(n_networks, 'int32')\n",
    "                for zz in range(locs_network.shape[0]):\n",
    "                    locs_feature = locs_network[zz]\n",
    "                    if comments:\n",
    "                        print (\"search for close feature to \", locs_feature)\n",
    "                        #if comments:\n",
    "\n",
    "                    #min_dist_count = 0\n",
    "                    for z in range(len(labels_array[ctr-1])):\n",
    "                        temp_temp = labels_array[ctr-1][z]\n",
    "                        if len(labels_array[ctr-1][z])==0:\n",
    "                            continue\n",
    "                        vect = labels_array[ctr-1][z]-locs_feature\n",
    "                        min_dist = np.min(scipy.spatial.distance.cdist(vect*0, vect))\n",
    "                        #print (\"min dist: \", min_dist)\n",
    "                        if min_dist < min_n_matches_history_override:\n",
    "                            min_dist_count[z]+=1\n",
    "\n",
    "                # if sufficient matches no need to compute and check centres\n",
    "                max_nearest = np.max(min_dist_count)\n",
    "                if max_nearest > 5:\n",
    "                    argmax_overwrite = np.argmax(min_dist_count)\n",
    "                    #n_matches_history[:,argmax_overwrite]-=100  # ensure the previous neetwork can only be matched to the current one\n",
    "                                                      # by starting/biasing the match count for the rest of the matches;\n",
    "                                                      # can also do this other ways\n",
    "                    n_matches_history[ctr_p,argmax_overwrite]+=1E5\n",
    "                    total_dist[ctr_p,argmax_overwrite]=0 #previous_c\n",
    "\n",
    "                    if comments:\n",
    "                        print (\"nearly similar network at previous time step; \")\n",
    "                        print (\" Min dist array: \", min_dist_count)\n",
    "                        print (\"         matching to prev network using argmax: \", argmax_overwrite)\n",
    "                        print (\"         current locs \", locs_network)\n",
    "                        print (\"         previous matched network: \", labels_array[ctr-1][argmax_overwrite])\n",
    "\n",
    "                else:\n",
    "                    # loop over each previous network\n",
    "                    for z in range(previous_centres.shape[1]):\n",
    "\n",
    "                        # loop over each previous history point: find how many historical matches it has\n",
    "                        for h in range(previous_centres[:,z].shape[0]):\n",
    "\n",
    "                            # May wish to implement an L1 not L2 distance \n",
    "                            # OR an individual feature distance rather than mean\n",
    "                            # OR find median xy point and use that as centre;\n",
    "                            #if True:\n",
    "                            vect = previous_centres[h,z]-centre_network\n",
    "                            min_dist = np.min(np.linalg.norm(vect))\n",
    "                            #print (\"VECT: \", vect, \" Min dist: \", min_dist)\n",
    "                            #min_dist = np.min(scipy.spatial.distance.cdist(vect*0, vect))\n",
    "                            if min_dist<min_dist_chunks:\n",
    "                                n_matches_history[ctr_p][z]+=1\n",
    "                                total_dist[ctr_p][z]+=min_dist #previous_centres[:,z].shape[0]-h)/previous_centres[:,z].shape[0])\n",
    "                                #total_dist[z]+=min_dist*((previous_centres[:,z].shape[0]-h)/previous_centres[:,z].shape[0])\n",
    "\n",
    "                                if comments:\n",
    "                                    print (z,h,\"adding dist: \", min_dist)\n",
    "\n",
    "            # check best matches from history\n",
    "            for ctr_p, p in enumerate(np.unique(labels_kept)):\n",
    "                idx = np.where(labels_all==p)[0]\n",
    "                locs_network = locs[idx]\n",
    "\n",
    "                if comments:\n",
    "                    print (\"\")\n",
    "                    print (\"animal:\", ctr_p, \"n_mathces_history:\", n_matches_history[ctr_p])\n",
    "                    print (\"locs: \", locs_network)\n",
    "\n",
    "                # Loop to ensure no better match found for a previous animal ID        \n",
    "                while True:\n",
    "                    argmax = np.argmax(n_matches_history[ctr_p])\n",
    "                    if comments:\n",
    "                        print (\"curretn best match for current locs network: \", locs_network)\n",
    "                        print (\"   is prev network: \", labels_array[ctr-1][argmax])\n",
    "\n",
    "                    if n_matches_history[ctr_p,argmax]==0:\n",
    "                        break\n",
    "\n",
    "                    # ensure no boetter match was found for the current val\n",
    "                    n_matches_all_column = n_matches_history[:,argmax]\n",
    "                    if np.max(n_matches_all_column)>n_matches_history[ctr_p,argmax]:\n",
    "\n",
    "                        argmax_col = np.argmax(n_matches_all_column)\n",
    "\n",
    "                        # check also distances for better fit:\n",
    "                        current_dist = total_dist[ctr_p][argmax]/n_matches_history[ctr_p][argmax]\n",
    "                        alternative_match_dist = total_dist[argmax_col][argmax]/n_matches_history[argmax_col][argmax]\n",
    "\n",
    "                        if comments:\n",
    "                            print (\" current dist: \", current_dist)\n",
    "                            print (\" alternative dist: \", alternative_match_dist)\n",
    "                        \n",
    "                        #if False:\n",
    "                        if alternative_match_dist< current_dist:\n",
    "\n",
    "                            n_matches_history[ctr_p, argmax]=0\n",
    "                            if comments: \n",
    "                                print (\" FOUND BETTER MACH WITH ANOTHER ANIMAL, setting argmax to zoer\")\n",
    "                        else:\n",
    "                            break\n",
    "                        #else:\n",
    "                        #    break\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "\n",
    "                # check case when more than 1 animal within merging distance and pick closest\n",
    "                max_count = n_matches_history[ctr_p][argmax]\n",
    "                if comments:\n",
    "                    print (\" argmax in match history: \", argmax)\n",
    "                    print ('               max count: ', max_count)\n",
    "\n",
    "                # check if more than one match for new network\n",
    "                # may wish to take all matches > 15 or max\n",
    "\n",
    "                idx11 = np.where(n_matches_history[ctr_p]==max_count)[0]\n",
    "                if (idx11.shape[0]>1) and max_count>0:\n",
    "\n",
    "                    # find what the averate distance between each matched history and the new frame are and \n",
    "                    # take the shortest average distance\n",
    "                    # check all the matches that had same amount as idx11\n",
    "                    match_means = total_dist[ctr_p][idx11]/n_matches_history[ctr_p][idx11]\n",
    "                    argmin_match = np.argmin(match_means)\n",
    "                    argmax = idx11[argmin_match]\n",
    "                    min_dist_found = match_means[argmin_match]\n",
    "\n",
    "                    if comments:\n",
    "                        print (\"DUPLICATE MATCHES: so looking at distances: \", total_dist[ctr_p][idx11])\n",
    "                        print (\"Original min distance found: \", min_dist_found)\n",
    "                        print (\"Average distancesL\", match_means, \" of argmin: \", argmin_match)\n",
    "                        print (\"total dists: \", total_dist[ctr_p])\n",
    "\n",
    "                    # check if matched network doesn't actually have even closer match:\n",
    "                    # so search in the column of the original best match to see if it has a better match\n",
    "                    idx13 = np.where(n_matches_history[:,argmax]==max_count)[0]\n",
    "                    match_means_network_match = total_dist[:,argmax][idx13]/n_matches_history[:,argmax][idx13]\n",
    "                    argmin_match_near = np.argmin(match_means_network_match)\n",
    "                    if match_means_network_match[argmin_match_near]<min_dist_found:\n",
    "\n",
    "                        # set the distance for the previous best match to very large value\n",
    "                        total_dist[ctr_p][argmax] = 1E10\n",
    "\n",
    "                        # recompute the argmax    \n",
    "                        match_means = total_dist[ctr_p][idx11]/n_matches_history[ctr_p][idx11]\n",
    "                        argmin_match = np.argmin(match_means)\n",
    "                        argmax = idx11[argmin_match]\n",
    "                        min_dist_found = match_means[argmin_match]\n",
    "\n",
    "                        if comments:\n",
    "                            print (\"BETTER MATCH WAS FOUND\", argmin_match)\n",
    "                            print (\"new min distance found: \", min_dist_found)\n",
    "\n",
    "\n",
    "                        #else:\n",
    "                        #    break\n",
    "                    else:\n",
    "                        if comments:\n",
    "                            print (\"NO BETTER MATCH FOUND\")\n",
    "\n",
    "                # check also to see if networked matched on centre-based distance doesn't actually ahve a better match via\n",
    "                #   feature-based matching with another dataset;\n",
    "                n_matches_all_column = n_matches_history[:,argmax]\n",
    "                if np.max(n_matches_all_column)>n_matches_history[ctr_p,argmax]:\n",
    "                    if comments:\n",
    "                        print (\"  Matches in columN: \", n_matches_all_column)\n",
    "                        print (\"     ARE GREATER THAN BEST Centre based match: \", n_matches_history)\n",
    "                        print (\"  ***************************************\")\n",
    "\n",
    "                # ############################################################ \n",
    "                # ######################## SAVE MATCH DATA ################### \n",
    "                # ############################################################ \n",
    "                if n_matches_history[ctr_p][argmax]>0:\n",
    "                    # replace locations of previous match \n",
    "                    idx_matches.append(argmax)\n",
    "                    idx_matches_p.append(p)\n",
    "                    if comments:\n",
    "                        print (\"Matched to previous centre: \", argmax)\n",
    "                        print (\"final argmax: \", argmax)\n",
    "                        print (\"locs current network:\", locs_network)\n",
    "                        print (\"to be inserted near previous network: \", labels_array[ctr-1][argmax])\n",
    "\n",
    "                # add it as a new tracklet\n",
    "                else:\n",
    "                    idx_matches.append(None)\n",
    "                    idx_matches_p.append(p)\n",
    "\n",
    "                # zero out the better match in this column\n",
    "                total_dist[:,argmax]=1E10\n",
    "                n_matches_history[:,argmax] = 0\n",
    "\n",
    "            # ############################################################\n",
    "            # ############################################################\n",
    "            # ############### INSERT THE MATCHED DATA ####################\n",
    "            # ############################################################\n",
    "            # ############################################################\n",
    "            if comments:\n",
    "                print(\"\")\n",
    "                print(\"FINAL NETWORK INSERTION STEP\")\n",
    "\n",
    "            # loop over the matched ids\n",
    "            for p in range(len(idx_matches)):\n",
    "                idx = np.where(labels_all==idx_matches_p[p])[0]\n",
    "                locs_network = locs[idx]\n",
    "                feature_types = list_idx[idx]\n",
    "                if comments:\n",
    "                    print (\"idx_matches[p]:\", idx_matches[p])\n",
    "\n",
    "                if idx_matches[p] is not None:\n",
    "\n",
    "                    # do a final check to make sure neighbouring timestep networks aren't super-far apart\n",
    "                    centre_prev_network = np.median(labels_array[ctr-1][idx_matches[p]],0)\n",
    "                    centre_current_network = np.median(locs_network,0)\n",
    "                    vect = centre_prev_network-centre_current_network\n",
    "                    dist_centre = np.linalg.norm(vect)\n",
    "                    if comments:\n",
    "                        print (\"   FINAL CHECK DISTANCE BETWEEN PREVIOUS AND CURRENT CENTRE\",\n",
    "                              \"  vect\" , vect,\n",
    "                              \"  dist: \", dist_centre)\n",
    "                    if dist_centre>=max_network_jump:\n",
    "                        if comments:\n",
    "                            print (\" SKIPPED NETWORK DUE TO LARGE JUMP!\")\n",
    "                    else:\n",
    "                        labels_array[ctr][idx_matches[p]]=locs_network\n",
    "                        features_array[ctr][idx_matches[p]]=feature_types\n",
    "\n",
    "                        if comments:\n",
    "                            print (\"\")\n",
    "                            print (\"inserted: \", locs_network)\n",
    "                            print (\" at location: \", idx_matches[p])\n",
    "                            print (\"ctr-1 values at this location: \",idx_matches[p],\n",
    "                                   \" has values: \",\n",
    "                                   labels_array[ctr-1][idx_matches[p]])\n",
    "                else:\n",
    "                    if comments:\n",
    "                        print (\"network doesn't have match: \", locs_network)\n",
    "\n",
    "            # ############################################################\n",
    "            # ############################################################\n",
    "            # ############### REVIEW UNMATCHED DATA ####################\n",
    "            # ############################################################\n",
    "            # ############################################################           \n",
    "            ctr_inner=0\n",
    "            for p in range(len(idx_matches)):\n",
    "                if idx_matches[p] is None:\n",
    "                    idx = np.where(labels_all==idx_matches_p[p])[0]\n",
    "                    feature_types = list_idx[idx]\n",
    "                    locs_network = locs[idx]\n",
    "                    if comments:\n",
    "                        print (\"Inserting missed network to end:\", locs_network)\n",
    "                    # search for an empty list to populate\n",
    "                    while True:\n",
    "                        # find the first empty location to insert data\n",
    "                        if len(labels_array[ctr][ctr_inner])==0:\n",
    "\n",
    "                            # do a final check to make sure neighbouring timestep networks aren't super-far apart\n",
    "                            if len(labels_array[ctr-1][ctr_inner])==0:\n",
    "                                centre_prev_network=np.array((1E10,1E10))\n",
    "                            else:\n",
    "                                centre_prev_network = np.median(labels_array[ctr-1][ctr_inner],0)\n",
    "                            \n",
    "                            vect = centre_prev_network-centre_current_network\n",
    "                            dist_centre = np.linalg.norm(vect)\n",
    "                            if comments:\n",
    "                                print (\"   FINAL CHECK DISTANCE BETWEEN PREVIOUS AND CURRENT CENTRE\",\n",
    "                                      \"  vect\" , vect,\n",
    "                                      \"  dist: \", dist_centre)\n",
    "                            if dist_centre>=max_network_jump:\n",
    "                                if comments:\n",
    "                                    print (\" SKIPPED NETWORK DUE TO LARGE JUMP!\")\n",
    "\n",
    "                                # exit and skip this segment altogether\n",
    "                                break\n",
    "\n",
    "                            else:\n",
    "                                labels_array[ctr][ctr_inner]=locs_network\n",
    "                                features_array[ctr][ctr_inner]=feature_types\n",
    "                                ctr_inner+=1\n",
    "                                break\n",
    "                        ctr_inner+=1\n",
    "\n",
    "            # add centres to \n",
    "\n",
    "            historical_centres[1:]=historical_centres[:-1] #[0,p]=np.mean(locs_network,0))\n",
    "            for pp in range(len(labels_array[ctr])):\n",
    "                if len(labels_array[ctr][pp])==0:\n",
    "                    continue\n",
    "                locs_new = labels_array[ctr][pp]\n",
    "                centres_new = np.median(locs_new,0)\n",
    "                if comments:\n",
    "                    print (\"centres new\", pp, centres_new)\n",
    "                #historical_centres[0,pp]=centres_new\n",
    "                historical_centres[0,pp]=centres_new\n",
    "\n",
    "\n",
    "            if comments:\n",
    "                print (k, \" FINAL labels array: \", labels_array[ctr])\n",
    "\n",
    "        # for first time point\n",
    "        else:\n",
    "            for k in range(n_networks):\n",
    "                labels_array[ctr].append([])\n",
    "                features_array[ctr].append([])\n",
    "\n",
    "            for n, p in enumerate(np.unique(labels_kept)):\n",
    "                idx = np.where(labels_all==p)[0]\n",
    "                locs_network = locs[idx]\n",
    "                feature_types = list_idx[idx]\n",
    "\n",
    "                #print (\"locs network\", locs_network)\n",
    "                labels_array[ctr][n]=locs_network\n",
    "                features_array[ctr][n]=feature_types\n",
    "\n",
    "                # fill the entire history with starting locations:\n",
    "                historical_centres[:,n]=np.median(locs_network,0)\n",
    "\n",
    "            if comments:\n",
    "                print (\"FIRST saved labels array: \", labels_array)\n",
    "    #             break\n",
    "    #         break\n",
    "    #     break\n",
    "\n",
    "        ctr+=1   \n",
    "        #print (\"\")\n",
    "        #print (\"\")\n",
    "\n",
    "    # convert the output to a rectangular matrix with correct locations for data;\n",
    "    final_features = np.zeros((n_networks*14, len(labels_array),2), 'float32')\n",
    "    print (\"Final eatures; \", final_features.shape)\n",
    "    for k in range(len(labels_array)):\n",
    "        if k%1000==0:\n",
    "            print (k)\n",
    "        for p in range(len(labels_array[k])):\n",
    "            #for l in range(len(labels_array[k][p])):\n",
    "                #print (data[k][p][l])\n",
    "            label_locs = features_array[k][p]\n",
    "            if len(label_locs)>0:\n",
    "                #print (k, p, label_locs)\n",
    "                final_features[label_locs+p*14,k]=labels_array[k][p]\n",
    "\n",
    "    tracesx = final_features[:,:,0]\n",
    "    tracesy = final_features[:,:,1]\n",
    "    probs = np.ones(tracesy.shape, 'float32')\n",
    "    print (\"SAVING:to file: \", fname_out)\n",
    "    np.savez(fname_out,\n",
    "            tracesx=tracesx,\n",
    "            tracesy=tracesy,\n",
    "            probs =probs\n",
    "            )    \n",
    "    \n",
    "    return fname_out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make inference traces arrays from Anqi's data\n",
    "# fname = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/markers/markers'\n",
    "\n",
    "# inferences = []\n",
    "# for k in range(14):\n",
    "#     fname_temp = fname + str(k)+'.npy'\n",
    "#     temp = np.load(fname_temp)\n",
    "#     inferences.append(temp)\n",
    "# inferences=np.array(inferences).transpose(1,0,2,3)\n",
    "# print (inferences.shape)\n",
    "\n",
    "# np.save('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/markers/markers_inferences_traces.npy', inferences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [00:00<00:16, 59.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " traces_inferences [n_frames, #_non_unique_featres]:  (28800, 11)\n",
      "/media/cat/4TBSSD/dan/cohort2/2020_07_19_16_32_55_857727_compressedDLC_resnet50_cohort2Oct21shuffle1_200000_full_traces_inferences.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1000 [00:00<00:17, 57.75it/s]/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 1000/1000 [00:17<00:00, 55.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eatures;  (98, 1000, 2)\n",
      "0\n",
      "SAVING:to file:  /media/cat/4TBSSD/dan/cohort2/2020_07_19_16_32_55_857727_compressedDLC_resnet50_cohort2Oct21shuffle1_200000_full_traces_inferences_0_1000.npz\n"
     ]
    }
   ],
   "source": [
    "#fname = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/2020-3-16_12_54_07_193951_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full.pickle'\n",
    "fname = '/media/cat/4TBSSD/dan/cohort2/2020_07_19_16_32_55_857727_compressedDLC_resnet50_cohort2Oct21shuffle1_200000_full.pickle'\n",
    "# fname_in = fname_in[:-7]+\"_traces_inferences.npy\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "ctr_comments = []\n",
    "network_history = 5\n",
    "min_network= 4\n",
    "n_networks = 7\n",
    "start = 0\n",
    "end = 1000\n",
    "fname_out = connected_components_function(fname,\n",
    "                                          ctr_comments,\n",
    "                                          network_history,\n",
    "                                          min_network,\n",
    "                                          n_networks,\n",
    "                                          start,\n",
    "                                          end\n",
    "                                         )\n",
    "# PARALLEL VERSION OF CC OVER FILES (NOT FRAMES)\n",
    "#fnames = ['/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/2020-3-16_12_54_07_193951_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full_1000frames.pickle']#\n",
    "\n",
    "#if True:\n",
    "#    import parmap\n",
    "#    parmap.map(connected_components_function, fnames,\n",
    "#              pm_processes=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels array:  (100, 70, 2)\n",
      "0\n",
      "[378.959 355.665 359.257 326.89  332.002 317.228 293.326 268.85  257.51\n",
      "   0.    245.954 237.661 226.822 226.52 ]\n",
      "14\n",
      "[1027.058 1009.102 1019.859 1010.843 1044.049 1029.055 1025.898 1025.063\n",
      " 1019.651 1009.94  1007.614 1003.3   1000.934  998.27 ]\n",
      "28\n",
      "[   0.       0.    1164.489 1180.25  1156.742 1161.645 1147.505 1134.365\n",
      " 1121.112 1112.532 1099.776 1093.513    0.       0.   ]\n",
      "42\n",
      "[   0.       0.       0.    1142.271 1170.475 1147.864 1140.955 1138.411\n",
      " 1125.099 1100.705    0.       0.       0.       0.   ]\n",
      "56\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0\n",
      "[379.025 356.079 362.445 327.023 332.006 317.167 293.349 268.727 258.498\n",
      "   0.      0.      0.      0.      0.   ]\n",
      "14\n",
      "[ 979.877  988.323 1020.102 1001.03  1035.045 1018.207 1019.509 1021.541\n",
      " 1019.806 1005.538 1001.532  994.607  986.272  982.961]\n",
      "28\n",
      "[   0.       0.    1163.764 1180.338 1156.94  1162.136 1146.138 1132.008\n",
      " 1116.756 1107.67  1099.636 1093.288 1092.592    0.   ]\n",
      "42\n",
      "[   0.       0.       0.    1140.62  1162.867 1148.22  1141.273 1139.505\n",
      " 1123.256 1099.27     0.       0.       0.       0.   ]\n",
      "56\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0\n",
      "[379.048 356.096 362.511 327.122 332.01  317.167 293.338 268.751 258.474\n",
      "   0.      0.      0.      0.      0.   ]\n",
      "14\n",
      "[   0.       0.       0.     972.007 1011.895  995.423 1002.104 1010.091\n",
      " 1012.166 1004.925 1003.057  997.218  987.59   989.115]\n",
      "28\n",
      "[   0.       0.       0.    1172.278 1156.918 1161.524 1146.421 1132.854\n",
      " 1116.394 1106.382 1099.912 1093.696 1092.684    0.   ]\n",
      "42\n",
      "[   0.       0.       0.    1140.478 1163.86  1152.705 1142.111 1140.104\n",
      " 1123.035 1098.847    0.       0.       0.       0.   ]\n",
      "56\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0\n",
      "[379.145 355.853 359.107 326.748 331.671 317.022 293.173 268.881 258.178\n",
      "   0.    257.113 237.696 226.38  226.168]\n",
      "14\n",
      "[   0.       0.       0.     924.183  971.844  963.433  971.162  993.218\n",
      "  997.874 1003.33  1002.543 1008.411 1001.846 1004.901]\n",
      "28\n",
      "[   0.       0.       0.    1180.524 1157.442 1160.955 1146.165 1132.319\n",
      " 1116.564 1107.021 1099.266 1093.106 1092.383    0.   ]\n",
      "42\n",
      "[   0.       0.       0.    1140.041 1168.924 1148.345 1147.09  1140.018\n",
      " 1123.103 1095.389    0.       0.       0.       0.   ]\n",
      "56\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0\n",
      "[379.085 356.311 359.074 326.899 331.714 317.156 293.29  268.68  257.895\n",
      "   0.    257.185 237.415 225.968 225.196]\n",
      "14\n",
      "[   0.       0.       0.       0.       0.       0.       0.       0.\n",
      "  982.065 1000.353 1002.808 1014.411 1017.088 1021.416]\n",
      "28\n",
      "[   0.       0.    1148.419 1180.881 1157.683 1161.141 1146.108 1132.426\n",
      " 1116.605 1106.784 1095.672 1092.799 1092.128    0.   ]\n",
      "42\n",
      "[   0.       0.       0.    1141.351 1171.437 1148.942 1147.899 1146.607\n",
      " 1123.301 1094.948    0.       0.       0.       0.   ]\n",
      "56\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0\n",
      "[379.02  355.889 359.346 326.626 331.688 316.976 293.32  268.716 257.449\n",
      "   0.      0.      0.      0.      0.   ]\n",
      "14\n",
      "[   0.       0.       0.       0.       0.       0.     948.125  957.408\n",
      "  965.189  977.44   986.278  999.204 1014.375 1026.829]\n",
      "28\n",
      "[   0.       0.    1156.13  1180.356 1157.241 1161.125 1146.329 1132.335\n",
      " 1116.657 1106.255 1095.068 1092.735 1092.021    0.   ]\n",
      "42\n",
      "[   0.       0.       0.    1140.894 1169.874 1148.588 1147.203 1140.424\n",
      " 1123.558 1108.212    0.       0.       0.       0.   ]\n",
      "56\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0\n",
      "[379.281 355.606 359.418 326.572 331.702 316.988 294.019 269.174 257.44\n",
      "   0.      0.      0.      0.      0.   ]\n",
      "14\n",
      "[   0.       0.       0.       0.       0.       0.       0.     932.274\n",
      "  947.357  960.325  968.199  982.745  995.497 1005.971]\n",
      "28\n",
      "[   0.       0.    1156.341 1174.549 1158.221 1160.96  1146.521 1132.825\n",
      " 1116.803 1104.796 1095.024 1092.703 1091.982    0.   ]\n",
      "42\n",
      "[   0.       0.       0.    1141.363 1168.859 1148.772 1141.501 1139.917\n",
      " 1123.551 1108.475    0.       0.       0.       0.   ]\n",
      "56\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0\n",
      "[379.294 355.59  359.32  326.284 331.728 317.006 294.034 269.154 257.151\n",
      " 260.321 245.999 237.383 227.088 227.023]\n",
      "14\n",
      "[  0.      0.      0.      0.      0.      0.    923.709 931.478 940.43\n",
      " 957.491 960.552 969.084 982.386 993.686]\n",
      "28\n",
      "[   0.       0.       0.    1180.141 1158.628 1162.692 1146.649 1131.778\n",
      " 1116.275 1106.669 1095.086 1092.72  1091.802    0.   ]\n",
      "42\n",
      "[   0.       0.       0.    1140.533 1170.368 1154.051 1140.943 1139.375\n",
      " 1123.324 1108.393    0.       0.       0.       0.   ]\n",
      "56\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "14\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "28\n",
      "[   0.       0.       0.       0.       0.    1157.007 1138.505 1124.556\n",
      " 1110.567 1098.841 1092.063 1091.914 1091.633    0.   ]\n",
      "42\n",
      "[   0.       0.       0.    1142.964 1163.434 1148.913 1141.121 1147.155\n",
      " 1124.378 1109.663 1084.157    0.       0.       0.   ]\n",
      "56\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0\n",
      "[379.036 355.952 359.114 326.505 331.668 316.946 293.341 268.753 257.015\n",
      " 250.714 246.102 237.266 226.321 225.251]\n",
      "14\n",
      "[  0.      0.      0.    851.938 891.935 868.081 884.695 892.811 915.536\n",
      " 932.098 935.548 946.115 952.254 958.428]\n",
      "28\n",
      "[   0.       0.       0.       0.       0.    1164.405 1155.426 1124.788\n",
      " 1116.296 1100.675 1092.395 1089.42  1091.552    0.   ]\n",
      "42\n",
      "[   0.       0.       0.    1141.633 1163.47  1147.73  1143.135 1139.872\n",
      " 1124.364 1108.983 1084.117 1076.12     0.       0.   ]\n",
      "56\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 10, not 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-8e70d0a483a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mctr_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mctr_plot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m#ax=plt.subplot(1,2,t+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DLC-GPU/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DLC-GPU/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1417\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DLC-GPU/lib/python3.7/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     raise ValueError(\n\u001b[0;32m---> 66\u001b[0;31m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[1;32m     67\u001b[0m                 self._subplotspec = GridSpec(\n\u001b[1;32m     68\u001b[0m                         rows, cols, figure=self.figure)[int(num) - 1]\n",
      "\u001b[0;31mValueError\u001b[0m: num must be 1 <= num <= 10, not 11"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "######################### VISUALIZE 2D LOCATIONS ###############################\n",
    "################################################################################\n",
    "################################################################################\n",
    "clrs_new = ['blue','red','cyan','green','yellow','pink','magenta','white','lightgreen','lightblue']\n",
    "frame_id = 0\n",
    "offset = 40\n",
    "# load video data\n",
    "video_name = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/2020-3-16_12_54_07_193951_compressed.avi'\n",
    "original_vid = cv2.VideoCapture(video_name)\n",
    "original_vid.set(cv2.CAP_PROP_POS_FRAMES, start+offset)\n",
    "\n",
    "# load traces\n",
    "data = np.load(fname_out)\n",
    "tracesx = data['tracesx'].T\n",
    "tracesy = data['tracesy'].T\n",
    "\n",
    "labels_array = np.array((tracesx, tracesy))\n",
    "labels_array = labels_array.transpose(1,2,0)\n",
    "print (\"Labels array: \", labels_array.shape)\n",
    "\n",
    "#scale_percent = 400 # percent of original size\n",
    "width = 320*4\n",
    "height = 256*4\n",
    "dim = (width, height)\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.tight_layout(pad=1.08, h_pad=None, w_pad=None, rect=None)\n",
    "\n",
    "for ctr_plot, t in enumerate(np.arange(0+offset,end-start+offset,1)):\n",
    "    ax=plt.subplot(2,5,ctr_plot+1)\n",
    "    #ax=plt.subplot(1,2,t+1)\n",
    "    \n",
    "    ret, frame = original_vid.read()\n",
    "    \n",
    "    # resize image\n",
    "    resized = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA) \n",
    "\n",
    "    plt.imshow(resized,alpha=.7)\n",
    "\n",
    "    for a in range(0,labels_array.shape[1],14):\n",
    "        print (a)\n",
    "        tempx = labels_array[t, a:a + 14][:,0]\n",
    "        tempy = labels_array[t, a:a+14][:,1]\n",
    "        \n",
    "        print (tempx)\n",
    "        plt.scatter(\n",
    "            tempx, \n",
    "            tempy, \n",
    "            c=clrs_new[a//14],alpha=.8)\n",
    "    \n",
    "    plt.title(str(t+frame_id)+\" \" +str(round((t+frame_id)/25.,1)),fontsize=8)\n",
    "#     plt.ylim(1024,0)\n",
    "#     plt.xlim(0,1280)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730 0\n",
      "[0. 0.]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-72f3da58bd4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             plt.scatter(np.array(labels_array[frame_id][k])[:,0], \n\u001b[0m\u001b[1;32m     14\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     c=clrs_new[k])\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "frame_id = 583\n",
    "#print (labels_array[200])\n",
    "#clrs_new = ['blue','red','cyan','green','yellow','pink','magenta','white','lightgreen','lightblue']\n",
    "\n",
    "\n",
    "fig=plt.figure()\n",
    "frame_ids = [730]\n",
    "for frame_id in frame_ids:\n",
    "    for k in range(len(labels_array[frame_id])):\n",
    "        print (frame_id, k)\n",
    "        print (labels_array[frame_id][k])\n",
    "        if len(labels_array[frame_id][k])>0:\n",
    "            plt.scatter(np.array(labels_array[frame_id][k])[:,0], \n",
    "                    np.array(labels_array[frame_id][k])[:,1],\n",
    "                    c=clrs_new[k])\n",
    "        #print (np.array(labels_array[frame_id+t][k]).shape)\n",
    "        if np.array(labels_array[frame_id][k]).shape[0]>0:\n",
    "            plt.scatter(np.array(labels_array[frame_id][k])[:,0].mean(0),\n",
    "              np.array(labels_array[frame_id][k])[:,1].mean(0),\n",
    "              s=100, c=clrs_new[k])\n",
    "\n",
    "frame_ids = [731]\n",
    "for frame_id in frame_ids:\n",
    "    for k in range(len(labels_array[frame_id])):\n",
    "\n",
    "        if np.array(labels_array[frame_id][k]).shape[0]>0:\n",
    "            plt.scatter(np.array(labels_array[frame_id][k])[:,0].mean(0),\n",
    "              np.array(labels_array[frame_id][k])[:,1].mean(0),\n",
    "              s=100, c=clrs_new[k])\n",
    "\n",
    "\n",
    "    \n",
    "#plt.title(str(t+frame_id))\n",
    "plt.ylim(1024,0)\n",
    "plt.xlim(0,1280)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89988, 14)\n"
     ]
    }
   ],
   "source": [
    "traces_inferences = np.load('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/2020-3-16_12_54_07_193951_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full_traces_inferences.npy',allow_pickle=True)\n",
    "\n",
    "print (traces_inferences.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1000 [00:00<00:11, 82.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:12<00:00, 78.56it/s]\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "######################### MAKE VIDEOS ###################################\n",
    "#########################################################################\n",
    "\n",
    "# Add fixed labels to video:\n",
    "reassembled = np.load('/media/cat/4TBSSD/dan/cohort2/2020_07_19_16_32_55_857727_compressedDLC_resnet50_cohort2Oct21shuffle1_200000_full_traces_inferences_0_1000.npz')\n",
    "tracesx_re = reassembled['tracesx'].T\n",
    "tracesy_re = reassembled['tracesy'].T\n",
    "print (tracesx_re.shape)\n",
    "\n",
    "# OPTIONAL MAKE VIDEO TO REVIEW ASSEMBLED VS. INFERENCE LABELS (PRE-FIX)\n",
    "# colors have weird inversion; red is blue and cyan is yellow\n",
    "#colors_4 = ['blue','red','cyan','green','pink','orange']\n",
    "\n",
    "#          pup1     pup2    female  male\n",
    "colors_4= ['orange','green', 'blue', 'red', 'cyan','white','yellow']\n",
    "\n",
    "video_name = '/media/cat/4TBSSD/dan/cohort2/2020_07_19_16_32_55_857727.avi'\n",
    "original_vid = cv2.VideoCapture(video_name)\n",
    "\n",
    "# video sizes\n",
    "size_vid = np.array([1280,1024])\n",
    "scale = 1\n",
    "dot_size = 8//scale\n",
    "\n",
    "#out_dir = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/2020-3-9_08_18_49_128168/'\n",
    "fname_out = video_name[:-4]+\"_corrected.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc('M','P','E','G')\n",
    "video_out = cv2.VideoWriter(fname_out,fourcc, 25, (size_vid[0]//scale,size_vid[1]//scale), True)\n",
    "\n",
    "#dot_size = 10//scale\n",
    "# setup cutoff \n",
    "pcutoff = 0.01\n",
    "\n",
    "# go through first videos\n",
    "from tqdm import trange\n",
    "\n",
    "original_vid.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "for n in trange(start,end, 1):\n",
    "    ret, frame = original_vid.read()\n",
    "    #print (n, frame.shape)\n",
    "    cv2.putText(frame, str(n), (50, 100), font, 5, (255, 255, 0), 5)\n",
    "    frame = frame[::scale, ::scale]\n",
    "    \n",
    "    #cv2.putText(frame,   text,location,  font,font size, font color,    font weight, line)\n",
    "    #cv2.putText(img, text, pos, font_face, scale, color, 1, cv2.LINE_AA)\n",
    "    \n",
    "#     for k in range(traces_inferences.shape[1]):\n",
    "#         for p in range(len(traces_inferences[n,k])):\n",
    "#             y = int(traces_inferences[n][k][p][0])//scale + 2\n",
    "#             x = int(traces_inferences[n][k][p][1])//scale + 2\n",
    "         \n",
    "#             frame[x-dot_size:x+dot_size,y-dot_size:y+dot_size]= (np.float32(\n",
    "#                 matplotlib.colors.to_rgb('white'))*255.).astype('uint8')\n",
    "    \n",
    "    \n",
    "    for k in range(14*n_networks):\n",
    "        y = tracesx_re[n,k]\n",
    "        x = tracesy_re[n,k]\n",
    "        \n",
    "        if np.isnan(x) or np.isnan(y):\n",
    "            continue\n",
    "        else:\n",
    "            x=int(x)//scale\n",
    "            y=int(y)//scale\n",
    "            \n",
    "            frame[x-dot_size:x+dot_size,y-dot_size:y+dot_size]= (np.float32(\n",
    "                matplotlib.colors.to_rgb(colors_4[k//14]))*255.).astype('uint8')\n",
    "            #print (colors_4[k])\n",
    "            #frame[y-dot_size:y+dot_size,x-dot_size:x+dot_size]= (np.float32(\n",
    "            #    matplotlib.colors.to_rgb(colors_4[z//14]))*255.).astype('uint8')\n",
    "                \n",
    "    #print (\"\")\n",
    "    video_out.write(frame)\n",
    "\n",
    "    #print (\"\")\n",
    "\n",
    "video_out.release()\n",
    "original_vid.release()\n",
    "#cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 337)\n",
      "(56, 1000)\n"
     ]
    }
   ],
   "source": [
    "data1 = np.load('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/markers/markers_traces_inferences.npz')\n",
    "tracesx = data1['tracesx']\n",
    "print (tracesx.shape)\n",
    "\n",
    "data2 = np.load('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/2020-3-16_12_54_07_193951_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full_1000frames_traces_inferences.npz')\n",
    "tracesx = data2['tracesx']\n",
    "print (tracesx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
