{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# \n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import trange\n",
    "import parmap\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "#import umap\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sleap\n",
    "\n",
    "import sklearn.experimental\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# \n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "# \n",
    "\n",
    "#import numba\n",
    "#from numba import jit\n",
    "import parmap\n",
    "\n",
    "# \n",
    "from track import track\n",
    "from convert import convert\n",
    "from ethogram import ethogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_id(labels_gt, labels_pr):\n",
    "    \"\"\"Compute ID metrics for two labels.\"\"\"\n",
    "    n_gt, n_pr = 0, 0\n",
    "    tp, fp, fn  = 0, 0, 0\n",
    "    gt_without_id = 0\n",
    "    for lf_gt, lf_pr in zip(labels_gt, labels_pr):\n",
    "        assert lf_gt.frame_idx == lf_pr.frame_idx  # weak check but it's what we have until we fix the video metadata serialization\n",
    "\n",
    "        n_gt += len(lf_gt)\n",
    "        n_pr += len(lf_pr)\n",
    "\n",
    "        positive_pairs, false_negatives = sleap.nn.evals.match_instances(lf_gt, lf_pr)\n",
    "        fn += len(false_negatives)  # gt instances that were not found\n",
    "        fp += len(lf_pr) - len(positive_pairs)  # extra pr instances\n",
    "        \n",
    "        for inst_gt, inst_pr, _ in positive_pairs:\n",
    "            if inst_gt.track is None:  # gt not id labeled\n",
    "                gt_without_id += 1\n",
    "                continue\n",
    "            if inst_pr.track is None:  # pr not id labeled\n",
    "                fn += 1\n",
    "                continue\n",
    "            if inst_pr.track.name == inst_gt.track.name:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "    # \n",
    "    return {\"n_gt\": n_gt,\n",
    "            \"n_pr\": n_pr,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"gt_without_id\": gt_without_id,\n",
    "            \"acc\": tp / n_gt}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "############# MAKE TRAINING PKGs ON WORKSTATION #2 #############\n",
    "################################################################\n",
    "import sleap\n",
    "import numpy as np\n",
    "\n",
    "sleap.disable_preallocation()\n",
    "sleap.versions()\n",
    "\n",
    "# \n",
    "cohort = 'cohort1'\n",
    "time_of_day = 'day'\n",
    "root_dir = os.path.join('/home/cat/code/sleap/',cohort,time_of_day,\n",
    "                        'cross_validation/')\n",
    "\n",
    "# Load sleap pkg file containing labeled data and images\n",
    "labels = sleap.load_file(root_dir+'/'+cohort+\"_\"+time_of_day+'.pkg.slp')\n",
    "\n",
    "# Extract just user labeled frames\n",
    "user_labels = labels.extract(labels.user_labeled_frame_inds)\n",
    "\n",
    "# Subsample labels\n",
    "n_samples = [100,200,300,400,450]\n",
    "\n",
    "# \n",
    "for n_sample in n_samples:\n",
    "    idx = np.random.choice(np.arange(n_sample), n_sample, replace=False)  # This extracts 100 unique random ids from 500 total\n",
    "    train_user_labels = user_labels.extract(idx)\n",
    "\n",
    "    # Save index\n",
    "    np.save(root_dir + '/idx_'+cohort+\"_\"+time_of_day+'_subsample_'+str(n_sample)+'.npy', idx)\n",
    "\n",
    "    # Save new .pkg file\n",
    "    train_user_labels.save(root_dir + \"/\"+cohort+\"_\"+time_of_day+\"_\"+\"_subsample_\"+str(n_sample)+\".pkg.slp\", \n",
    "                           with_images = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "############# MAKE TEST PKGs ON WORKSTATION #1 #############\n",
    "############################################################\n",
    "import sleap\n",
    "import numpy as np\n",
    "\n",
    "sleap.disable_preallocation()\n",
    "sleap.versions()\n",
    "\n",
    "# \n",
    "cohort = 'cohort1'\n",
    "time_of_day = 'day'\n",
    "root_dir = os.path.join('/home/cat/data/',cohort,time_of_day)\n",
    "\n",
    "\n",
    "# Load sleap pkg file containing labeled data and images\n",
    "labels = sleap.load_file(root_dir+'/'+cohort+\"_\"+time_of_day+'.pkg.slp')\n",
    "\n",
    "# Extract just user labeled frames\n",
    "user_labels = labels.extract(labels.user_labeled_frame_inds)\n",
    "\n",
    "# Subsample labels\n",
    "n_samples = [100,200,300,400,450]\n",
    "\n",
    "# \n",
    "for n_sample in n_samples:\n",
    "    idx_train = root_dir + '/idx_'+cohort+'_'+time_of_day+'_subsample_'+str(n_sample)+'.npy' # load saved training set indexes\n",
    "    \n",
    "    # delete the training indexes from the total # of frames (500) to get all potential training indexes\n",
    "    idx_test = np.delete(np.arange(500), idx_train)\n",
    "    \n",
    "    # subselect only 50 training\n",
    "    idx_test = np.random.choice(idx_test, 50, replace=False)\n",
    "        \n",
    "    test_user_labels = user_labels.extract(idx)\n",
    "\n",
    "    # Save test .pkg file\n",
    "    test_user_labels.save(root_dir + cohort+\"_\"+time_of_day+\"_subsample_\"+str(n_sample)+\"_test.pkg.slp\", \n",
    "                          with_images = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLEAP: 1.1.1\n",
      "TensorFlow: 2.3.1\n",
      "Numpy: 1.18.5\n",
      "Python: 3.7.0\n",
      "OS: Linux-5.4.0-84-generic-x86_64-with-debian-buster-sid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.71it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.71it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.20it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.73it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DONE \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####################################################\n",
    "################# EVALUTE ERRORS ###################\n",
    "####################################################\n",
    "\n",
    "def get_matrices(cohort, labels_gt, labels_pr):\n",
    "\n",
    "#     if cohort == 'cohort1':\n",
    "#         tp = {'male':0, \n",
    "#               'female':0,\n",
    "#               'pup shaved':0,\n",
    "#               'pup unshaved':0\n",
    "#               }\n",
    "\n",
    "#         fp = {'male':0, \n",
    "#                'female':0,\n",
    "#                'pup shaved':0,\n",
    "#                'pup unshaved':0\n",
    "#               }\n",
    "\n",
    "#         fn = {'male':0, \n",
    "#                'female':0,\n",
    "#                'pup shaved':0,\n",
    "#                'pup unshaved':0\n",
    "#               }\n",
    "\n",
    "#         n_labeled = {'male':0, \n",
    "#                        'female':0,\n",
    "#                        'pup shaved':0,\n",
    "#                        'pup unshaved':0\n",
    "#                       }\n",
    "    else:\n",
    "        tp = {'male':0, \n",
    "          'female':0,\n",
    "          'pup1':0,\n",
    "          'pup2':0,\n",
    "          'pup3':0,\n",
    "          'pup4':0\n",
    "          }\n",
    "\n",
    "        fp = {'male':0, \n",
    "          'female':0,\n",
    "          'pup1':0,\n",
    "          'pup2':0,\n",
    "          'pup3':0,\n",
    "          'pup4':0\n",
    "          }\n",
    "        \n",
    "        fn = {'male':0, \n",
    "          'female':0,\n",
    "          'pup1':0,\n",
    "          'pup2':0,\n",
    "          'pup3':0,\n",
    "          'pup4':0\n",
    "          }\n",
    "        \n",
    "        n_labeled = {'male':0, \n",
    "          'female':0,\n",
    "          'pup1':0,\n",
    "          'pup2':0,\n",
    "          'pup3':0,\n",
    "          'pup4':0\n",
    "          }\n",
    "    \n",
    "    \n",
    "    # loop over all labeled frames\n",
    "    for lf_gt, lf_pr in zip(labels_gt, labels_pr):\n",
    "        assert lf_gt.frame_idx == lf_pr.frame_idx  # weak check but it's what we have until we fix the video metadata serialization\n",
    "        # n_gt += len(lf_gt)\n",
    "        # n_pr += len(lf_pr)\n",
    "\n",
    "        positive_pairs, false_negatives = sleap.nn.evals.match_instances(lf_gt, lf_pr)\n",
    "        # FN = len(false_negatives)  # gt instances that were not found\n",
    "        # FP = len(lf_pr) - len(positive_pairs)  # extra pr instances\n",
    "\n",
    "                \n",
    "        # \n",
    "        for inst_gt, inst_pr, _ in positive_pairs:\n",
    "           \n",
    "            # \n",
    "            if inst_gt.track is None:  # gt not id labeled\n",
    "                #gt_without_id += 1\n",
    "                continue\n",
    "                \n",
    "            # \n",
    "            if inst_pr.track is None:  # pr not id labeled\n",
    "                fn[inst_pr.track.name]+=1\n",
    "                continue\n",
    "\n",
    "            # \n",
    "            if inst_pr.track.name == inst_gt.track.name:\n",
    "                tp[inst_pr.track.name]+=1\n",
    "\n",
    "            else:\n",
    "                fp[inst_pr.track.name]+=1\n",
    "                pass \n",
    "        \n",
    "        # add animal specific \n",
    "        for inst_gt in false_negatives:\n",
    "            if inst_gt.track is not None:\n",
    "                fn[inst_gt.track.name]+=1\n",
    "           \n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "import sleap\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import trange\n",
    "\n",
    "sleap.disable_preallocation()\n",
    "sleap.versions()\n",
    "\n",
    "# \n",
    "cohort = 'cohort1'\n",
    "time_of_day = 'night'\n",
    "root_dir = os.path.join('/media/cat/256GB/dan/',cohort,time_of_day)\n",
    "\n",
    "#\n",
    "if cohort=='cohort1':\n",
    "    tracks = ['male', 'female', 'pup shaved', 'pup unshaved']\n",
    "    n_animals=4\n",
    "else:\n",
    "    tracks = ['male', 'female', 'pup1', 'pup2', 'pup3','pup4']\n",
    "    n_animals=6\n",
    "\n",
    "    \n",
    "# load gt labels\n",
    "n_samples = [100, 200, 300, 400, 450]\n",
    "\n",
    "#\n",
    "tp_array = np.zeros((5,10,n_animals),'int32')\n",
    "fp_array = np.zeros((5,10,n_animals),'int32')\n",
    "fn_array = np.zeros((5,10,n_animals),'int32')\n",
    "\n",
    "#\n",
    "for n,n_sample in enumerate(n_samples):\n",
    "    \n",
    "    # \n",
    "    for k in trange(10):\n",
    "        \n",
    "        #\n",
    "        labels_gt = sleap.load_file(root_dir+\"/test_\"+str(k)+\"_subsample_\"+str(n_sample)+\".pkg.slp\")\n",
    "        labels_pr = sleap.load_file(root_dir+\"/test_\"+str(k)+\"_subsample_\"+str(n_sample)+\".pkg.slp.predictions.slp\")\n",
    "\n",
    "        # \n",
    "        tp, fp, fn = get_matrices(cohort, labels_gt, labels_pr)\n",
    "\n",
    "        # \n",
    "        for a in range(n_animals):\n",
    "            tp_array[n,k,a] = tp[tracks[a]]\n",
    "            fp_array[n,k,a] = fp[tracks[a]]\n",
    "            fn_array[n,k,a] = fn[tracks[a]]\n",
    "\n",
    "        if n_sample==450:\n",
    "            break\n",
    "            \n",
    "print (\" DONE \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#####################################\n",
    "#####################################\n",
    "\n",
    "def plot_scatter(temp, clr, label):\n",
    "    mean = np.mean(temp)/50.\n",
    "    std = np.std(temp)/50.\n",
    "\n",
    "    plt.scatter(n_samples[n], mean, label=label, c=clr)\n",
    "    plt.errorbar(n_samples[n], mean, yerr=std,\n",
    "                c=clr)#, xlolims=True, label='xlolims=True')\n",
    "    return mean\n",
    "        \n",
    "        \n",
    "n_s = np.arange(5)\n",
    "\n",
    "if cohort=='cohort1':\n",
    "    n_animals=4\n",
    "else:\n",
    "    n_animals=6\n",
    "\n",
    "labels = ['true pos','false pos','false neg']\n",
    "for n in n_s:\n",
    "\n",
    "    #\n",
    "    for a in range(n_animals):\n",
    "        ax=plt.subplot(2,n_animals//2,a+1)\n",
    "\n",
    "        temp = tp_array[n,:,a]\n",
    "        if n==4:\n",
    "            temp = temp[0]\n",
    "\n",
    "        clr='black'\n",
    "        mean = plot_scatter(temp,clr, labels[0])\n",
    "        \n",
    "        # false positive\n",
    "        temp2 = fp_array[n,:,a]\n",
    "        if n==4:\n",
    "            temp2 = temp2[0]\n",
    "        clr='red'\n",
    "        mean = plot_scatter(temp2,clr, labels[1])\n",
    "\n",
    "        \n",
    "        # false negative\n",
    "        temp2 = fn_array[n,:,a]\n",
    "        if n==4:\n",
    "            temp2 = temp2[0]\n",
    "        clr='blue'\n",
    "        mean = plot_scatter(temp2,clr, labels[2])\n",
    "        \n",
    "        # \n",
    "        plt.ylim(0,1.0)\n",
    "        plt.xlim(0,1000)\n",
    "        plt.title(tracks[a])\n",
    "    # #\n",
    "        if n==0 and a==0:\n",
    "            plt.legend()\n",
    "plt.suptitle(cohort+ \" \"+time_of_day)\n",
    "plt.show()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#################################################\n",
    "#################################################\n",
    "cohort = 'cohort1'\n",
    "time_of_day = 'day'\n",
    "root_dir = os.path.join('/media/cat/256GB/dan/',cohort,time_of_day)\n",
    "\n",
    "n_samples = [100, 200,300,400,450]\n",
    "for n_sample in n_samples:\n",
    "\n",
    "    for k in range(10):\n",
    "\n",
    "        labels_gt = sleap.load_file(root_dir+\"/test_\"+str(k)+\"_subsample_\"+str(n_sample)+\".pkg.slp\")\n",
    "        labels_pr = sleap.load_file(root_dir+\"/test_\"+str(k)+\"_subsample_\"+str(n_sample)+\".pkg.slp.predictions.slp\")\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len labels:  Labels(labeled_frames=50, videos=11, skeletons=1, tracks=4) 50\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#################################################\n",
    "#################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5329341317365269\n",
      "1 0.5680473372781065\n",
      "2 0.5886075949367089\n",
      "3 0.5963855421686747\n",
      "4 0.5802469135802469\n",
      "5 0.5301204819277109\n",
      "6 0.5031055900621118\n",
      "7 0.5534591194968553\n",
      "8 0.5060240963855421\n",
      "9 0.6144578313253012\n",
      "0 0.5859872611464968\n",
      "1 0.6012658227848101\n",
      "2 0.5414012738853503\n",
      "3 0.5636363636363636\n",
      "4 0.525974025974026\n",
      "5 0.524390243902439\n",
      "6 0.515527950310559\n",
      "7 0.5032679738562091\n",
      "8 0.6078431372549019\n",
      "9 0.5652173913043478\n",
      "0 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n",
      "9 0.0\n",
      "0 0.7161290322580646\n",
      "1 0.6776315789473685\n",
      "2 0.7092198581560284\n",
      "3 0.6797385620915033\n",
      "4 0.7077922077922078\n",
      "5 0.7328767123287672\n",
      "6 0.7074829931972789\n",
      "7 0.6912751677852349\n",
      "8 0.6870748299319728\n",
      "9 0.68\n",
      "0 0.8823529411764706\n",
      "[100, 200, 300, 400, 450]\n",
      "[0.5573388638897785, 0.5534511444055503, 0.0, 0.6989220942488427, 0.8823529411764707]\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "############# PLOT EVALUATION RESULTS #################\n",
    "#######################################################\n",
    "import pickle\n",
    "def load_obj(name ):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "n_samples = [100,200,300,400,450]\n",
    "fig=plt.figure()\n",
    "\n",
    "cohorts = ['cohort1','cohort2']\n",
    "times = ['day','night']\n",
    "\n",
    "cohorts = ['cohort1']\n",
    "times = ['night']\n",
    "\n",
    "ctr=0\n",
    "for cohort in cohorts:\n",
    "    for time_of_day in times:\n",
    "        ax=plt.subplot(2,2,ctr+1)\n",
    "        fits = []\n",
    "\n",
    "        for n_sample in n_samples:\n",
    "\n",
    "            acc = []\n",
    "            for k in range(10):\n",
    "\n",
    "                res = load_obj('/media/cat/256GB/dan/'+cohort+'/'+time_of_day+'/res_'+str(k)+'_subsample_'+str(n_sample)+'.pkl')\n",
    "\n",
    "                temp = res['acc']\n",
    "                print (k, temp)\n",
    "                acc.append(temp)\n",
    "                \n",
    "                if n_sample==450:\n",
    "                    for p in range(9):\n",
    "                        acc.append(temp)\n",
    "                    break\n",
    "            # \n",
    "            mean = np.mean(acc)\n",
    "            std = np.std(acc)\n",
    "            plt.scatter(n_sample, mean, label=str(n_sample))\n",
    "\n",
    "            plt.errorbar(n_sample, mean, yerr=std )#, xlolims=True, label='xlolims=True')\n",
    "\n",
    "            fits.append(mean)\n",
    "            \n",
    "\n",
    "\n",
    "        #\n",
    "        print (n_samples)\n",
    "        print (fits)\n",
    "        p = np.poly1d(np.polyfit(n_samples, fits, 1))\n",
    "\n",
    "        x = np.arange(3000)\n",
    "\n",
    "        px = p(x)\n",
    "        plt.plot(x,px,label='linear fit')\n",
    "\n",
    "        #\n",
    "        diffs = px-0.8\n",
    "        idx = np.argmin(np.abs(diffs))\n",
    "        plt.plot([0,5000],[0.8,0.8],'--',c='grey',label='80% acc')\n",
    "        plt.plot([idx,idx],[0,1],'--',c='grey')\n",
    "\n",
    "\n",
    "        diffs = px-0.9\n",
    "        idx = np.argmin(np.abs(diffs))\n",
    "        plt.plot([0,5000],[0.9,0.9],'--',c='lightblue',label='90% acc')\n",
    "        plt.plot([idx,idx],[0,1],'--',c='lightblue')\n",
    "\n",
    "\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        # \n",
    "        plt.ylim(0,1)\n",
    "        plt.xlim(0,1000)\n",
    "        plt.ylabel(\"Accuracy \\n(50 validation frames; std shown)\")\n",
    "        plt.xlabel(\"# of training frames\")\n",
    "        plt.title(\"Cohort 1 (4 gerbils) - daytime videos\",fontsize=16)\n",
    "\n",
    "    #\n",
    "        plt.title(cohort+ \" \"+time_of_day)\n",
    "        ctr+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_gt': 239, 'n_pr': 178, 'tp': 153, 'fp': 20, 'fn': 10, 'gt_without_id': 5, 'acc': 0.6401673640167364}\n"
     ]
    }
   ],
   "source": [
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.count:  <bound method Sequence.count of Labels(labeled_frames=2397, videos=12, skeletons=1, tracks=4)>\n",
      "0 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(0, 1024, 1280, 3), backend=HDF5Video)\n",
      "1 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(20, 1024, 1280, 3), backend=HDF5Video)\n",
      "2 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(50, 1024, 1280, 3), backend=HDF5Video)\n",
      "3 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(90, 1024, 1280, 3), backend=HDF5Video)\n",
      "4 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(115, 1024, 1280, 3), backend=HDF5Video)\n",
      "5 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(50, 1024, 1280, 3), backend=HDF5Video)\n",
      "6 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(10, 1024, 1280, 3), backend=HDF5Video)\n",
      "7 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(10, 1024, 1280, 3), backend=HDF5Video)\n",
      "8 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(20, 1024, 1280, 3), backend=HDF5Video)\n",
      "9 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(30, 1024, 1280, 3), backend=HDF5Video)\n",
      "10 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(25, 1024, 1280, 3), backend=HDF5Video)\n",
      "11 Video(filename=/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp, shape=(10, 1024, 1280, 3), backend=HDF5Video)\n",
      "(20, 1024, 1280, 3)\n",
      "\n",
      "Labels(labeled_frames=450, videos=12, skeletons=1, tracks=4)\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "########### LOAD AND SPLIT DATA ############\n",
    "############################################\n",
    "\n",
    "# Load sleap pkg file containing labeled data and images\n",
    "labels = sleap.load_file('/media/cat/256GB/dan/cohort1/cohort1_day_only_labeled_frames.pkg.slp')\n",
    "\n",
    "print (\"labels.count: \", labels.count)\n",
    "\n",
    "for k in range(len(labels.videos)):\n",
    "    #print (k, labels.labels[k])\n",
    "    print (k, labels.videos[k])\n",
    " \n",
    "print (labels.videos[1].shape)\n",
    "\n",
    "print ('')\n",
    "\n",
    "idx_train = np.arange(450)\n",
    "labels_train = labels.extract(idx_train) \n",
    "print (labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#        #\n",
    "# labels_train = labels.extract(idx_train, copy=True)\n",
    "# labels_val = labels.extract(idx_val, copy=True)\n",
    "# labels_test = labels.extract(idx_test, copy=True)\n",
    "\n",
    "\n",
    "# # Generate a 0.8/0.1/0.1 split\n",
    "# labels_train, labels_test = labels.split(n=0.9)\n",
    "# # labels_val, labels_test = labels_val_test.split(n=0.5)\n",
    "\n",
    "\n",
    "# # # Save with images\n",
    "# # labels_train.save(\"train.pkg.slp\", with_images=True)\n",
    "# # #labels_val.save(\"val.pkg.slp\", with_images=True)\n",
    "# # labels_test.save(\"test.pkg.slp\", with_images=True)\n",
    "\n",
    "\n",
    "# # # Reload and check the grond truh labels in each of these packages:\n",
    "# # labels = sleap.load_file(\"train.pkg.slp\")\n",
    "# labels...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=True)\n",
      "TRAIN: [ 0  2  3  4  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 24 25 26 27\n",
      " 29 30 31 32 33 34 35 36 37 38 39 40 41 42 44 45 46 47 48 50 51 52 53 54\n",
      " 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 74 75 76 77 78 80\n",
      " 81 82 83 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] TEST: [ 1  5 17 23 28 43 49 73 79 84]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 27 28 29 30 31 32 33 34 36 37 38 39 40 42 43 44 45 46 47 48 49 50 51\n",
      " 52 54 55 56 57 58 59 60 61 62 63 64 65 68 69 70 71 72 73 74 75 76 77 78\n",
      " 79 80 81 82 83 84 85 86 87 88 89 91 92 93 94 95 97 98] TEST: [13 26 35 41 53 66 67 90 96 99]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 16 17 19 20 21 22 23 24 25 26\n",
      " 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 69 70 71 72 73 75 76 77\n",
      " 78 79 80 81 82 84 85 86 87 88 89 90 91 93 94 96 98 99] TEST: [11 15 18 52 68 74 83 92 95 97]\n",
      "TRAIN: [ 0  1  2  3  4  5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 28 29 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52\n",
      " 53 54 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 72 73 74 75 77 78 79\n",
      " 81 82 83 84 85 86 88 89 90 91 92 93 94 95 96 97 98 99] TEST: [ 6  7  8 27 30 55 71 76 80 87]\n",
      "TRAIN: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 15 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29 30 31 32 33 34 35 36 38 39 40 41 43 44 45 46 47 48 49 50 52 53\n",
      " 54 55 56 57 59 61 62 63 64 65 66 67 68 69 71 72 73 74 75 76 77 78 79 80\n",
      " 81 82 83 84 85 86 87 89 90 91 92 93 94 95 96 97 98 99] TEST: [ 4 14 16 37 42 51 58 60 70 88]\n",
      "TRAIN: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 36 37 39 40 41 42 43 44 45 46 48 49 51 52\n",
      " 53 54 55 57 58 60 61 62 63 64 65 66 67 68 70 71 73 74 75 76 77 79 80 81\n",
      " 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] TEST: [ 0 19 38 47 50 56 59 69 72 78]\n",
      "TRAIN: [ 0  1  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 22 23 24 25 26\n",
      " 27 28 30 31 32 33 34 35 37 38 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n",
      " 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 78 79\n",
      " 80 81 82 83 84 87 88 89 90 91 92 93 94 95 96 97 98 99] TEST: [ 2 12 21 29 36 39 54 77 85 86]\n",
      "TRAIN: [ 0  1  2  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 21 22 23 25 26 27\n",
      " 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 46 47 48 49 50 51 52\n",
      " 53 54 55 56 57 58 59 60 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78\n",
      " 79 80 83 84 85 86 87 88 89 90 91 92 93 95 96 97 98 99] TEST: [ 3  9 20 24 45 61 62 81 82 94]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 23 24 26\n",
      " 27 28 29 30 31 32 33 34 35 36 37 38 39 41 42 43 44 45 46 47 49 50 51 52\n",
      " 53 54 55 56 58 59 60 61 62 63 65 66 67 68 69 70 71 72 73 74 76 77 78 79\n",
      " 80 81 82 83 84 85 86 87 88 90 91 92 94 95 96 97 98 99] TEST: [10 22 25 40 48 57 64 75 89 93]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 35 36 37 38 39 40 41 42 43 45 47 48 49 50 51 52 53\n",
      " 54 55 56 57 58 59 60 61 62 64 66 67 68 69 70 71 72 73 74 75 76 77 78 79\n",
      " 80 81 82 83 84 85 86 87 88 89 90 92 93 94 95 96 97 99] TEST: [31 32 33 34 44 46 63 65 91 98]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "X = np.arange(100)\n",
    "y = np.zeros(100)\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "print(kf)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_train: [411 271 127 200   2 422 360 106 368 392 464 400 341  12 386 166  49 245\n",
      "  88 218 275 111  78  77  95 205 138 348  74  57 153 165 281 171 371   5\n",
      "  45  43   0   7  65  92 300 316 402 337 370 135 454 363 235 163  22 263\n",
      "  64 314 190  70 322 162 172  97  68   3 262 351 320 404 486 283 497 425\n",
      " 108 330 434 141 164 357 498 143 145 173 239 419   1 352 225  20 113 313]\n",
      "idx_test:  [ 25 397 250 191 215  83  18 212 276]\n",
      "idx_train: [392 464 400 341  12 386 166  49 245  88 218 275 111  78  77  95 205 138\n",
      " 348  74  57 153 165 281 171 371   5  45  43   0   7  65  92 300 316 402\n",
      " 337 370 135 454 363 235 163  22 263  64 314 190  70 322 162 172  97  68\n",
      "   3 262 351 320 404 486 283 497 425 108 330 434 141 164 357 498 143 145\n",
      " 173 239 419   1 352 225  20 113 313  25 397 250 191 215  83  18 212 276]\n",
      "idx_test:  [267 181  90  37 347   4 439 323 130]\n",
      "idx_train: [ 88 218 275 111  78  77  95 205 138 348  74  57 153 165 281 171 371   5\n",
      "  45  43   0   7  65  92 300 316 402 337 370 135 454 363 235 163  22 263\n",
      "  64 314 190  70 322 162 172  97  68   3 262 351 320 404 486 283 497 425\n",
      " 108 330 434 141 164 357 498 143 145 173 239 419   1 352 225  20 113 313\n",
      "  25 397 250 191 215  83  18 212 276 267 181  90  37 347   4 439 323 130]\n",
      "idx_test:  [234  30 100 365 297 467 188 193 353]\n",
      "idx_train: [348  74  57 153 165 281 171 371   5  45  43   0   7  65  92 300 316 402\n",
      " 337 370 135 454 363 235 163  22 263  64 314 190  70 322 162 172  97  68\n",
      "   3 262 351 320 404 486 283 497 425 108 330 434 141 164 357 498 143 145\n",
      " 173 239 419   1 352 225  20 113 313  25 397 250 191 215  83  18 212 276\n",
      " 267 181  90  37 347   4 439 323 130 234  30 100 365 297 467 188 193 353]\n",
      "idx_test:  [260 148 349  87 232 219 423 255 183]\n",
      "idx_train: [ 45  43   0   7  65  92 300 316 402 337 370 135 454 363 235 163  22 263\n",
      "  64 314 190  70 322 162 172  97  68   3 262 351 320 404 486 283 497 425\n",
      " 108 330 434 141 164 357 498 143 145 173 239 419   1 352 225  20 113 313\n",
      "  25 397 250 191 215  83  18 212 276 267 181  90  37 347   4 439 323 130\n",
      " 234  30 100 365 297 467 188 193 353 260 148 349  87 232 219 423 255 183]\n",
      "idx_test:  [413 186  56 466  76 261 104 491  98]\n",
      "idx_train: [337 370 135 454 363 235 163  22 263  64 314 190  70 322 162 172  97  68\n",
      "   3 262 351 320 404 486 283 497 425 108 330 434 141 164 357 498 143 145\n",
      " 173 239 419   1 352 225  20 113 313  25 397 250 191 215  83  18 212 276\n",
      " 267 181  90  37 347   4 439 323 130 234  30 100 365 297 467 188 193 353\n",
      " 260 148 349  87 232 219 423 255 183 413 186  56 466  76 261 104 491  98]\n",
      "idx_test:  [ 47 442 383 381 149  66 268 308 298]\n",
      "idx_train: [ 64 314 190  70 322 162 172  97  68   3 262 351 320 404 486 283 497 425\n",
      " 108 330 434 141 164 357 498 143 145 173 239 419   1 352 225  20 113 313\n",
      "  25 397 250 191 215  83  18 212 276 267 181  90  37 347   4 439 323 130\n",
      " 234  30 100 365 297 467 188 193 353 260 148 349  87 232 219 423 255 183\n",
      " 413 186  56 466  76 261 104 491  98  47 442 383 381 149  66 268 308 298]\n",
      "idx_test:  [334 436 273 416 345 203  14 207 358]\n",
      "idx_train: [  3 262 351 320 404 486 283 497 425 108 330 434 141 164 357 498 143 145\n",
      " 173 239 419   1 352 225  20 113 313  25 397 250 191 215  83  18 212 276\n",
      " 267 181  90  37 347   4 439 323 130 234  30 100 365 297 467 188 193 353\n",
      " 260 148 349  87 232 219 423 255 183 413 186  56 466  76 261 104 491  98\n",
      "  47 442 383 381 149  66 268 308 298 334 436 273 416 345 203  14 207 358]\n",
      "idx_test:  [355  61 490 294 226 344 208 441  33]\n",
      "idx_train: [108 330 434 141 164 357 498 143 145 173 239 419   1 352 225  20 113 313\n",
      "  25 397 250 191 215  83  18 212 276 267 181  90  37 347   4 439 323 130\n",
      " 234  30 100 365 297 467 188 193 353 260 148 349  87 232 219 423 255 183\n",
      " 413 186  56 466  76 261 104 491  98  47 442 383 381 149  66 268 308 298\n",
      " 334 436 273 416 345 203  14 207 358 355  61 490 294 226 344 208 441  33]\n",
      "idx_test:  [105 388  39  80 266 336  51 495 264]\n",
      "idx_train: [173 239 419   1 352 225  20 113 313  25 397 250 191 215  83  18 212 276\n",
      " 267 181  90  37 347   4 439 323 130 234  30 100 365 297 467 188 193 353\n",
      " 260 148 349  87 232 219 423 255 183 413 186  56 466  76 261 104 491  98\n",
      "  47 442 383 381 149  66 268 308 298 334 436 273 416 345 203  14 207 358\n",
      " 355  61 490 294 226 344 208 441  33 105 388  39  80 266 336  51 495 264]\n",
      "idx_test:  [122 216   6 169  79 115 484 137 224]\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "### CUSTOM SPLIT FUNCTION ###\n",
    "#############################\n",
    "\n",
    "# load sleap label file\n",
    "# labels = sleap.load_file(...)\n",
    "\n",
    "#\n",
    "n_tot_labels = 500      # total # of human labels\n",
    "\n",
    "n_labels = 100          # from 100 to 500 labels \n",
    "\n",
    "n_validation = 10\n",
    "\n",
    "val_ratio = 0.9\n",
    "\n",
    "n_train = int(n_labels*val_ratio)\n",
    "n_test = int(n_labels*(1-val_ratio))\n",
    "    \n",
    "# shuffled res\n",
    "idx = np.random.choice(np.arange(n_tot_labels),n_tot_labels, replace=False)    \n",
    "\n",
    "# \n",
    "for k in range(n_validation):\n",
    "    idx_train = idx[n_test*k:n_test*k+n_train]\n",
    "    idx_test = idx[n_test*k+n_train:n_test*(k+1)+n_train]\n",
    "    print (\"idx_train:\", idx_train)\n",
    "    print (\"idx_test: \", idx_test)\n",
    "    \n",
    "#     # \n",
    "#     labels_train = labels.extract(idx_train, copy=True)\n",
    "#     labels_test = labels.extract(idx_test, copy=True)\n",
    "\n",
    "#     # Save with images\n",
    "#     labels_train.save(\"train_\"+str(ctr)+\".pkg.slp\", with_images=True)\n",
    "#     labels_test.save(\"test_\"+str(ctr)+\".pkg.slp\", with_images=True)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "######### TRAIN SLEAP ON A TRAINING PKG DATASET #########\n",
    "#########################################################\n",
    "\n",
    "# comamnd line run:\n",
    "command = 'sleap-train '+'train.pkg.slp'\n",
    "\n",
    "os.command(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "######### PREDICT ON THE TEST SET #########\n",
    "###########################################\n",
    "\n",
    "# comamnd line run:\n",
    "command = 'sleap-train '+'test.pkg.slp'\n",
    "\n",
    "os.command(command)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "######### PREDICT ON THE TEST SET #########\n",
    "###########################################\n",
    "\n",
    "# Reload and check the grond truh labels in each of these packages:\n",
    "labels = sleap.load_file(\"train.pkg.slp\")\n",
    "labels....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "### REDEFINE SPLIT FUNCTION TO ALLOW FOR PROPER XVALIDATION ###\n",
    "###############################################################\n",
    "\n",
    "def split( \n",
    "         self, n: Union[float, int], copy: bool = True \n",
    "     ) -> Tuple[\"Labels\", \"Labels\"]: \n",
    "         \"\"\"Split labels randomly. \n",
    "  \n",
    "         Args: \n",
    "             n: Number or fraction of elements in the first split. \n",
    "             copy: If `True` (the default), return copies of the labels. \n",
    "  \n",
    "         Returns: \n",
    "             A tuple of `(labels_a, labels_b)` where both are `sleap.Labels` instances \n",
    "             subsampled from these labels. \n",
    "  \n",
    "         Notes: \n",
    "             If there is only 1 labeled frame, this will return two copies of the same \n",
    "             labels. For `len(labels) > 1`, splits are guaranteed to be mutually \n",
    "             exclusive. \n",
    "  \n",
    "         Example: \n",
    "             You can generate multiple splits by calling this repeatedly: \n",
    "  \n",
    "             ```py \n",
    "             # Generate a 0.8/0.1/0.1 train/val/test split. \n",
    "             labels_train, labels_val_test = labels.split(n=0.8) \n",
    "             labels_val, labels_test = labels_val_test.split(n=0.5) \n",
    "             ``` \n",
    "         \"\"\" \n",
    "         if len(self) == 1: \n",
    "             if copy: \n",
    "                 return self.copy(), self.copy() \n",
    "             else: \n",
    "                 return self, self \n",
    "  \n",
    "         # Split indices. \n",
    "         if type(n) != int: \n",
    "             n = round(len(self) * n) \n",
    "         n = max(min(n, len(self) - 1), 1) \n",
    "         idx_a, idx_b = train_test_split(list(range(len(self))), train_size=n) \n",
    "  \n",
    "         return self.extract(idx_a, copy=copy), self.extract(idx_b, copy=copy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
